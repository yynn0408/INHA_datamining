{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 와인 감별사 : 와인의 Quality를 분류하는 Classifier 만들기\n",
    "\n",
    "## 1. 과제 설명\n",
    "이번 과제에서는 케라스(Keras)를 활용하여, 와인의 품질을 분류하는 인공신경망 분류기를 만들어 볼 것입니다.\n",
    "케라스는 Tensorflow, Theano 등의 딥 러닝 라이브러리 위에서 동작하는 오픈 소스 라이브러리로, 보다 쉬운 API를 제공함으로써 모델 설계 및 학습, 테스트가 간단하다는 장점이 있습니다. \n",
    "\n",
    "### 1.1 케라스 설치를 위한 필수 라이브러리\n",
    "케라스를 설치하기 전에 먼저 필수적으로 설치해야 할 것들이 있습니다.\n",
    "* Anaconda : Python 3.x 버전, Numpy, Pandas, SciPy, sklearn 등 필수 라이브러리들이 포함된 통합 배포 팩\n",
    "<br> 아나콘다 설치 : https://www.anaconda.com/distribution/#download-section\n",
    "* Tensorflow : Google에서 개발한 오픈 소스 딥 러닝 라이브러리. <b>설치된 Python 버전과 호환되는 것으로 설치할것!</b>\n",
    "<br> 텐서플로우 설치 : https://www.tensorflow.org/install/pip\n",
    "<br> * CPU 버전을 설치할 것을 권장. \n",
    "\n",
    "### 1.2 케라스 설치\n",
    "위 라이브러리들을 설치한 후, 케라스를 설치합니다.\n",
    "* https://keras.io/#installation\n",
    "\n",
    "### 1.3 케라스 설치 확인\n",
    "케라스가 올바르게 설치되었는지 확인하기 위해, 케라스를 Import한 뒤 버전을 출력해봅니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 케라스의 버전이 출력되면 정상입니다. (출력되는 버전은 위 예시와 다를 수도 있음)<br> 나중에 신경망을 만들기 위한 클래스들도 함께 Import 합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Set 설명\n",
    " 본 과제에서 사용할 데이터 셋은 UCI에서 제공되는 Wine Quality Data Set입니다. (https://archive.ics.uci.edu/ml/datasets/Wine+Quality) 데이터는 레드 와인 1599개, 화이트 와인 4898개의 화학적 특성을 포함하고 있습니다. 데이터는 두 개의 CSV(Comma-seperated values)형태로 제공되며, 구성은 다음과 같습니다.\n",
    "* 화이트 와인 / 레드 와인 CSV 파일\n",
    "* 11개의 실수(Real) 입력 변수 (X)\n",
    "    * fixed acidity\n",
    "    * volatile acidity\n",
    "    * citric acid\n",
    "    * residual sugar\n",
    "    * chlorides\n",
    "    * free sulfur dioxide\n",
    "    * total sulfur dioxide\n",
    "    * density\n",
    "    * pH\n",
    "    * sulphates\n",
    "    * alcohol\n",
    "* 1개의 클래스 레이블 (Y)\n",
    "   * quality (0~10, 0: Very poor, 10: Very excellent)\n",
    "* Missing Value 없음\n",
    "* 클래스들이 불균등하게 분포함.\n",
    "\n",
    "더 자세한 사항은 블랙보드에 함께 올라가있는 설명 파일을 참고하도록 합시다.\n",
    "\n",
    "### 2.1 데이터 로드\n",
    "데이터 분석에서 가장 많이 사용되는 라이브러리 중 하나인 Pandas와 Numpy를 Import하겠습니다. Pandas는 데이터 분석에 유용한 데이터 타입인 DataFrame을 제공하며, Numpy는 효율적이고 빠른 매트릭스 연산을 지원합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.__version__\n",
    "pd.options.display.max_rows=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas를 이용해서 CSV 파일을 읽어들이도록 합시다. white_wine 변수에는 화이트 와인 데이터를, red_wine 변수에는 레드 와인 데이터를 읽어들입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45</td>\n",
       "      <td>170</td>\n",
       "      <td>1.001</td>\n",
       "      <td>3</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14</td>\n",
       "      <td>132</td>\n",
       "      <td>0.994</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.4</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.05</td>\n",
       "      <td>30</td>\n",
       "      <td>97</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47</td>\n",
       "      <td>186</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.4</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47</td>\n",
       "      <td>186</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.4</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.4</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.05</td>\n",
       "      <td>30</td>\n",
       "      <td>97</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.16</td>\n",
       "      <td>7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>30</td>\n",
       "      <td>136</td>\n",
       "      <td>0.9949</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.47</td>\n",
       "      <td>9.6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4891</th>\n",
       "      <td>5.7</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.038</td>\n",
       "      <td>38</td>\n",
       "      <td>121</td>\n",
       "      <td>0.99074</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.46</td>\n",
       "      <td>10.6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4892</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.032</td>\n",
       "      <td>29</td>\n",
       "      <td>112</td>\n",
       "      <td>0.99298</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0.54</td>\n",
       "      <td>9.7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24</td>\n",
       "      <td>92</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.5</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57</td>\n",
       "      <td>168</td>\n",
       "      <td>0.9949</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30</td>\n",
       "      <td>111</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20</td>\n",
       "      <td>110</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897</th>\n",
       "      <td>6</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.02</td>\n",
       "      <td>22</td>\n",
       "      <td>98</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4898 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     fixed acidity volatile acidity citric acid residual sugar chlorides  \\\n",
       "0                7             0.27        0.36           20.7     0.045   \n",
       "1              6.3              0.3        0.34            1.6     0.049   \n",
       "2              8.1             0.28         0.4            6.9      0.05   \n",
       "3              7.2             0.23        0.32            8.5     0.058   \n",
       "4              7.2             0.23        0.32            8.5     0.058   \n",
       "5              8.1             0.28         0.4            6.9      0.05   \n",
       "6              6.2             0.32        0.16              7     0.045   \n",
       "...            ...              ...         ...            ...       ...   \n",
       "4891           5.7             0.21        0.32            0.9     0.038   \n",
       "4892           6.5             0.23        0.38            1.3     0.032   \n",
       "4893           6.2             0.21        0.29            1.6     0.039   \n",
       "4894           6.6             0.32        0.36              8     0.047   \n",
       "4895           6.5             0.24        0.19            1.2     0.041   \n",
       "4896           5.5             0.29         0.3            1.1     0.022   \n",
       "4897             6             0.21        0.38            0.8      0.02   \n",
       "\n",
       "     free sulfur dioxide total sulfur dioxide  density    pH sulphates  \\\n",
       "0                     45                  170    1.001     3      0.45   \n",
       "1                     14                  132    0.994   3.3      0.49   \n",
       "2                     30                   97   0.9951  3.26      0.44   \n",
       "3                     47                  186   0.9956  3.19       0.4   \n",
       "4                     47                  186   0.9956  3.19       0.4   \n",
       "5                     30                   97   0.9951  3.26      0.44   \n",
       "6                     30                  136   0.9949  3.18      0.47   \n",
       "...                  ...                  ...      ...   ...       ...   \n",
       "4891                  38                  121  0.99074  3.24      0.46   \n",
       "4892                  29                  112  0.99298  3.29      0.54   \n",
       "4893                  24                   92  0.99114  3.27       0.5   \n",
       "4894                  57                  168   0.9949  3.15      0.46   \n",
       "4895                  30                  111  0.99254  2.99      0.46   \n",
       "4896                  20                  110  0.98869  3.34      0.38   \n",
       "4897                  22                   98  0.98941  3.26      0.32   \n",
       "\n",
       "     alcohol quality  \n",
       "0        8.8       6  \n",
       "1        9.5       6  \n",
       "2       10.1       6  \n",
       "3        9.9       6  \n",
       "4        9.9       6  \n",
       "5       10.1       6  \n",
       "6        9.6       6  \n",
       "...      ...     ...  \n",
       "4891    10.6       6  \n",
       "4892     9.7       5  \n",
       "4893    11.2       6  \n",
       "4894     9.6       5  \n",
       "4895     9.4       6  \n",
       "4896    12.8       7  \n",
       "4897    11.8       6  \n",
       "\n",
       "[4898 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#########################코드########################\n",
    "white_wine=pd.read_csv('C:\\\\Users\\\\82109\\\\Desktop\\\\PPT\\\\wine data\\\\winequality-white.csv',sep=',',dtype='unicode')\n",
    "red_wine=pd.read_csv('C:\\\\Users\\\\82109\\\\Desktop\\\\PPT\\\\wine data\\\\winequality-red.csv',sep=',',dtype='unicode')\n",
    "\n",
    "white_wine.dtypes\n",
    "display(white_wine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 데이터 전처리\n",
    "데이터를 읽어들인 뒤, 읽어들인 데이터프레임을 display 함수를 통해 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82109\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype object were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\82109\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype object were all converted to float64 by StandardScaler.\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.172097</td>\n",
       "      <td>-0.081770</td>\n",
       "      <td>0.213280</td>\n",
       "      <td>2.821349</td>\n",
       "      <td>-0.035355</td>\n",
       "      <td>0.569932</td>\n",
       "      <td>0.744565</td>\n",
       "      <td>2.331512</td>\n",
       "      <td>-1.246921</td>\n",
       "      <td>-0.349184</td>\n",
       "      <td>-1.393152</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.657501</td>\n",
       "      <td>0.215896</td>\n",
       "      <td>0.048001</td>\n",
       "      <td>-0.944765</td>\n",
       "      <td>0.147747</td>\n",
       "      <td>-1.253019</td>\n",
       "      <td>-0.149685</td>\n",
       "      <td>-0.009154</td>\n",
       "      <td>0.740029</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>-0.824276</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.475751</td>\n",
       "      <td>0.017452</td>\n",
       "      <td>0.543838</td>\n",
       "      <td>0.100282</td>\n",
       "      <td>0.193523</td>\n",
       "      <td>-0.312141</td>\n",
       "      <td>-0.973336</td>\n",
       "      <td>0.358665</td>\n",
       "      <td>0.475102</td>\n",
       "      <td>-0.436816</td>\n",
       "      <td>-0.336667</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.409125</td>\n",
       "      <td>-0.478657</td>\n",
       "      <td>-0.117278</td>\n",
       "      <td>0.415768</td>\n",
       "      <td>0.559727</td>\n",
       "      <td>0.687541</td>\n",
       "      <td>1.121091</td>\n",
       "      <td>0.525855</td>\n",
       "      <td>0.011480</td>\n",
       "      <td>-0.787342</td>\n",
       "      <td>-0.499203</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.409125</td>\n",
       "      <td>-0.478657</td>\n",
       "      <td>-0.117278</td>\n",
       "      <td>0.415768</td>\n",
       "      <td>0.559727</td>\n",
       "      <td>0.687541</td>\n",
       "      <td>1.121091</td>\n",
       "      <td>0.525855</td>\n",
       "      <td>0.011480</td>\n",
       "      <td>-0.787342</td>\n",
       "      <td>-0.499203</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.475751</td>\n",
       "      <td>0.017452</td>\n",
       "      <td>0.543838</td>\n",
       "      <td>0.100282</td>\n",
       "      <td>0.193523</td>\n",
       "      <td>-0.312141</td>\n",
       "      <td>-0.973336</td>\n",
       "      <td>0.358665</td>\n",
       "      <td>0.475102</td>\n",
       "      <td>-0.436816</td>\n",
       "      <td>-0.336667</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.776015</td>\n",
       "      <td>0.414339</td>\n",
       "      <td>-1.439511</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>-0.035355</td>\n",
       "      <td>-0.312141</td>\n",
       "      <td>-0.055553</td>\n",
       "      <td>0.291789</td>\n",
       "      <td>-0.054751</td>\n",
       "      <td>-0.173921</td>\n",
       "      <td>-0.743008</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4891</th>\n",
       "      <td>-1.368585</td>\n",
       "      <td>-0.677101</td>\n",
       "      <td>-0.117278</td>\n",
       "      <td>-1.082790</td>\n",
       "      <td>-0.355784</td>\n",
       "      <td>0.158298</td>\n",
       "      <td>-0.408546</td>\n",
       "      <td>-1.099236</td>\n",
       "      <td>0.342639</td>\n",
       "      <td>-0.261553</td>\n",
       "      <td>0.069674</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4892</th>\n",
       "      <td>-0.420473</td>\n",
       "      <td>-0.478657</td>\n",
       "      <td>0.378559</td>\n",
       "      <td>-1.003919</td>\n",
       "      <td>-0.630437</td>\n",
       "      <td>-0.370946</td>\n",
       "      <td>-0.620342</td>\n",
       "      <td>-0.350223</td>\n",
       "      <td>0.673797</td>\n",
       "      <td>0.439499</td>\n",
       "      <td>-0.661739</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>-0.776015</td>\n",
       "      <td>-0.677101</td>\n",
       "      <td>-0.365197</td>\n",
       "      <td>-0.944765</td>\n",
       "      <td>-0.310008</td>\n",
       "      <td>-0.664970</td>\n",
       "      <td>-1.091000</td>\n",
       "      <td>-0.965483</td>\n",
       "      <td>0.541334</td>\n",
       "      <td>0.088973</td>\n",
       "      <td>0.557282</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>-0.301959</td>\n",
       "      <td>0.414339</td>\n",
       "      <td>0.213280</td>\n",
       "      <td>0.317179</td>\n",
       "      <td>0.056196</td>\n",
       "      <td>1.275590</td>\n",
       "      <td>0.697499</td>\n",
       "      <td>0.291789</td>\n",
       "      <td>-0.253446</td>\n",
       "      <td>-0.261553</td>\n",
       "      <td>-0.743008</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>-0.420473</td>\n",
       "      <td>-0.379435</td>\n",
       "      <td>-1.191592</td>\n",
       "      <td>-1.023637</td>\n",
       "      <td>-0.218457</td>\n",
       "      <td>-0.312141</td>\n",
       "      <td>-0.643875</td>\n",
       "      <td>-0.497350</td>\n",
       "      <td>-1.313153</td>\n",
       "      <td>-0.261553</td>\n",
       "      <td>-0.905544</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896</th>\n",
       "      <td>-1.605613</td>\n",
       "      <td>0.116674</td>\n",
       "      <td>-0.282557</td>\n",
       "      <td>-1.043355</td>\n",
       "      <td>-1.088192</td>\n",
       "      <td>-0.900190</td>\n",
       "      <td>-0.667408</td>\n",
       "      <td>-1.784717</td>\n",
       "      <td>1.004955</td>\n",
       "      <td>-0.962605</td>\n",
       "      <td>1.857572</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897</th>\n",
       "      <td>-1.013043</td>\n",
       "      <td>-0.677101</td>\n",
       "      <td>0.378559</td>\n",
       "      <td>-1.102508</td>\n",
       "      <td>-1.179743</td>\n",
       "      <td>-0.782580</td>\n",
       "      <td>-0.949803</td>\n",
       "      <td>-1.543962</td>\n",
       "      <td>0.475102</td>\n",
       "      <td>-1.488394</td>\n",
       "      <td>1.044891</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4898 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     0.172097 -0.081770  0.213280  2.821349 -0.035355  0.569932  0.744565   \n",
       "1    -0.657501  0.215896  0.048001 -0.944765  0.147747 -1.253019 -0.149685   \n",
       "2     1.475751  0.017452  0.543838  0.100282  0.193523 -0.312141 -0.973336   \n",
       "3     0.409125 -0.478657 -0.117278  0.415768  0.559727  0.687541  1.121091   \n",
       "4     0.409125 -0.478657 -0.117278  0.415768  0.559727  0.687541  1.121091   \n",
       "5     1.475751  0.017452  0.543838  0.100282  0.193523 -0.312141 -0.973336   \n",
       "6    -0.776015  0.414339 -1.439511  0.120000 -0.035355 -0.312141 -0.055553   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4891 -1.368585 -0.677101 -0.117278 -1.082790 -0.355784  0.158298 -0.408546   \n",
       "4892 -0.420473 -0.478657  0.378559 -1.003919 -0.630437 -0.370946 -0.620342   \n",
       "4893 -0.776015 -0.677101 -0.365197 -0.944765 -0.310008 -0.664970 -1.091000   \n",
       "4894 -0.301959  0.414339  0.213280  0.317179  0.056196  1.275590  0.697499   \n",
       "4895 -0.420473 -0.379435 -1.191592 -1.023637 -0.218457 -0.312141 -0.643875   \n",
       "4896 -1.605613  0.116674 -0.282557 -1.043355 -1.088192 -0.900190 -0.667408   \n",
       "4897 -1.013043 -0.677101  0.378559 -1.102508 -1.179743 -0.782580 -0.949803   \n",
       "\n",
       "             7         8         9        10 quality  \n",
       "0     2.331512 -1.246921 -0.349184 -1.393152       6  \n",
       "1    -0.009154  0.740029  0.001342 -0.824276       6  \n",
       "2     0.358665  0.475102 -0.436816 -0.336667       6  \n",
       "3     0.525855  0.011480 -0.787342 -0.499203       6  \n",
       "4     0.525855  0.011480 -0.787342 -0.499203       6  \n",
       "5     0.358665  0.475102 -0.436816 -0.336667       6  \n",
       "6     0.291789 -0.054751 -0.173921 -0.743008       6  \n",
       "...        ...       ...       ...       ...     ...  \n",
       "4891 -1.099236  0.342639 -0.261553  0.069674       6  \n",
       "4892 -0.350223  0.673797  0.439499 -0.661739       5  \n",
       "4893 -0.965483  0.541334  0.088973  0.557282       6  \n",
       "4894  0.291789 -0.253446 -0.261553 -0.743008       5  \n",
       "4895 -0.497350 -1.313153 -0.261553 -0.905544       6  \n",
       "4896 -1.784717  1.004955 -0.962605  1.857572       7  \n",
       "4897 -1.543962  0.475102 -1.488394  1.044891       6  \n",
       "\n",
       "[4898 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82109\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype object were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\82109\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype object were all converted to float64 by StandardScaler.\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.172097</td>\n",
       "      <td>-0.081770</td>\n",
       "      <td>0.213280</td>\n",
       "      <td>2.821349</td>\n",
       "      <td>-0.035355</td>\n",
       "      <td>0.569932</td>\n",
       "      <td>0.744565</td>\n",
       "      <td>2.331512</td>\n",
       "      <td>-1.246921</td>\n",
       "      <td>-0.349184</td>\n",
       "      <td>-1.393152</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.657501</td>\n",
       "      <td>0.215896</td>\n",
       "      <td>0.048001</td>\n",
       "      <td>-0.944765</td>\n",
       "      <td>0.147747</td>\n",
       "      <td>-1.253019</td>\n",
       "      <td>-0.149685</td>\n",
       "      <td>-0.009154</td>\n",
       "      <td>0.740029</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>-0.824276</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.475751</td>\n",
       "      <td>0.017452</td>\n",
       "      <td>0.543838</td>\n",
       "      <td>0.100282</td>\n",
       "      <td>0.193523</td>\n",
       "      <td>-0.312141</td>\n",
       "      <td>-0.973336</td>\n",
       "      <td>0.358665</td>\n",
       "      <td>0.475102</td>\n",
       "      <td>-0.436816</td>\n",
       "      <td>-0.336667</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.409125</td>\n",
       "      <td>-0.478657</td>\n",
       "      <td>-0.117278</td>\n",
       "      <td>0.415768</td>\n",
       "      <td>0.559727</td>\n",
       "      <td>0.687541</td>\n",
       "      <td>1.121091</td>\n",
       "      <td>0.525855</td>\n",
       "      <td>0.011480</td>\n",
       "      <td>-0.787342</td>\n",
       "      <td>-0.499203</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.409125</td>\n",
       "      <td>-0.478657</td>\n",
       "      <td>-0.117278</td>\n",
       "      <td>0.415768</td>\n",
       "      <td>0.559727</td>\n",
       "      <td>0.687541</td>\n",
       "      <td>1.121091</td>\n",
       "      <td>0.525855</td>\n",
       "      <td>0.011480</td>\n",
       "      <td>-0.787342</td>\n",
       "      <td>-0.499203</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.475751</td>\n",
       "      <td>0.017452</td>\n",
       "      <td>0.543838</td>\n",
       "      <td>0.100282</td>\n",
       "      <td>0.193523</td>\n",
       "      <td>-0.312141</td>\n",
       "      <td>-0.973336</td>\n",
       "      <td>0.358665</td>\n",
       "      <td>0.475102</td>\n",
       "      <td>-0.436816</td>\n",
       "      <td>-0.336667</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.776015</td>\n",
       "      <td>0.414339</td>\n",
       "      <td>-1.439511</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>-0.035355</td>\n",
       "      <td>-0.312141</td>\n",
       "      <td>-0.055553</td>\n",
       "      <td>0.291789</td>\n",
       "      <td>-0.054751</td>\n",
       "      <td>-0.173921</td>\n",
       "      <td>-0.743008</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4891</th>\n",
       "      <td>-1.368585</td>\n",
       "      <td>-0.677101</td>\n",
       "      <td>-0.117278</td>\n",
       "      <td>-1.082790</td>\n",
       "      <td>-0.355784</td>\n",
       "      <td>0.158298</td>\n",
       "      <td>-0.408546</td>\n",
       "      <td>-1.099236</td>\n",
       "      <td>0.342639</td>\n",
       "      <td>-0.261553</td>\n",
       "      <td>0.069674</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4892</th>\n",
       "      <td>-0.420473</td>\n",
       "      <td>-0.478657</td>\n",
       "      <td>0.378559</td>\n",
       "      <td>-1.003919</td>\n",
       "      <td>-0.630437</td>\n",
       "      <td>-0.370946</td>\n",
       "      <td>-0.620342</td>\n",
       "      <td>-0.350223</td>\n",
       "      <td>0.673797</td>\n",
       "      <td>0.439499</td>\n",
       "      <td>-0.661739</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>-0.776015</td>\n",
       "      <td>-0.677101</td>\n",
       "      <td>-0.365197</td>\n",
       "      <td>-0.944765</td>\n",
       "      <td>-0.310008</td>\n",
       "      <td>-0.664970</td>\n",
       "      <td>-1.091000</td>\n",
       "      <td>-0.965483</td>\n",
       "      <td>0.541334</td>\n",
       "      <td>0.088973</td>\n",
       "      <td>0.557282</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>-0.301959</td>\n",
       "      <td>0.414339</td>\n",
       "      <td>0.213280</td>\n",
       "      <td>0.317179</td>\n",
       "      <td>0.056196</td>\n",
       "      <td>1.275590</td>\n",
       "      <td>0.697499</td>\n",
       "      <td>0.291789</td>\n",
       "      <td>-0.253446</td>\n",
       "      <td>-0.261553</td>\n",
       "      <td>-0.743008</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>-0.420473</td>\n",
       "      <td>-0.379435</td>\n",
       "      <td>-1.191592</td>\n",
       "      <td>-1.023637</td>\n",
       "      <td>-0.218457</td>\n",
       "      <td>-0.312141</td>\n",
       "      <td>-0.643875</td>\n",
       "      <td>-0.497350</td>\n",
       "      <td>-1.313153</td>\n",
       "      <td>-0.261553</td>\n",
       "      <td>-0.905544</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896</th>\n",
       "      <td>-1.605613</td>\n",
       "      <td>0.116674</td>\n",
       "      <td>-0.282557</td>\n",
       "      <td>-1.043355</td>\n",
       "      <td>-1.088192</td>\n",
       "      <td>-0.900190</td>\n",
       "      <td>-0.667408</td>\n",
       "      <td>-1.784717</td>\n",
       "      <td>1.004955</td>\n",
       "      <td>-0.962605</td>\n",
       "      <td>1.857572</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897</th>\n",
       "      <td>-1.013043</td>\n",
       "      <td>-0.677101</td>\n",
       "      <td>0.378559</td>\n",
       "      <td>-1.102508</td>\n",
       "      <td>-1.179743</td>\n",
       "      <td>-0.782580</td>\n",
       "      <td>-0.949803</td>\n",
       "      <td>-1.543962</td>\n",
       "      <td>0.475102</td>\n",
       "      <td>-1.488394</td>\n",
       "      <td>1.044891</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4898 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     0.172097 -0.081770  0.213280  2.821349 -0.035355  0.569932  0.744565   \n",
       "1    -0.657501  0.215896  0.048001 -0.944765  0.147747 -1.253019 -0.149685   \n",
       "2     1.475751  0.017452  0.543838  0.100282  0.193523 -0.312141 -0.973336   \n",
       "3     0.409125 -0.478657 -0.117278  0.415768  0.559727  0.687541  1.121091   \n",
       "4     0.409125 -0.478657 -0.117278  0.415768  0.559727  0.687541  1.121091   \n",
       "5     1.475751  0.017452  0.543838  0.100282  0.193523 -0.312141 -0.973336   \n",
       "6    -0.776015  0.414339 -1.439511  0.120000 -0.035355 -0.312141 -0.055553   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4891 -1.368585 -0.677101 -0.117278 -1.082790 -0.355784  0.158298 -0.408546   \n",
       "4892 -0.420473 -0.478657  0.378559 -1.003919 -0.630437 -0.370946 -0.620342   \n",
       "4893 -0.776015 -0.677101 -0.365197 -0.944765 -0.310008 -0.664970 -1.091000   \n",
       "4894 -0.301959  0.414339  0.213280  0.317179  0.056196  1.275590  0.697499   \n",
       "4895 -0.420473 -0.379435 -1.191592 -1.023637 -0.218457 -0.312141 -0.643875   \n",
       "4896 -1.605613  0.116674 -0.282557 -1.043355 -1.088192 -0.900190 -0.667408   \n",
       "4897 -1.013043 -0.677101  0.378559 -1.102508 -1.179743 -0.782580 -0.949803   \n",
       "\n",
       "             7         8         9        10 quality  \n",
       "0     2.331512 -1.246921 -0.349184 -1.393152       6  \n",
       "1    -0.009154  0.740029  0.001342 -0.824276       6  \n",
       "2     0.358665  0.475102 -0.436816 -0.336667       6  \n",
       "3     0.525855  0.011480 -0.787342 -0.499203       6  \n",
       "4     0.525855  0.011480 -0.787342 -0.499203       6  \n",
       "5     0.358665  0.475102 -0.436816 -0.336667       6  \n",
       "6     0.291789 -0.054751 -0.173921 -0.743008       6  \n",
       "...        ...       ...       ...       ...     ...  \n",
       "4891 -1.099236  0.342639 -0.261553  0.069674       6  \n",
       "4892 -0.350223  0.673797  0.439499 -0.661739       5  \n",
       "4893 -0.965483  0.541334  0.088973  0.557282       6  \n",
       "4894  0.291789 -0.253446 -0.261553 -0.743008       5  \n",
       "4895 -0.497350 -1.313153 -0.261553 -0.905544       6  \n",
       "4896 -1.784717  1.004955 -0.962605  1.857572       7  \n",
       "4897 -1.543962  0.475102 -1.488394  1.044891       6  \n",
       "\n",
       "[4898 rows x 12 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler,MinMaxScaler,StandardScaler\n",
    "def preprocess(df):\n",
    "    Scaler=StandardScaler()\n",
    "    data=df.iloc[:,:-1]\n",
    "    qual=df.iloc[:,-1]\n",
    "    Scaler.fit(data)\n",
    "    df=Scaler.transform(data)\n",
    "    df=pd.DataFrame(df)\n",
    "    qual=pd.DataFrame(qual)\n",
    "    df=pd.concat([df,qual],axis=1)\n",
    "    return df\n",
    "    \n",
    "display(preprocess(white_wine))\n",
    "#display(white_wine)\n",
    "preprocess(white_wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82109\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype object were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\82109\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype object were all converted to float64 by StandardScaler.\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.528360</td>\n",
       "      <td>0.961877</td>\n",
       "      <td>-1.391472</td>\n",
       "      <td>-0.453218</td>\n",
       "      <td>-0.243707</td>\n",
       "      <td>-0.466240</td>\n",
       "      <td>-0.379145</td>\n",
       "      <td>0.558274</td>\n",
       "      <td>1.288643</td>\n",
       "      <td>-0.579207</td>\n",
       "      <td>-0.960246</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.298547</td>\n",
       "      <td>1.967442</td>\n",
       "      <td>-1.391472</td>\n",
       "      <td>0.043416</td>\n",
       "      <td>0.223875</td>\n",
       "      <td>0.872555</td>\n",
       "      <td>0.624333</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>-0.719933</td>\n",
       "      <td>0.128950</td>\n",
       "      <td>-0.584777</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.298547</td>\n",
       "      <td>1.297065</td>\n",
       "      <td>-1.186070</td>\n",
       "      <td>-0.169427</td>\n",
       "      <td>0.096353</td>\n",
       "      <td>-0.083727</td>\n",
       "      <td>0.229023</td>\n",
       "      <td>0.134264</td>\n",
       "      <td>-0.331177</td>\n",
       "      <td>-0.048089</td>\n",
       "      <td>-0.584777</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.654856</td>\n",
       "      <td>-1.384443</td>\n",
       "      <td>1.484154</td>\n",
       "      <td>-0.453218</td>\n",
       "      <td>-0.264960</td>\n",
       "      <td>0.107529</td>\n",
       "      <td>0.411474</td>\n",
       "      <td>0.664277</td>\n",
       "      <td>-0.979104</td>\n",
       "      <td>-0.461180</td>\n",
       "      <td>-0.584777</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.528360</td>\n",
       "      <td>0.961877</td>\n",
       "      <td>-1.391472</td>\n",
       "      <td>-0.453218</td>\n",
       "      <td>-0.243707</td>\n",
       "      <td>-0.466240</td>\n",
       "      <td>-0.379145</td>\n",
       "      <td>0.558274</td>\n",
       "      <td>1.288643</td>\n",
       "      <td>-0.579207</td>\n",
       "      <td>-0.960246</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.528360</td>\n",
       "      <td>0.738418</td>\n",
       "      <td>-1.391472</td>\n",
       "      <td>-0.524166</td>\n",
       "      <td>-0.264960</td>\n",
       "      <td>-0.274984</td>\n",
       "      <td>-0.196694</td>\n",
       "      <td>0.558274</td>\n",
       "      <td>1.288643</td>\n",
       "      <td>-0.579207</td>\n",
       "      <td>-0.960246</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.241094</td>\n",
       "      <td>0.403229</td>\n",
       "      <td>-1.083370</td>\n",
       "      <td>-0.666062</td>\n",
       "      <td>-0.392483</td>\n",
       "      <td>-0.083727</td>\n",
       "      <td>0.381066</td>\n",
       "      <td>-0.183745</td>\n",
       "      <td>-0.072005</td>\n",
       "      <td>-1.169337</td>\n",
       "      <td>-0.960246</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592</th>\n",
       "      <td>-1.160343</td>\n",
       "      <td>-0.099554</td>\n",
       "      <td>-0.723916</td>\n",
       "      <td>-0.169427</td>\n",
       "      <td>-0.243707</td>\n",
       "      <td>1.255068</td>\n",
       "      <td>-0.196694</td>\n",
       "      <td>-0.533554</td>\n",
       "      <td>0.705508</td>\n",
       "      <td>0.542042</td>\n",
       "      <td>0.541630</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1593</th>\n",
       "      <td>-0.873078</td>\n",
       "      <td>0.514959</td>\n",
       "      <td>-0.980669</td>\n",
       "      <td>-0.453218</td>\n",
       "      <td>-0.413736</td>\n",
       "      <td>1.159440</td>\n",
       "      <td>-0.257511</td>\n",
       "      <td>-0.125443</td>\n",
       "      <td>0.705508</td>\n",
       "      <td>0.955133</td>\n",
       "      <td>-0.866379</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>-1.217796</td>\n",
       "      <td>0.403229</td>\n",
       "      <td>-0.980669</td>\n",
       "      <td>-0.382271</td>\n",
       "      <td>0.053845</td>\n",
       "      <td>1.541953</td>\n",
       "      <td>-0.075061</td>\n",
       "      <td>-0.978765</td>\n",
       "      <td>0.899886</td>\n",
       "      <td>-0.461180</td>\n",
       "      <td>0.072294</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>-1.390155</td>\n",
       "      <td>0.123905</td>\n",
       "      <td>-0.877968</td>\n",
       "      <td>-0.240375</td>\n",
       "      <td>-0.541259</td>\n",
       "      <td>2.211351</td>\n",
       "      <td>0.137798</td>\n",
       "      <td>-0.862162</td>\n",
       "      <td>1.353436</td>\n",
       "      <td>0.601055</td>\n",
       "      <td>0.729364</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>-1.160343</td>\n",
       "      <td>-0.099554</td>\n",
       "      <td>-0.723916</td>\n",
       "      <td>-0.169427</td>\n",
       "      <td>-0.243707</td>\n",
       "      <td>1.255068</td>\n",
       "      <td>-0.196694</td>\n",
       "      <td>-0.533554</td>\n",
       "      <td>0.705508</td>\n",
       "      <td>0.542042</td>\n",
       "      <td>0.541630</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>-1.390155</td>\n",
       "      <td>0.654620</td>\n",
       "      <td>-0.775267</td>\n",
       "      <td>-0.382271</td>\n",
       "      <td>-0.264960</td>\n",
       "      <td>1.541953</td>\n",
       "      <td>-0.075061</td>\n",
       "      <td>-0.676657</td>\n",
       "      <td>1.677400</td>\n",
       "      <td>0.305990</td>\n",
       "      <td>-0.209308</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>-1.332702</td>\n",
       "      <td>-1.216849</td>\n",
       "      <td>1.021999</td>\n",
       "      <td>0.752894</td>\n",
       "      <td>-0.434990</td>\n",
       "      <td>0.203158</td>\n",
       "      <td>-0.135878</td>\n",
       "      <td>-0.666057</td>\n",
       "      <td>0.511130</td>\n",
       "      <td>0.010924</td>\n",
       "      <td>0.541630</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0    -0.528360  0.961877 -1.391472 -0.453218 -0.243707 -0.466240 -0.379145   \n",
       "1    -0.298547  1.967442 -1.391472  0.043416  0.223875  0.872555  0.624333   \n",
       "2    -0.298547  1.297065 -1.186070 -0.169427  0.096353 -0.083727  0.229023   \n",
       "3     1.654856 -1.384443  1.484154 -0.453218 -0.264960  0.107529  0.411474   \n",
       "4    -0.528360  0.961877 -1.391472 -0.453218 -0.243707 -0.466240 -0.379145   \n",
       "5    -0.528360  0.738418 -1.391472 -0.524166 -0.264960 -0.274984 -0.196694   \n",
       "6    -0.241094  0.403229 -1.083370 -0.666062 -0.392483 -0.083727  0.381066   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1592 -1.160343 -0.099554 -0.723916 -0.169427 -0.243707  1.255068 -0.196694   \n",
       "1593 -0.873078  0.514959 -0.980669 -0.453218 -0.413736  1.159440 -0.257511   \n",
       "1594 -1.217796  0.403229 -0.980669 -0.382271  0.053845  1.541953 -0.075061   \n",
       "1595 -1.390155  0.123905 -0.877968 -0.240375 -0.541259  2.211351  0.137798   \n",
       "1596 -1.160343 -0.099554 -0.723916 -0.169427 -0.243707  1.255068 -0.196694   \n",
       "1597 -1.390155  0.654620 -0.775267 -0.382271 -0.264960  1.541953 -0.075061   \n",
       "1598 -1.332702 -1.216849  1.021999  0.752894 -0.434990  0.203158 -0.135878   \n",
       "\n",
       "             7         8         9        10 quality  \n",
       "0     0.558274  1.288643 -0.579207 -0.960246       5  \n",
       "1     0.028261 -0.719933  0.128950 -0.584777       5  \n",
       "2     0.134264 -0.331177 -0.048089 -0.584777       5  \n",
       "3     0.664277 -0.979104 -0.461180 -0.584777       6  \n",
       "4     0.558274  1.288643 -0.579207 -0.960246       5  \n",
       "5     0.558274  1.288643 -0.579207 -0.960246       5  \n",
       "6    -0.183745 -0.072005 -1.169337 -0.960246       5  \n",
       "...        ...       ...       ...       ...     ...  \n",
       "1592 -0.533554  0.705508  0.542042  0.541630       6  \n",
       "1593 -0.125443  0.705508  0.955133 -0.866379       6  \n",
       "1594 -0.978765  0.899886 -0.461180  0.072294       5  \n",
       "1595 -0.862162  1.353436  0.601055  0.729364       6  \n",
       "1596 -0.533554  0.705508  0.542042  0.541630       6  \n",
       "1597 -0.676657  1.677400  0.305990 -0.209308       5  \n",
       "1598 -0.666057  0.511130  0.010924  0.541630       6  \n",
       "\n",
       "[1599 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82109\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype object were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\82109\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype object were all converted to float64 by StandardScaler.\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.528360</td>\n",
       "      <td>0.961877</td>\n",
       "      <td>-1.391472</td>\n",
       "      <td>-0.453218</td>\n",
       "      <td>-0.243707</td>\n",
       "      <td>-0.466240</td>\n",
       "      <td>-0.379145</td>\n",
       "      <td>0.558274</td>\n",
       "      <td>1.288643</td>\n",
       "      <td>-0.579207</td>\n",
       "      <td>-0.960246</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.298547</td>\n",
       "      <td>1.967442</td>\n",
       "      <td>-1.391472</td>\n",
       "      <td>0.043416</td>\n",
       "      <td>0.223875</td>\n",
       "      <td>0.872555</td>\n",
       "      <td>0.624333</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>-0.719933</td>\n",
       "      <td>0.128950</td>\n",
       "      <td>-0.584777</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.298547</td>\n",
       "      <td>1.297065</td>\n",
       "      <td>-1.186070</td>\n",
       "      <td>-0.169427</td>\n",
       "      <td>0.096353</td>\n",
       "      <td>-0.083727</td>\n",
       "      <td>0.229023</td>\n",
       "      <td>0.134264</td>\n",
       "      <td>-0.331177</td>\n",
       "      <td>-0.048089</td>\n",
       "      <td>-0.584777</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.654856</td>\n",
       "      <td>-1.384443</td>\n",
       "      <td>1.484154</td>\n",
       "      <td>-0.453218</td>\n",
       "      <td>-0.264960</td>\n",
       "      <td>0.107529</td>\n",
       "      <td>0.411474</td>\n",
       "      <td>0.664277</td>\n",
       "      <td>-0.979104</td>\n",
       "      <td>-0.461180</td>\n",
       "      <td>-0.584777</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.528360</td>\n",
       "      <td>0.961877</td>\n",
       "      <td>-1.391472</td>\n",
       "      <td>-0.453218</td>\n",
       "      <td>-0.243707</td>\n",
       "      <td>-0.466240</td>\n",
       "      <td>-0.379145</td>\n",
       "      <td>0.558274</td>\n",
       "      <td>1.288643</td>\n",
       "      <td>-0.579207</td>\n",
       "      <td>-0.960246</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.528360</td>\n",
       "      <td>0.738418</td>\n",
       "      <td>-1.391472</td>\n",
       "      <td>-0.524166</td>\n",
       "      <td>-0.264960</td>\n",
       "      <td>-0.274984</td>\n",
       "      <td>-0.196694</td>\n",
       "      <td>0.558274</td>\n",
       "      <td>1.288643</td>\n",
       "      <td>-0.579207</td>\n",
       "      <td>-0.960246</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.241094</td>\n",
       "      <td>0.403229</td>\n",
       "      <td>-1.083370</td>\n",
       "      <td>-0.666062</td>\n",
       "      <td>-0.392483</td>\n",
       "      <td>-0.083727</td>\n",
       "      <td>0.381066</td>\n",
       "      <td>-0.183745</td>\n",
       "      <td>-0.072005</td>\n",
       "      <td>-1.169337</td>\n",
       "      <td>-0.960246</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592</th>\n",
       "      <td>-1.160343</td>\n",
       "      <td>-0.099554</td>\n",
       "      <td>-0.723916</td>\n",
       "      <td>-0.169427</td>\n",
       "      <td>-0.243707</td>\n",
       "      <td>1.255068</td>\n",
       "      <td>-0.196694</td>\n",
       "      <td>-0.533554</td>\n",
       "      <td>0.705508</td>\n",
       "      <td>0.542042</td>\n",
       "      <td>0.541630</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1593</th>\n",
       "      <td>-0.873078</td>\n",
       "      <td>0.514959</td>\n",
       "      <td>-0.980669</td>\n",
       "      <td>-0.453218</td>\n",
       "      <td>-0.413736</td>\n",
       "      <td>1.159440</td>\n",
       "      <td>-0.257511</td>\n",
       "      <td>-0.125443</td>\n",
       "      <td>0.705508</td>\n",
       "      <td>0.955133</td>\n",
       "      <td>-0.866379</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>-1.217796</td>\n",
       "      <td>0.403229</td>\n",
       "      <td>-0.980669</td>\n",
       "      <td>-0.382271</td>\n",
       "      <td>0.053845</td>\n",
       "      <td>1.541953</td>\n",
       "      <td>-0.075061</td>\n",
       "      <td>-0.978765</td>\n",
       "      <td>0.899886</td>\n",
       "      <td>-0.461180</td>\n",
       "      <td>0.072294</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>-1.390155</td>\n",
       "      <td>0.123905</td>\n",
       "      <td>-0.877968</td>\n",
       "      <td>-0.240375</td>\n",
       "      <td>-0.541259</td>\n",
       "      <td>2.211351</td>\n",
       "      <td>0.137798</td>\n",
       "      <td>-0.862162</td>\n",
       "      <td>1.353436</td>\n",
       "      <td>0.601055</td>\n",
       "      <td>0.729364</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>-1.160343</td>\n",
       "      <td>-0.099554</td>\n",
       "      <td>-0.723916</td>\n",
       "      <td>-0.169427</td>\n",
       "      <td>-0.243707</td>\n",
       "      <td>1.255068</td>\n",
       "      <td>-0.196694</td>\n",
       "      <td>-0.533554</td>\n",
       "      <td>0.705508</td>\n",
       "      <td>0.542042</td>\n",
       "      <td>0.541630</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>-1.390155</td>\n",
       "      <td>0.654620</td>\n",
       "      <td>-0.775267</td>\n",
       "      <td>-0.382271</td>\n",
       "      <td>-0.264960</td>\n",
       "      <td>1.541953</td>\n",
       "      <td>-0.075061</td>\n",
       "      <td>-0.676657</td>\n",
       "      <td>1.677400</td>\n",
       "      <td>0.305990</td>\n",
       "      <td>-0.209308</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>-1.332702</td>\n",
       "      <td>-1.216849</td>\n",
       "      <td>1.021999</td>\n",
       "      <td>0.752894</td>\n",
       "      <td>-0.434990</td>\n",
       "      <td>0.203158</td>\n",
       "      <td>-0.135878</td>\n",
       "      <td>-0.666057</td>\n",
       "      <td>0.511130</td>\n",
       "      <td>0.010924</td>\n",
       "      <td>0.541630</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0    -0.528360  0.961877 -1.391472 -0.453218 -0.243707 -0.466240 -0.379145   \n",
       "1    -0.298547  1.967442 -1.391472  0.043416  0.223875  0.872555  0.624333   \n",
       "2    -0.298547  1.297065 -1.186070 -0.169427  0.096353 -0.083727  0.229023   \n",
       "3     1.654856 -1.384443  1.484154 -0.453218 -0.264960  0.107529  0.411474   \n",
       "4    -0.528360  0.961877 -1.391472 -0.453218 -0.243707 -0.466240 -0.379145   \n",
       "5    -0.528360  0.738418 -1.391472 -0.524166 -0.264960 -0.274984 -0.196694   \n",
       "6    -0.241094  0.403229 -1.083370 -0.666062 -0.392483 -0.083727  0.381066   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1592 -1.160343 -0.099554 -0.723916 -0.169427 -0.243707  1.255068 -0.196694   \n",
       "1593 -0.873078  0.514959 -0.980669 -0.453218 -0.413736  1.159440 -0.257511   \n",
       "1594 -1.217796  0.403229 -0.980669 -0.382271  0.053845  1.541953 -0.075061   \n",
       "1595 -1.390155  0.123905 -0.877968 -0.240375 -0.541259  2.211351  0.137798   \n",
       "1596 -1.160343 -0.099554 -0.723916 -0.169427 -0.243707  1.255068 -0.196694   \n",
       "1597 -1.390155  0.654620 -0.775267 -0.382271 -0.264960  1.541953 -0.075061   \n",
       "1598 -1.332702 -1.216849  1.021999  0.752894 -0.434990  0.203158 -0.135878   \n",
       "\n",
       "             7         8         9        10 quality  \n",
       "0     0.558274  1.288643 -0.579207 -0.960246       5  \n",
       "1     0.028261 -0.719933  0.128950 -0.584777       5  \n",
       "2     0.134264 -0.331177 -0.048089 -0.584777       5  \n",
       "3     0.664277 -0.979104 -0.461180 -0.584777       6  \n",
       "4     0.558274  1.288643 -0.579207 -0.960246       5  \n",
       "5     0.558274  1.288643 -0.579207 -0.960246       5  \n",
       "6    -0.183745 -0.072005 -1.169337 -0.960246       5  \n",
       "...        ...       ...       ...       ...     ...  \n",
       "1592 -0.533554  0.705508  0.542042  0.541630       6  \n",
       "1593 -0.125443  0.705508  0.955133 -0.866379       6  \n",
       "1594 -0.978765  0.899886 -0.461180  0.072294       5  \n",
       "1595 -0.862162  1.353436  0.601055  0.729364       6  \n",
       "1596 -0.533554  0.705508  0.542042  0.541630       6  \n",
       "1597 -0.676657  1.677400  0.305990 -0.209308       5  \n",
       "1598 -0.666057  0.511130  0.010924  0.541630       6  \n",
       "\n",
       "[1599 rows x 12 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(preprocess(red_wine))\n",
    "preprocess(red_wine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 데이터프레임을 입력 변수와 정답 셋(클래스 레이블)으로 나누는 함수를 작성하겠습니다.<br>\n",
    "<b>generate_data</b>함수는 데이터프레임 객체와 테스트 셋 비율을 입력으로 받아, 네 개의 numpy array를 반환합니다. 트레이닝 셋과 테스트 셋의 비율은 training_set_ratio에 의해 결정됩니다.\n",
    "* Function : generate_data\n",
    " * 입력\n",
    "     * pd.DataFrame : df\n",
    "     * double : training_set_ratio  \n",
    " * 출력\n",
    "     * np.array : X_train\n",
    "     * np.array : Y_train\n",
    "     * np.array : X_test\n",
    "     * np.array : Y_test\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "def generate_data(df, t_r):\n",
    "    x_data=df.iloc[:,:-1]\n",
    "    y_data=df.iloc[:,-1]\n",
    "    X_train=x_data.sample(frac=t_r,random_state=0)#random_state=seed\n",
    "    Y_train=y_data.sample(frac=t_r,random_state=0)#random_state=seed\n",
    "    X_test=x_data.drop(X_train.index)\n",
    "    Y_test=y_data.drop(Y_train.index)\n",
    "    Y_train=keras.utils.to_categorical(Y_train,num_classes=None)\n",
    "    Y_test=keras.utils.to_categorical(Y_test,num_classes=None)\n",
    "    return X_train.values, Y_train, X_test.values, Y_test\n",
    "\n",
    "#####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = generate_data(white_wine, 0.7)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "작성한 함수를 호출하여 화이트 와인 데이터에 대해 트레이닝 셋과 테스트 셋의 입력과 정답이 적절하게 생성되었는지 확인합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 케라스를 이용한 모델 생성, 학습, 테스트\n",
    "입력 데이터와 정답 셋이 만들어졌으니 케라스를 사용하여 각 데이터에 대한 분류기를 생성하고, 트레이닝 셋으로 학습시킨 뒤 테스트 정확도를 관찰합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 과제\n",
    "### 1. 화이트 와인 분류 모델과 레드 와인 분류 모델 설계 및 학습\n",
    "* 하나의 히든 레이어에 32개의 노드를 가진 인공신경망 모델 생성 및 모델 학습\n",
    "* 트레이닝 Epoch에 따라 Loss의 변화를 그래프로 시각화\n",
    "* 테스트 셋에 대한 정확도 기록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "3429/3429 [==============================] - 2s 490us/step - loss: 3.4178 - acc: 0.2313\n",
      "Epoch 2/300\n",
      "3429/3429 [==============================] - 0s 60us/step - loss: 1.5398 - acc: 0.4316\n",
      "Epoch 3/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.3363 - acc: 0.4436\n",
      "Epoch 4/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.3021 - acc: 0.4372\n",
      "Epoch 5/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.2748 - acc: 0.4401\n",
      "Epoch 6/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.2660 - acc: 0.4386\n",
      "Epoch 7/300\n",
      "3429/3429 [==============================] - 0s 63us/step - loss: 1.2632 - acc: 0.4497\n",
      "Epoch 8/300\n",
      "3429/3429 [==============================] - 0s 56us/step - loss: 1.2616 - acc: 0.4459\n",
      "Epoch 9/300\n",
      "3429/3429 [==============================] - 0s 54us/step - loss: 1.2482 - acc: 0.4517\n",
      "Epoch 10/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.2460 - acc: 0.4351\n",
      "Epoch 11/300\n",
      "3429/3429 [==============================] - 0s 60us/step - loss: 1.2442 - acc: 0.4450\n",
      "Epoch 12/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.2351 - acc: 0.4567\n",
      "Epoch 13/300\n",
      "3429/3429 [==============================] - 0s 67us/step - loss: 1.2333 - acc: 0.4555\n",
      "Epoch 14/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.2325 - acc: 0.4436\n",
      "Epoch 15/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.2226 - acc: 0.4614\n",
      "Epoch 16/300\n",
      "3429/3429 [==============================] - 0s 54us/step - loss: 1.2186 - acc: 0.4579\n",
      "Epoch 17/300\n",
      "3429/3429 [==============================] - 0s 60us/step - loss: 1.2126 - acc: 0.4622\n",
      "Epoch 18/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.2095 - acc: 0.4643\n",
      "Epoch 19/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.2104 - acc: 0.4494\n",
      "Epoch 20/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.2037 - acc: 0.4640\n",
      "Epoch 21/300\n",
      "3429/3429 [==============================] - 0s 43us/step - loss: 1.2087 - acc: 0.4547\n",
      "Epoch 22/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.2013 - acc: 0.4552\n",
      "Epoch 23/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.2047 - acc: 0.4579\n",
      "Epoch 24/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.1913 - acc: 0.4649\n",
      "Epoch 25/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.1973 - acc: 0.4707\n",
      "Epoch 26/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.1888 - acc: 0.4558\n",
      "Epoch 27/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.1875 - acc: 0.4640\n",
      "Epoch 28/300\n",
      "3429/3429 [==============================] - 0s 55us/step - loss: 1.1815 - acc: 0.4678\n",
      "Epoch 29/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.1773 - acc: 0.4818\n",
      "Epoch 30/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.1752 - acc: 0.4666\n",
      "Epoch 31/300\n",
      "3429/3429 [==============================] - 0s 37us/step - loss: 1.1740 - acc: 0.4774\n",
      "Epoch 32/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.1762 - acc: 0.4704\n",
      "Epoch 33/300\n",
      "3429/3429 [==============================] - 0s 56us/step - loss: 1.1719 - acc: 0.4692\n",
      "Epoch 34/300\n",
      "3429/3429 [==============================] - 0s 55us/step - loss: 1.1704 - acc: 0.4829\n",
      "Epoch 35/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.1678 - acc: 0.4789\n",
      "Epoch 36/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.1663 - acc: 0.4800\n",
      "Epoch 37/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.1654 - acc: 0.4736\n",
      "Epoch 38/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.1603 - acc: 0.4780\n",
      "Epoch 39/300\n",
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.1596 - acc: 0.4824\n",
      "Epoch 40/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.1603 - acc: 0.4824\n",
      "Epoch 41/300\n",
      "3429/3429 [==============================] - 0s 37us/step - loss: 1.1613 - acc: 0.4762\n",
      "Epoch 42/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.1579 - acc: 0.4879\n",
      "Epoch 43/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.1571 - acc: 0.4803\n",
      "Epoch 44/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.1550 - acc: 0.4777: 0s - loss: 1.1526 - acc: 0.48\n",
      "Epoch 45/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.1510 - acc: 0.4923\n",
      "Epoch 46/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.1489 - acc: 0.4984\n",
      "Epoch 47/300\n",
      "3429/3429 [==============================] - 0s 45us/step - loss: 1.1539 - acc: 0.4791\n",
      "Epoch 48/300\n",
      "3429/3429 [==============================] - 0s 43us/step - loss: 1.1446 - acc: 0.4861\n",
      "Epoch 49/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.1431 - acc: 0.4937\n",
      "Epoch 50/300\n",
      "3429/3429 [==============================] - 0s 44us/step - loss: 1.1466 - acc: 0.4841\n",
      "Epoch 51/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.1427 - acc: 0.4803\n",
      "Epoch 52/300\n",
      "3429/3429 [==============================] - 0s 54us/step - loss: 1.1458 - acc: 0.4786\n",
      "Epoch 53/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.1459 - acc: 0.4885\n",
      "Epoch 54/300\n",
      "3429/3429 [==============================] - 0s 54us/step - loss: 1.1410 - acc: 0.4882\n",
      "Epoch 55/300\n",
      "3429/3429 [==============================] - 0s 57us/step - loss: 1.1438 - acc: 0.4841\n",
      "Epoch 56/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.1381 - acc: 0.4853\n",
      "Epoch 57/300\n",
      "3429/3429 [==============================] - 0s 54us/step - loss: 1.1408 - acc: 0.4885\n",
      "Epoch 58/300\n",
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.1332 - acc: 0.4929\n",
      "Epoch 59/300\n",
      "3429/3429 [==============================] - 0s 56us/step - loss: 1.1347 - acc: 0.4800\n",
      "Epoch 60/300\n",
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.1374 - acc: 0.4964\n",
      "Epoch 61/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.1385 - acc: 0.4899\n",
      "Epoch 62/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.1348 - acc: 0.4969\n",
      "Epoch 63/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.1329 - acc: 0.4815\n",
      "Epoch 64/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.1339 - acc: 0.4958\n",
      "Epoch 65/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.1383 - acc: 0.4914\n",
      "Epoch 66/300\n",
      "3429/3429 [==============================] - 0s 90us/step - loss: 1.1313 - acc: 0.4864\n",
      "Epoch 67/300\n",
      "3429/3429 [==============================] - 0s 74us/step - loss: 1.1333 - acc: 0.4929\n",
      "Epoch 68/300\n",
      "3429/3429 [==============================] - 0s 43us/step - loss: 1.1326 - acc: 0.4940\n",
      "Epoch 69/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.1253 - acc: 0.4987\n",
      "Epoch 70/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.1285 - acc: 0.4969\n",
      "Epoch 71/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.1281 - acc: 0.4931\n",
      "Epoch 72/300\n",
      "3429/3429 [==============================] - 0s 45us/step - loss: 1.1284 - acc: 0.4987\n",
      "Epoch 73/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.1181 - acc: 0.4961\n",
      "Epoch 74/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.1280 - acc: 0.4850\n",
      "Epoch 75/300\n",
      "3429/3429 [==============================] - 0s 42us/step - loss: 1.1238 - acc: 0.4943\n",
      "Epoch 76/300\n",
      "3429/3429 [==============================] - 0s 76us/step - loss: 1.1249 - acc: 0.5057\n",
      "Epoch 77/300\n",
      "3429/3429 [==============================] - 0s 76us/step - loss: 1.1231 - acc: 0.5045\n",
      "Epoch 78/300\n",
      "3429/3429 [==============================] - 0s 73us/step - loss: 1.1223 - acc: 0.4966\n",
      "Epoch 79/300\n",
      "3429/3429 [==============================] - 0s 54us/step - loss: 1.1140 - acc: 0.4952\n",
      "Epoch 80/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.1209 - acc: 0.4993\n",
      "Epoch 81/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.1176 - acc: 0.5080\n",
      "Epoch 82/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.1189 - acc: 0.5019\n",
      "Epoch 83/300\n",
      "3429/3429 [==============================] - 0s 45us/step - loss: 1.1170 - acc: 0.5051\n",
      "Epoch 84/300\n",
      "3429/3429 [==============================] - 0s 44us/step - loss: 1.1176 - acc: 0.5019\n",
      "Epoch 85/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.1148 - acc: 0.5051\n",
      "Epoch 86/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.1223 - acc: 0.5080\n",
      "Epoch 87/300\n",
      "3429/3429 [==============================] - 0s 85us/step - loss: 1.1190 - acc: 0.4931\n",
      "Epoch 88/300\n",
      "3429/3429 [==============================] - 0s 55us/step - loss: 1.1149 - acc: 0.4975\n",
      "Epoch 89/300\n",
      "3429/3429 [==============================] - 0s 39us/step - loss: 1.1158 - acc: 0.5086\n",
      "Epoch 90/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.1116 - acc: 0.5010\n",
      "Epoch 91/300\n",
      "3429/3429 [==============================] - 0s 54us/step - loss: 1.1204 - acc: 0.4966\n",
      "Epoch 92/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.1152 - acc: 0.5031\n",
      "Epoch 93/300\n",
      "3429/3429 [==============================] - 0s 83us/step - loss: 1.1153 - acc: 0.5063\n",
      "Epoch 94/300\n",
      "3429/3429 [==============================] - 0s 81us/step - loss: 1.1150 - acc: 0.5071\n",
      "Epoch 95/300\n",
      "3429/3429 [==============================] - 0s 83us/step - loss: 1.1032 - acc: 0.5162\n",
      "Epoch 96/300\n",
      "3429/3429 [==============================] - 0s 59us/step - loss: 1.1081 - acc: 0.5092\n",
      "Epoch 97/300\n",
      "3429/3429 [==============================] - 0s 44us/step - loss: 1.1098 - acc: 0.5066\n",
      "Epoch 98/300\n",
      "3429/3429 [==============================] - 0s 44us/step - loss: 1.1103 - acc: 0.5039\n",
      "Epoch 99/300\n",
      "3429/3429 [==============================] - 0s 63us/step - loss: 1.1090 - acc: 0.5101\n",
      "Epoch 100/300\n",
      "3429/3429 [==============================] - 0s 76us/step - loss: 1.1089 - acc: 0.5034\n",
      "Epoch 101/300\n",
      "3429/3429 [==============================] - 0s 80us/step - loss: 1.1040 - acc: 0.5034\n",
      "Epoch 102/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.1096 - acc: 0.4949\n",
      "Epoch 103/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.1059 - acc: 0.5139\n",
      "Epoch 104/300\n",
      "3429/3429 [==============================] - 0s 45us/step - loss: 1.1042 - acc: 0.5077\n",
      "Epoch 105/300\n",
      "3429/3429 [==============================] - 0s 73us/step - loss: 1.1133 - acc: 0.4917\n",
      "Epoch 106/300\n",
      "3429/3429 [==============================] - 0s 73us/step - loss: 1.1025 - acc: 0.5066\n",
      "Epoch 107/300\n",
      "3429/3429 [==============================] - 0s 72us/step - loss: 1.1063 - acc: 0.5060\n",
      "Epoch 108/300\n",
      "3429/3429 [==============================] - 0s 78us/step - loss: 1.1043 - acc: 0.5121\n",
      "Epoch 109/300\n",
      "3429/3429 [==============================] - 0s 72us/step - loss: 1.1063 - acc: 0.5077\n",
      "Epoch 110/300\n",
      "3429/3429 [==============================] - 0s 70us/step - loss: 1.1061 - acc: 0.5168\n",
      "Epoch 111/300\n",
      "3429/3429 [==============================] - 0s 73us/step - loss: 1.1014 - acc: 0.5086\n",
      "Epoch 112/300\n",
      "3429/3429 [==============================] - 0s 44us/step - loss: 1.1091 - acc: 0.5095\n",
      "Epoch 113/300\n",
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.0958 - acc: 0.5165\n",
      "Epoch 114/300\n",
      "3429/3429 [==============================] - 0s 56us/step - loss: 1.1064 - acc: 0.5174\n",
      "Epoch 115/300\n",
      "3429/3429 [==============================] - 0s 65us/step - loss: 1.0968 - acc: 0.5095\n",
      "Epoch 116/300\n",
      "3429/3429 [==============================] - 0s 78us/step - loss: 1.1023 - acc: 0.5136\n",
      "Epoch 117/300\n",
      "3429/3429 [==============================] - 0s 81us/step - loss: 1.0944 - acc: 0.5176\n",
      "Epoch 118/300\n",
      "3429/3429 [==============================] - 0s 74us/step - loss: 1.1028 - acc: 0.5039\n",
      "Epoch 119/300\n",
      "3429/3429 [==============================] - 0s 84us/step - loss: 1.0957 - acc: 0.5223\n",
      "Epoch 120/300\n",
      "3429/3429 [==============================] - 0s 73us/step - loss: 1.0989 - acc: 0.5174\n",
      "Epoch 121/300\n",
      "3429/3429 [==============================] - 0s 75us/step - loss: 1.0963 - acc: 0.5039\n",
      "Epoch 122/300\n",
      "3429/3429 [==============================] - 0s 73us/step - loss: 1.0992 - acc: 0.5176\n",
      "Epoch 123/300\n",
      "3429/3429 [==============================] - 0s 72us/step - loss: 1.0980 - acc: 0.5042\n",
      "Epoch 124/300\n",
      "3429/3429 [==============================] - 0s 80us/step - loss: 1.0933 - acc: 0.5124\n",
      "Epoch 125/300\n",
      "3429/3429 [==============================] - 0s 59us/step - loss: 1.0893 - acc: 0.5156: 0s - loss: 1.0666 - acc: 0.5\n",
      "Epoch 126/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0926 - acc: 0.5121\n",
      "Epoch 127/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.0953 - acc: 0.5185\n",
      "Epoch 128/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.0933 - acc: 0.5106\n",
      "Epoch 129/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.0966 - acc: 0.5071\n",
      "Epoch 130/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0887 - acc: 0.5147\n",
      "Epoch 131/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.0856 - acc: 0.5133\n",
      "Epoch 132/300\n",
      "3429/3429 [==============================] - 0s 56us/step - loss: 1.0982 - acc: 0.5139\n",
      "Epoch 133/300\n",
      "3429/3429 [==============================] - 0s 44us/step - loss: 1.0863 - acc: 0.5133\n",
      "Epoch 134/300\n",
      "3429/3429 [==============================] - 0s 45us/step - loss: 1.0946 - acc: 0.5171\n",
      "Epoch 135/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.0895 - acc: 0.5159\n",
      "Epoch 136/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0893 - acc: 0.5206\n",
      "Epoch 137/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.0818 - acc: 0.5182\n",
      "Epoch 138/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.0909 - acc: 0.5150\n",
      "Epoch 139/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0863 - acc: 0.5141\n",
      "Epoch 140/300\n",
      "3429/3429 [==============================] - 0s 43us/step - loss: 1.0889 - acc: 0.5045\n",
      "Epoch 141/300\n",
      "3429/3429 [==============================] - 0s 45us/step - loss: 1.0856 - acc: 0.5124\n",
      "Epoch 142/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.0840 - acc: 0.5206\n",
      "Epoch 143/300\n",
      "3429/3429 [==============================] - 0s 43us/step - loss: 1.0852 - acc: 0.5176\n",
      "Epoch 144/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.0820 - acc: 0.5150\n",
      "Epoch 145/300\n",
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.0827 - acc: 0.5112\n",
      "Epoch 146/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0868 - acc: 0.5168\n",
      "Epoch 147/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.0837 - acc: 0.5197\n",
      "Epoch 148/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0824 - acc: 0.5191\n",
      "Epoch 149/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0854 - acc: 0.5168\n",
      "Epoch 150/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.0830 - acc: 0.5203\n",
      "Epoch 151/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0810 - acc: 0.5235\n",
      "Epoch 152/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.0810 - acc: 0.5229\n",
      "Epoch 153/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.0795 - acc: 0.5211\n",
      "Epoch 154/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.0781 - acc: 0.5182\n",
      "Epoch 155/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.0777 - acc: 0.5147\n",
      "Epoch 156/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.0837 - acc: 0.5156\n",
      "Epoch 157/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0802 - acc: 0.5171\n",
      "Epoch 158/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0816 - acc: 0.5176\n",
      "Epoch 159/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0793 - acc: 0.5287\n",
      "Epoch 160/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0832 - acc: 0.5165\n",
      "Epoch 161/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0778 - acc: 0.5287\n",
      "Epoch 162/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0768 - acc: 0.5258\n",
      "Epoch 163/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0818 - acc: 0.5179\n",
      "Epoch 164/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.0766 - acc: 0.5176\n",
      "Epoch 165/300\n",
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.0764 - acc: 0.5147\n",
      "Epoch 166/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.0831 - acc: 0.5174\n",
      "Epoch 167/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0775 - acc: 0.5220\n",
      "Epoch 168/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0746 - acc: 0.5244\n",
      "Epoch 169/300\n",
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.0753 - acc: 0.5188\n",
      "Epoch 170/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0752 - acc: 0.5206\n",
      "Epoch 171/300\n",
      "3429/3429 [==============================] - 0s 56us/step - loss: 1.0782 - acc: 0.5244\n",
      "Epoch 172/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.0659 - acc: 0.5276\n",
      "Epoch 173/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0788 - acc: 0.5214\n",
      "Epoch 174/300\n",
      "3429/3429 [==============================] - 0s 45us/step - loss: 1.0755 - acc: 0.5308\n",
      "Epoch 175/300\n",
      "3429/3429 [==============================] - 0s 87us/step - loss: 1.0707 - acc: 0.5308\n",
      "Epoch 176/300\n",
      "3429/3429 [==============================] - 0s 72us/step - loss: 1.0721 - acc: 0.5235\n",
      "Epoch 177/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.0676 - acc: 0.5311\n",
      "Epoch 178/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0689 - acc: 0.5249\n",
      "Epoch 179/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.0688 - acc: 0.5331\n",
      "Epoch 180/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0673 - acc: 0.5209\n",
      "Epoch 181/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.0718 - acc: 0.5197\n",
      "Epoch 182/300\n",
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.0702 - acc: 0.5281\n",
      "Epoch 183/300\n",
      "3429/3429 [==============================] - 0s 54us/step - loss: 1.0655 - acc: 0.5252\n",
      "Epoch 184/300\n",
      "3429/3429 [==============================] - 0s 54us/step - loss: 1.0711 - acc: 0.5206\n",
      "Epoch 185/300\n",
      "3429/3429 [==============================] - 0s 68us/step - loss: 1.0655 - acc: 0.5284\n",
      "Epoch 186/300\n",
      "3429/3429 [==============================] - 0s 65us/step - loss: 1.0724 - acc: 0.5249\n",
      "Epoch 187/300\n",
      "3429/3429 [==============================] - 0s 62us/step - loss: 1.0609 - acc: 0.5319\n",
      "Epoch 188/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.0651 - acc: 0.5209\n",
      "Epoch 189/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.0675 - acc: 0.5223\n",
      "Epoch 190/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.0707 - acc: 0.5319\n",
      "Epoch 191/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.0662 - acc: 0.5290\n",
      "Epoch 192/300\n",
      "3429/3429 [==============================] - 0s 55us/step - loss: 1.0630 - acc: 0.5311\n",
      "Epoch 193/300\n",
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.0713 - acc: 0.5232\n",
      "Epoch 194/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.0614 - acc: 0.5255\n",
      "Epoch 195/300\n",
      "3429/3429 [==============================] - 0s 44us/step - loss: 1.0681 - acc: 0.5311\n",
      "Epoch 196/300\n",
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.0657 - acc: 0.5270\n",
      "Epoch 197/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0635 - acc: 0.5261\n",
      "Epoch 198/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0673 - acc: 0.5302\n",
      "Epoch 199/300\n",
      "3429/3429 [==============================] - 0s 59us/step - loss: 1.0674 - acc: 0.5209\n",
      "Epoch 200/300\n",
      "3429/3429 [==============================] - 0s 57us/step - loss: 1.0650 - acc: 0.5281\n",
      "Epoch 201/300\n",
      "3429/3429 [==============================] - 0s 37us/step - loss: 1.0637 - acc: 0.5284\n",
      "Epoch 202/300\n",
      "3429/3429 [==============================] - 0s 37us/step - loss: 1.0661 - acc: 0.5220\n",
      "Epoch 203/300\n",
      "3429/3429 [==============================] - 0s 40us/step - loss: 1.0678 - acc: 0.5261\n",
      "Epoch 204/300\n",
      "3429/3429 [==============================] - 0s 43us/step - loss: 1.0637 - acc: 0.5381\n",
      "Epoch 205/300\n",
      "3429/3429 [==============================] - 0s 41us/step - loss: 1.0682 - acc: 0.5273\n",
      "Epoch 206/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.0598 - acc: 0.5273\n",
      "Epoch 207/300\n",
      "3429/3429 [==============================] - 0s 41us/step - loss: 1.0675 - acc: 0.5319\n",
      "Epoch 208/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0625 - acc: 0.5264\n",
      "Epoch 209/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.0656 - acc: 0.5302\n",
      "Epoch 210/300\n",
      "3429/3429 [==============================] - 0s 41us/step - loss: 1.0609 - acc: 0.5276\n",
      "Epoch 211/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.0693 - acc: 0.5249\n",
      "Epoch 212/300\n",
      "3429/3429 [==============================] - 0s 44us/step - loss: 1.0558 - acc: 0.5325\n",
      "Epoch 213/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0628 - acc: 0.5244\n",
      "Epoch 214/300\n",
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.0591 - acc: 0.5244\n",
      "Epoch 215/300\n",
      "3429/3429 [==============================] - 0s 41us/step - loss: 1.0619 - acc: 0.5255\n",
      "Epoch 216/300\n",
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.0591 - acc: 0.5316\n",
      "Epoch 217/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0572 - acc: 0.5281\n",
      "Epoch 218/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.0580 - acc: 0.5346\n",
      "Epoch 219/300\n",
      "3429/3429 [==============================] - 0s 45us/step - loss: 1.0552 - acc: 0.5372\n",
      "Epoch 220/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0601 - acc: 0.5346\n",
      "Epoch 221/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.0604 - acc: 0.5287\n",
      "Epoch 222/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0595 - acc: 0.5389\n",
      "Epoch 223/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.0575 - acc: 0.5249\n",
      "Epoch 224/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.0600 - acc: 0.5375\n",
      "Epoch 225/300\n",
      "3429/3429 [==============================] - 0s 58us/step - loss: 1.0615 - acc: 0.5337\n",
      "Epoch 226/300\n",
      "3429/3429 [==============================] - 0s 44us/step - loss: 1.0604 - acc: 0.5255\n",
      "Epoch 227/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0521 - acc: 0.5357\n",
      "Epoch 228/300\n",
      "3429/3429 [==============================] - 0s 44us/step - loss: 1.0588 - acc: 0.5328\n",
      "Epoch 229/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.0587 - acc: 0.5305\n",
      "Epoch 230/300\n",
      "3429/3429 [==============================] - 0s 42us/step - loss: 1.0576 - acc: 0.5322\n",
      "Epoch 231/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.0534 - acc: 0.5311\n",
      "Epoch 232/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.0554 - acc: 0.5244\n",
      "Epoch 233/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.0566 - acc: 0.5284\n",
      "Epoch 234/300\n",
      "3429/3429 [==============================] - 0s 54us/step - loss: 1.0584 - acc: 0.5363\n",
      "Epoch 235/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.0605 - acc: 0.5375\n",
      "Epoch 236/300\n",
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.0528 - acc: 0.5346\n",
      "Epoch 237/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.0591 - acc: 0.5270\n",
      "Epoch 238/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.0584 - acc: 0.5290\n",
      "Epoch 239/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.0555 - acc: 0.5395\n",
      "Epoch 240/300\n",
      "3429/3429 [==============================] - 0s 40us/step - loss: 1.0514 - acc: 0.5348\n",
      "Epoch 241/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.0566 - acc: 0.5369\n",
      "Epoch 242/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0554 - acc: 0.5174\n",
      "Epoch 243/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.0500 - acc: 0.5430\n",
      "Epoch 244/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3429/3429 [==============================] - 0s 44us/step - loss: 1.0526 - acc: 0.5319\n",
      "Epoch 245/300\n",
      "3429/3429 [==============================] - 0s 56us/step - loss: 1.0572 - acc: 0.5322\n",
      "Epoch 246/300\n",
      "3429/3429 [==============================] - 0s 59us/step - loss: 1.0536 - acc: 0.5311\n",
      "Epoch 247/300\n",
      "3429/3429 [==============================] - 0s 63us/step - loss: 1.0563 - acc: 0.5284\n",
      "Epoch 248/300\n",
      "3429/3429 [==============================] - 0s 59us/step - loss: 1.0555 - acc: 0.5346\n",
      "Epoch 249/300\n",
      "3429/3429 [==============================] - 0s 61us/step - loss: 1.0522 - acc: 0.5340\n",
      "Epoch 250/300\n",
      "3429/3429 [==============================] - 0s 58us/step - loss: 1.0546 - acc: 0.5319\n",
      "Epoch 251/300\n",
      "3429/3429 [==============================] - 0s 65us/step - loss: 1.0607 - acc: 0.5279\n",
      "Epoch 252/300\n",
      "3429/3429 [==============================] - 0s 95us/step - loss: 1.0511 - acc: 0.5395\n",
      "Epoch 253/300\n",
      "3429/3429 [==============================] - 0s 86us/step - loss: 1.0567 - acc: 0.5270\n",
      "Epoch 254/300\n",
      "3429/3429 [==============================] - 0s 89us/step - loss: 1.0535 - acc: 0.5381\n",
      "Epoch 255/300\n",
      "3429/3429 [==============================] - 0s 87us/step - loss: 1.0499 - acc: 0.5314\n",
      "Epoch 256/300\n",
      "3429/3429 [==============================] - 0s 70us/step - loss: 1.0621 - acc: 0.5346\n",
      "Epoch 257/300\n",
      "3429/3429 [==============================] - 0s 58us/step - loss: 1.0481 - acc: 0.5381\n",
      "Epoch 258/300\n",
      "3429/3429 [==============================] - 0s 64us/step - loss: 1.0608 - acc: 0.5357\n",
      "Epoch 259/300\n",
      "3429/3429 [==============================] - 0s 39us/step - loss: 1.0501 - acc: 0.5375\n",
      "Epoch 260/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.0477 - acc: 0.5407\n",
      "Epoch 261/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.0553 - acc: 0.5418\n",
      "Epoch 262/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.0460 - acc: 0.5369\n",
      "Epoch 263/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.0562 - acc: 0.5398\n",
      "Epoch 264/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0530 - acc: 0.5270\n",
      "Epoch 265/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.0492 - acc: 0.5369\n",
      "Epoch 266/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0489 - acc: 0.5404\n",
      "Epoch 267/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.0524 - acc: 0.5413\n",
      "Epoch 268/300\n",
      "3429/3429 [==============================] - 0s 57us/step - loss: 1.0494 - acc: 0.5404\n",
      "Epoch 269/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.0462 - acc: 0.5366\n",
      "Epoch 270/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0506 - acc: 0.5386\n",
      "Epoch 271/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.0453 - acc: 0.5395\n",
      "Epoch 272/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.0521 - acc: 0.5284\n",
      "Epoch 273/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.0484 - acc: 0.5439\n",
      "Epoch 274/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.0449 - acc: 0.5351\n",
      "Epoch 275/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0437 - acc: 0.5357\n",
      "Epoch 276/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.0482 - acc: 0.5357\n",
      "Epoch 277/300\n",
      "3429/3429 [==============================] - 0s 44us/step - loss: 1.0484 - acc: 0.5378\n",
      "Epoch 278/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0487 - acc: 0.5378\n",
      "Epoch 279/300\n",
      "3429/3429 [==============================] - 0s 44us/step - loss: 1.0492 - acc: 0.5337\n",
      "Epoch 280/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.0459 - acc: 0.5392\n",
      "Epoch 281/300\n",
      "3429/3429 [==============================] - 0s 45us/step - loss: 1.0493 - acc: 0.5369\n",
      "Epoch 282/300\n",
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.0442 - acc: 0.5305\n",
      "Epoch 283/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.0400 - acc: 0.5389\n",
      "Epoch 284/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0477 - acc: 0.5375\n",
      "Epoch 285/300\n",
      "3429/3429 [==============================] - 0s 45us/step - loss: 1.0502 - acc: 0.5270\n",
      "Epoch 286/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.0443 - acc: 0.5383\n",
      "Epoch 287/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.0467 - acc: 0.5445\n",
      "Epoch 288/300\n",
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.0511 - acc: 0.5381\n",
      "Epoch 289/300\n",
      "3429/3429 [==============================] - 0s 54us/step - loss: 1.0464 - acc: 0.5401\n",
      "Epoch 290/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.0540 - acc: 0.5328\n",
      "Epoch 291/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.0445 - acc: 0.5378\n",
      "Epoch 292/300\n",
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.0424 - acc: 0.5424\n",
      "Epoch 293/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.0483 - acc: 0.5378\n",
      "Epoch 294/300\n",
      "3429/3429 [==============================] - 0s 55us/step - loss: 1.0483 - acc: 0.5343\n",
      "Epoch 295/300\n",
      "3429/3429 [==============================] - 0s 54us/step - loss: 1.0506 - acc: 0.5401\n",
      "Epoch 296/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.0531 - acc: 0.5392\n",
      "Epoch 297/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.0451 - acc: 0.5413\n",
      "Epoch 298/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0482 - acc: 0.5375\n",
      "Epoch 299/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0431 - acc: 0.5401\n",
      "Epoch 300/300\n",
      "3429/3429 [==============================] - 0s 57us/step - loss: 1.0386 - acc: 0.5363\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "eveluate loss:1.0990839318972692, evaluate acc:0.537780803145804\n",
      "Epoch 1/300\n",
      "3429/3429 [==============================] - 2s 554us/step - loss: 3.6553 - acc: 0.3587\n",
      "Epoch 2/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.4260 - acc: 0.3715\n",
      "Epoch 3/300\n",
      "3429/3429 [==============================] - 0s 94us/step - loss: 1.3551 - acc: 0.3969\n",
      "Epoch 4/300\n",
      "3429/3429 [==============================] - 0s 66us/step - loss: 1.3446 - acc: 0.4089\n",
      "Epoch 5/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.3166 - acc: 0.4042\n",
      "Epoch 6/300\n",
      "3429/3429 [==============================] - 0s 75us/step - loss: 1.3095 - acc: 0.4185\n",
      "Epoch 7/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.2990 - acc: 0.4243\n",
      "Epoch 8/300\n",
      "3429/3429 [==============================] - 0s 43us/step - loss: 1.2836 - acc: 0.4313\n",
      "Epoch 9/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.2729 - acc: 0.4380\n",
      "Epoch 10/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.2657 - acc: 0.4342\n",
      "Epoch 11/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.2630 - acc: 0.4433\n",
      "Epoch 12/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.2460 - acc: 0.4538\n",
      "Epoch 13/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.2446 - acc: 0.4549\n",
      "Epoch 14/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.2503 - acc: 0.4395\n",
      "Epoch 15/300\n",
      "3429/3429 [==============================] - 0s 83us/step - loss: 1.2387 - acc: 0.4439\n",
      "Epoch 16/300\n",
      "3429/3429 [==============================] - 0s 92us/step - loss: 1.2347 - acc: 0.4514\n",
      "Epoch 17/300\n",
      "3429/3429 [==============================] - 0s 78us/step - loss: 1.2298 - acc: 0.4599\n",
      "Epoch 18/300\n",
      "3429/3429 [==============================] - 0s 58us/step - loss: 1.2252 - acc: 0.4549\n",
      "Epoch 19/300\n",
      "3429/3429 [==============================] - 0s 56us/step - loss: 1.2297 - acc: 0.4500\n",
      "Epoch 20/300\n",
      "3429/3429 [==============================] - 0s 60us/step - loss: 1.2285 - acc: 0.4494\n",
      "Epoch 21/300\n",
      "3429/3429 [==============================] - 0s 60us/step - loss: 1.2136 - acc: 0.4631\n",
      "Epoch 22/300\n",
      "3429/3429 [==============================] - 0s 54us/step - loss: 1.2120 - acc: 0.4593\n",
      "Epoch 23/300\n",
      "3429/3429 [==============================] - 0s 56us/step - loss: 1.2078 - acc: 0.4570\n",
      "Epoch 24/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.2092 - acc: 0.4520\n",
      "Epoch 25/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.2027 - acc: 0.4573\n",
      "Epoch 26/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.2003 - acc: 0.4663\n",
      "Epoch 27/300\n",
      "3429/3429 [==============================] - 0s 67us/step - loss: 1.1883 - acc: 0.4652\n",
      "Epoch 28/300\n",
      "3429/3429 [==============================] - 0s 85us/step - loss: 1.1948 - acc: 0.4643\n",
      "Epoch 29/300\n",
      "3429/3429 [==============================] - 0s 64us/step - loss: 1.1883 - acc: 0.4646\n",
      "Epoch 30/300\n",
      "3429/3429 [==============================] - 0s 67us/step - loss: 1.1872 - acc: 0.4681\n",
      "Epoch 31/300\n",
      "3429/3429 [==============================] - 0s 58us/step - loss: 1.1820 - acc: 0.4646\n",
      "Epoch 32/300\n",
      "3429/3429 [==============================] - 0s 54us/step - loss: 1.1735 - acc: 0.4745\n",
      "Epoch 33/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.1754 - acc: 0.4797\n",
      "Epoch 34/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.1803 - acc: 0.4762\n",
      "Epoch 35/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.1731 - acc: 0.4721\n",
      "Epoch 36/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.1745 - acc: 0.4832\n",
      "Epoch 37/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.1771 - acc: 0.4684\n",
      "Epoch 38/300\n",
      "3429/3429 [==============================] - 0s 54us/step - loss: 1.1624 - acc: 0.4756\n",
      "Epoch 39/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.1584 - acc: 0.4818\n",
      "Epoch 40/300\n",
      "3429/3429 [==============================] - 0s 43us/step - loss: 1.1656 - acc: 0.4780\n",
      "Epoch 41/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.1606 - acc: 0.4789\n",
      "Epoch 42/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.1627 - acc: 0.4704\n",
      "Epoch 43/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.1563 - acc: 0.4748\n",
      "Epoch 44/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.1471 - acc: 0.4911\n",
      "Epoch 45/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.1549 - acc: 0.4759\n",
      "Epoch 46/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.1538 - acc: 0.4742\n",
      "Epoch 47/300\n",
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.1534 - acc: 0.4780\n",
      "Epoch 48/300\n",
      "3429/3429 [==============================] - 0s 43us/step - loss: 1.1559 - acc: 0.4771\n",
      "Epoch 49/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.1518 - acc: 0.4812\n",
      "Epoch 50/300\n",
      "3429/3429 [==============================] - 0s 45us/step - loss: 1.1424 - acc: 0.4861\n",
      "Epoch 51/300\n",
      "3429/3429 [==============================] - 0s 55us/step - loss: 1.1456 - acc: 0.4809\n",
      "Epoch 52/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.1431 - acc: 0.4902\n",
      "Epoch 53/300\n",
      "3429/3429 [==============================] - 0s 55us/step - loss: 1.1414 - acc: 0.4934\n",
      "Epoch 54/300\n",
      "3429/3429 [==============================] - 0s 42us/step - loss: 1.1400 - acc: 0.4905\n",
      "Epoch 55/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.1364 - acc: 0.4873\n",
      "Epoch 56/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.1419 - acc: 0.4876\n",
      "Epoch 57/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.1428 - acc: 0.4824\n",
      "Epoch 58/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.1397 - acc: 0.4780\n",
      "Epoch 59/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.1362 - acc: 0.4969\n",
      "Epoch 60/300\n",
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.1242 - acc: 0.4990\n",
      "Epoch 61/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.1319 - acc: 0.5013\n",
      "Epoch 62/300\n",
      "3429/3429 [==============================] - 0s 45us/step - loss: 1.1363 - acc: 0.4850\n",
      "Epoch 63/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.1345 - acc: 0.4952\n",
      "Epoch 64/300\n",
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.1327 - acc: 0.4905\n",
      "Epoch 65/300\n",
      "3429/3429 [==============================] - 0s 57us/step - loss: 1.1329 - acc: 0.4896\n",
      "Epoch 66/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.1249 - acc: 0.4934\n",
      "Epoch 67/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.1322 - acc: 0.4859\n",
      "Epoch 68/300\n",
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.1237 - acc: 0.5025\n",
      "Epoch 69/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.1325 - acc: 0.4958\n",
      "Epoch 70/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.1206 - acc: 0.4984\n",
      "Epoch 71/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.1189 - acc: 0.4984\n",
      "Epoch 72/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.1236 - acc: 0.4891\n",
      "Epoch 73/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.1189 - acc: 0.5066\n",
      "Epoch 74/300\n",
      "3429/3429 [==============================] - 0s 44us/step - loss: 1.1174 - acc: 0.5036\n",
      "Epoch 75/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.1247 - acc: 0.5025\n",
      "Epoch 76/300\n",
      "3429/3429 [==============================] - 0s 43us/step - loss: 1.1190 - acc: 0.4955\n",
      "Epoch 77/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.1240 - acc: 0.4859\n",
      "Epoch 78/300\n",
      "3429/3429 [==============================] - 0s 45us/step - loss: 1.1245 - acc: 0.4850\n",
      "Epoch 79/300\n",
      "3429/3429 [==============================] - ETA: 0s - loss: 1.1182 - acc: 0.497 - 0s 45us/step - loss: 1.1165 - acc: 0.5004\n",
      "Epoch 80/300\n",
      "3429/3429 [==============================] - 0s 54us/step - loss: 1.1205 - acc: 0.4888\n",
      "Epoch 81/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.1199 - acc: 0.4987\n",
      "Epoch 82/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.1136 - acc: 0.4981\n",
      "Epoch 83/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.1157 - acc: 0.5063\n",
      "Epoch 84/300\n",
      "3429/3429 [==============================] - 0s 64us/step - loss: 1.1168 - acc: 0.4990\n",
      "Epoch 85/300\n",
      "3429/3429 [==============================] - 0s 67us/step - loss: 1.1095 - acc: 0.5054\n",
      "Epoch 86/300\n",
      "3429/3429 [==============================] - 0s 45us/step - loss: 1.1164 - acc: 0.4937\n",
      "Epoch 87/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.1128 - acc: 0.5083\n",
      "Epoch 88/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.1092 - acc: 0.5124\n",
      "Epoch 89/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.1102 - acc: 0.5004\n",
      "Epoch 90/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.1147 - acc: 0.5013\n",
      "Epoch 91/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.1043 - acc: 0.5063\n",
      "Epoch 92/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.1118 - acc: 0.4999\n",
      "Epoch 93/300\n",
      "3429/3429 [==============================] - 0s 59us/step - loss: 1.1081 - acc: 0.5165\n",
      "Epoch 94/300\n",
      "3429/3429 [==============================] - 0s 57us/step - loss: 1.1035 - acc: 0.5025\n",
      "Epoch 95/300\n",
      "3429/3429 [==============================] - 0s 59us/step - loss: 1.1071 - acc: 0.5031\n",
      "Epoch 96/300\n",
      "3429/3429 [==============================] - 0s 64us/step - loss: 1.1139 - acc: 0.4987\n",
      "Epoch 97/300\n",
      "3429/3429 [==============================] - 0s 60us/step - loss: 1.1104 - acc: 0.4987\n",
      "Epoch 98/300\n",
      "3429/3429 [==============================] - 0s 64us/step - loss: 1.1056 - acc: 0.5016\n",
      "Epoch 99/300\n",
      "3429/3429 [==============================] - 0s 103us/step - loss: 1.1087 - acc: 0.5141\n",
      "Epoch 100/300\n",
      "3429/3429 [==============================] - 0s 99us/step - loss: 1.1100 - acc: 0.5036\n",
      "Epoch 101/300\n",
      "3429/3429 [==============================] - 0s 58us/step - loss: 1.1043 - acc: 0.4996\n",
      "Epoch 102/300\n",
      "3429/3429 [==============================] - 0s 45us/step - loss: 1.1051 - acc: 0.5077\n",
      "Epoch 103/300\n",
      "3429/3429 [==============================] - 0s 77us/step - loss: 1.1054 - acc: 0.5051\n",
      "Epoch 104/300\n",
      "3429/3429 [==============================] - 0s 65us/step - loss: 1.1030 - acc: 0.5074\n",
      "Epoch 105/300\n",
      "3429/3429 [==============================] - 0s 66us/step - loss: 1.1056 - acc: 0.5025\n",
      "Epoch 106/300\n",
      "3429/3429 [==============================] - 0s 59us/step - loss: 1.1077 - acc: 0.5098\n",
      "Epoch 107/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.1019 - acc: 0.5063\n",
      "Epoch 108/300\n",
      "3429/3429 [==============================] - 0s 44us/step - loss: 1.1002 - acc: 0.5104\n",
      "Epoch 109/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.1033 - acc: 0.5001\n",
      "Epoch 110/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.1006 - acc: 0.5071\n",
      "Epoch 111/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.0996 - acc: 0.5034\n",
      "Epoch 112/300\n",
      "3429/3429 [==============================] - 0s 44us/step - loss: 1.1009 - acc: 0.5115\n",
      "Epoch 113/300\n",
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.0992 - acc: 0.5077\n",
      "Epoch 114/300\n",
      "3429/3429 [==============================] - 0s 43us/step - loss: 1.0957 - acc: 0.5147\n",
      "Epoch 115/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0989 - acc: 0.5086\n",
      "Epoch 116/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.1025 - acc: 0.5069\n",
      "Epoch 117/300\n",
      "3429/3429 [==============================] - 0s 56us/step - loss: 1.1101 - acc: 0.5039\n",
      "Epoch 118/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.0995 - acc: 0.5077\n",
      "Epoch 119/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.1008 - acc: 0.5016\n",
      "Epoch 120/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.1005 - acc: 0.5066\n",
      "Epoch 121/300\n",
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.0973 - acc: 0.5028\n",
      "Epoch 122/300\n",
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.0966 - acc: 0.5144\n",
      "Epoch 123/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0950 - acc: 0.5112\n",
      "Epoch 124/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.1031 - acc: 0.5074\n",
      "Epoch 125/300\n",
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.0938 - acc: 0.5034\n",
      "Epoch 126/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.1008 - acc: 0.5086\n",
      "Epoch 127/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.0922 - acc: 0.5133\n",
      "Epoch 128/300\n",
      "3429/3429 [==============================] - 0s 65us/step - loss: 1.0963 - acc: 0.5109\n",
      "Epoch 129/300\n",
      "3429/3429 [==============================] - 0s 61us/step - loss: 1.0953 - acc: 0.5144\n",
      "Epoch 130/300\n",
      "3429/3429 [==============================] - 0s 60us/step - loss: 1.0939 - acc: 0.5115\n",
      "Epoch 131/300\n",
      "3429/3429 [==============================] - 0s 69us/step - loss: 1.0975 - acc: 0.5083\n",
      "Epoch 132/300\n",
      "3429/3429 [==============================] - 0s 67us/step - loss: 1.0942 - acc: 0.5086\n",
      "Epoch 133/300\n",
      "3429/3429 [==============================] - 0s 69us/step - loss: 1.0941 - acc: 0.5153\n",
      "Epoch 134/300\n",
      "3429/3429 [==============================] - 0s 62us/step - loss: 1.0976 - acc: 0.5092\n",
      "Epoch 135/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0881 - acc: 0.5159\n",
      "Epoch 136/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.0878 - acc: 0.5071\n",
      "Epoch 137/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.0963 - acc: 0.5104\n",
      "Epoch 138/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.0933 - acc: 0.5127\n",
      "Epoch 139/300\n",
      "3429/3429 [==============================] - 0s 54us/step - loss: 1.0936 - acc: 0.5010\n",
      "Epoch 140/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0868 - acc: 0.5133\n",
      "Epoch 141/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.0852 - acc: 0.5162\n",
      "Epoch 142/300\n",
      "3429/3429 [==============================] - 0s 44us/step - loss: 1.0853 - acc: 0.5174\n",
      "Epoch 143/300\n",
      "3429/3429 [==============================] - 0s 44us/step - loss: 1.0939 - acc: 0.5051\n",
      "Epoch 144/300\n",
      "3429/3429 [==============================] - 0s 54us/step - loss: 1.0859 - acc: 0.5191\n",
      "Epoch 145/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0873 - acc: 0.5144\n",
      "Epoch 146/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.0845 - acc: 0.5165\n",
      "Epoch 147/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.0808 - acc: 0.5121\n",
      "Epoch 148/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.0906 - acc: 0.5063\n",
      "Epoch 149/300\n",
      "3429/3429 [==============================] - 0s 59us/step - loss: 1.0869 - acc: 0.5039\n",
      "Epoch 150/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0856 - acc: 0.5098\n",
      "Epoch 151/300\n",
      "3429/3429 [==============================] - 0s 56us/step - loss: 1.0824 - acc: 0.5121\n",
      "Epoch 152/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0823 - acc: 0.5118\n",
      "Epoch 153/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.0884 - acc: 0.5066\n",
      "Epoch 154/300\n",
      "3429/3429 [==============================] - 0s 69us/step - loss: 1.0843 - acc: 0.5159\n",
      "Epoch 155/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.0788 - acc: 0.5104\n",
      "Epoch 156/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.0770 - acc: 0.5290\n",
      "Epoch 157/300\n",
      "3429/3429 [==============================] - 0s 58us/step - loss: 1.0842 - acc: 0.5153\n",
      "Epoch 158/300\n",
      "3429/3429 [==============================] - 0s 40us/step - loss: 1.0774 - acc: 0.5174\n",
      "Epoch 159/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0770 - acc: 0.5182\n",
      "Epoch 160/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.0756 - acc: 0.5281\n",
      "Epoch 161/300\n",
      "3429/3429 [==============================] - 0s 45us/step - loss: 1.0810 - acc: 0.5144\n",
      "Epoch 162/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.0782 - acc: 0.5150\n",
      "Epoch 163/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0783 - acc: 0.5232\n",
      "Epoch 164/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.0750 - acc: 0.5185\n",
      "Epoch 165/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.0809 - acc: 0.5185\n",
      "Epoch 166/300\n",
      "3429/3429 [==============================] - 0s 64us/step - loss: 1.0779 - acc: 0.5106\n",
      "Epoch 167/300\n",
      "3429/3429 [==============================] - 0s 58us/step - loss: 1.0798 - acc: 0.5121\n",
      "Epoch 168/300\n",
      "3429/3429 [==============================] - 0s 63us/step - loss: 1.0771 - acc: 0.5130\n",
      "Epoch 169/300\n",
      "3429/3429 [==============================] - 0s 63us/step - loss: 1.0681 - acc: 0.5136\n",
      "Epoch 170/300\n",
      "3429/3429 [==============================] - 0s 63us/step - loss: 1.0745 - acc: 0.5185\n",
      "Epoch 171/300\n",
      "3429/3429 [==============================] - 0s 73us/step - loss: 1.0810 - acc: 0.5121\n",
      "Epoch 172/300\n",
      "3429/3429 [==============================] - 0s 70us/step - loss: 1.0761 - acc: 0.5252\n",
      "Epoch 173/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.0745 - acc: 0.5077\n",
      "Epoch 174/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0703 - acc: 0.5249\n",
      "Epoch 175/300\n",
      "3429/3429 [==============================] - 0s 69us/step - loss: 1.0793 - acc: 0.5176\n",
      "Epoch 176/300\n",
      "3429/3429 [==============================] - 0s 60us/step - loss: 1.0712 - acc: 0.5281\n",
      "Epoch 177/300\n",
      "3429/3429 [==============================] - 0s 43us/step - loss: 1.0658 - acc: 0.5168\n",
      "Epoch 178/300\n",
      "3429/3429 [==============================] - 0s 54us/step - loss: 1.0689 - acc: 0.5171\n",
      "Epoch 179/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.0736 - acc: 0.5246\n",
      "Epoch 180/300\n",
      "3429/3429 [==============================] - 0s 55us/step - loss: 1.0708 - acc: 0.5229\n",
      "Epoch 181/300\n",
      "3429/3429 [==============================] - 0s 40us/step - loss: 1.0705 - acc: 0.5238\n",
      "Epoch 182/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0724 - acc: 0.5206\n",
      "Epoch 183/300\n",
      "3429/3429 [==============================] - 0s 54us/step - loss: 1.0681 - acc: 0.5287\n",
      "Epoch 184/300\n",
      "3429/3429 [==============================] - 0s 54us/step - loss: 1.0738 - acc: 0.5276\n",
      "Epoch 185/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.0715 - acc: 0.5214\n",
      "Epoch 186/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.0710 - acc: 0.5200\n",
      "Epoch 187/300\n",
      "3429/3429 [==============================] - 0s 55us/step - loss: 1.0649 - acc: 0.5203\n",
      "Epoch 188/300\n",
      "3429/3429 [==============================] - 0s 61us/step - loss: 1.0744 - acc: 0.5185\n",
      "Epoch 189/300\n",
      "3429/3429 [==============================] - 0s 42us/step - loss: 1.0649 - acc: 0.5226\n",
      "Epoch 190/300\n",
      "3429/3429 [==============================] - 0s 55us/step - loss: 1.0698 - acc: 0.5244\n",
      "Epoch 191/300\n",
      "3429/3429 [==============================] - 0s 41us/step - loss: 1.0655 - acc: 0.5194\n",
      "Epoch 192/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.0630 - acc: 0.5223\n",
      "Epoch 193/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.0668 - acc: 0.5159\n",
      "Epoch 194/300\n",
      "3429/3429 [==============================] - 0s 67us/step - loss: 1.0683 - acc: 0.5232\n",
      "Epoch 195/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0619 - acc: 0.5174: 0s - loss: 1.0586 - acc: 0.51\n",
      "Epoch 196/300\n",
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.0690 - acc: 0.5267\n",
      "Epoch 197/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.0682 - acc: 0.5314\n",
      "Epoch 198/300\n",
      "3429/3429 [==============================] - 0s 58us/step - loss: 1.0655 - acc: 0.5217\n",
      "Epoch 199/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.0677 - acc: 0.5197\n",
      "Epoch 200/300\n",
      "3429/3429 [==============================] - 0s 42us/step - loss: 1.0650 - acc: 0.5244\n",
      "Epoch 201/300\n",
      "3429/3429 [==============================] - 0s 45us/step - loss: 1.0552 - acc: 0.5241\n",
      "Epoch 202/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.0752 - acc: 0.5042\n",
      "Epoch 203/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.0575 - acc: 0.5182\n",
      "Epoch 204/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0634 - acc: 0.5273\n",
      "Epoch 205/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.0642 - acc: 0.5281\n",
      "Epoch 206/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.0643 - acc: 0.5258\n",
      "Epoch 207/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.0590 - acc: 0.5133\n",
      "Epoch 208/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.0691 - acc: 0.5249\n",
      "Epoch 209/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.0564 - acc: 0.5197\n",
      "Epoch 210/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0596 - acc: 0.5284\n",
      "Epoch 211/300\n",
      "3429/3429 [==============================] - 0s 44us/step - loss: 1.0641 - acc: 0.5188\n",
      "Epoch 212/300\n",
      "3429/3429 [==============================] - 0s 55us/step - loss: 1.0612 - acc: 0.5281\n",
      "Epoch 213/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0643 - acc: 0.5185\n",
      "Epoch 214/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.0566 - acc: 0.5276\n",
      "Epoch 215/300\n",
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.0572 - acc: 0.5249\n",
      "Epoch 216/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.0599 - acc: 0.5267\n",
      "Epoch 217/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.0560 - acc: 0.5293\n",
      "Epoch 218/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0599 - acc: 0.5174\n",
      "Epoch 219/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.0580 - acc: 0.5244\n",
      "Epoch 220/300\n",
      "3429/3429 [==============================] - 0s 45us/step - loss: 1.0632 - acc: 0.5214\n",
      "Epoch 221/300\n",
      "3429/3429 [==============================] - 0s 45us/step - loss: 1.0558 - acc: 0.5276\n",
      "Epoch 222/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0524 - acc: 0.5235\n",
      "Epoch 223/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.0673 - acc: 0.5226\n",
      "Epoch 224/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.0579 - acc: 0.5305\n",
      "Epoch 225/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0547 - acc: 0.5281\n",
      "Epoch 226/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0577 - acc: 0.5276\n",
      "Epoch 227/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.0586 - acc: 0.5176\n",
      "Epoch 228/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.0583 - acc: 0.5328\n",
      "Epoch 229/300\n",
      "3429/3429 [==============================] - 0s 44us/step - loss: 1.0525 - acc: 0.5252\n",
      "Epoch 230/300\n",
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.0534 - acc: 0.5299\n",
      "Epoch 231/300\n",
      "3429/3429 [==============================] - 0s 55us/step - loss: 1.0558 - acc: 0.5281\n",
      "Epoch 232/300\n",
      "3429/3429 [==============================] - 0s 58us/step - loss: 1.0555 - acc: 0.5340\n",
      "Epoch 233/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.0571 - acc: 0.5296\n",
      "Epoch 234/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.0634 - acc: 0.5206\n",
      "Epoch 235/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.0495 - acc: 0.5308\n",
      "Epoch 236/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.0536 - acc: 0.5296\n",
      "Epoch 237/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0544 - acc: 0.5246\n",
      "Epoch 238/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.0524 - acc: 0.5290\n",
      "Epoch 239/300\n",
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.0620 - acc: 0.5174\n",
      "Epoch 240/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.0511 - acc: 0.5322\n",
      "Epoch 241/300\n",
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.0531 - acc: 0.5340\n",
      "Epoch 242/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0526 - acc: 0.5302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 243/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.0531 - acc: 0.5194\n",
      "Epoch 244/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0533 - acc: 0.5273\n",
      "Epoch 245/300\n",
      "3429/3429 [==============================] - 0s 63us/step - loss: 1.0519 - acc: 0.5346\n",
      "Epoch 246/300\n",
      "3429/3429 [==============================] - 0s 72us/step - loss: 1.0533 - acc: 0.5279\n",
      "Epoch 247/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.0497 - acc: 0.5372\n",
      "Epoch 248/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0494 - acc: 0.5308\n",
      "Epoch 249/300\n",
      "3429/3429 [==============================] - 0s 55us/step - loss: 1.0541 - acc: 0.5375\n",
      "Epoch 250/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.0502 - acc: 0.5363\n",
      "Epoch 251/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0491 - acc: 0.5264\n",
      "Epoch 252/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0507 - acc: 0.5223\n",
      "Epoch 253/300\n",
      "3429/3429 [==============================] - 0s 38us/step - loss: 1.0533 - acc: 0.5340\n",
      "Epoch 254/300\n",
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.0497 - acc: 0.5293\n",
      "Epoch 255/300\n",
      "3429/3429 [==============================] - 0s 43us/step - loss: 1.0506 - acc: 0.5264\n",
      "Epoch 256/300\n",
      "3429/3429 [==============================] - 0s 42us/step - loss: 1.0523 - acc: 0.5346\n",
      "Epoch 257/300\n",
      "3429/3429 [==============================] - 0s 39us/step - loss: 1.0494 - acc: 0.5314\n",
      "Epoch 258/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0560 - acc: 0.5305\n",
      "Epoch 259/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.0400 - acc: 0.5334\n",
      "Epoch 260/300\n",
      "3429/3429 [==============================] - 0s 43us/step - loss: 1.0498 - acc: 0.5293\n",
      "Epoch 261/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.0460 - acc: 0.5340\n",
      "Epoch 262/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.0491 - acc: 0.5296\n",
      "Epoch 263/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.0488 - acc: 0.5308\n",
      "Epoch 264/300\n",
      "3429/3429 [==============================] - 0s 54us/step - loss: 1.0513 - acc: 0.5299\n",
      "Epoch 265/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.0520 - acc: 0.5255\n",
      "Epoch 266/300\n",
      "3429/3429 [==============================] - 0s 54us/step - loss: 1.0448 - acc: 0.5389\n",
      "Epoch 267/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.0510 - acc: 0.5293\n",
      "Epoch 268/300\n",
      "3429/3429 [==============================] - 0s 61us/step - loss: 1.0489 - acc: 0.5267: 0s - loss: 1.0418 - acc: 0.522\n",
      "Epoch 269/300\n",
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.0494 - acc: 0.5261\n",
      "Epoch 270/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.0505 - acc: 0.5223\n",
      "Epoch 271/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.0469 - acc: 0.5302\n",
      "Epoch 272/300\n",
      "3429/3429 [==============================] - 0s 39us/step - loss: 1.0461 - acc: 0.5284\n",
      "Epoch 273/300\n",
      "3429/3429 [==============================] - 0s 37us/step - loss: 1.0467 - acc: 0.5293\n",
      "Epoch 274/300\n",
      "3429/3429 [==============================] - 0s 31us/step - loss: 1.0511 - acc: 0.5244\n",
      "Epoch 275/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.0539 - acc: 0.5214\n",
      "Epoch 276/300\n",
      "3429/3429 [==============================] - 0s 53us/step - loss: 1.0487 - acc: 0.5270\n",
      "Epoch 277/300\n",
      "3429/3429 [==============================] - 0s 45us/step - loss: 1.0397 - acc: 0.5331\n",
      "Epoch 278/300\n",
      "3429/3429 [==============================] - 0s 45us/step - loss: 1.0490 - acc: 0.5258\n",
      "Epoch 279/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0448 - acc: 0.5241\n",
      "Epoch 280/300\n",
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.0446 - acc: 0.5316\n",
      "Epoch 281/300\n",
      "3429/3429 [==============================] - 0s 51us/step - loss: 1.0400 - acc: 0.5276\n",
      "Epoch 282/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0433 - acc: 0.5369\n",
      "Epoch 283/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0424 - acc: 0.5401\n",
      "Epoch 284/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0439 - acc: 0.5340\n",
      "Epoch 285/300\n",
      "3429/3429 [==============================] - 0s 56us/step - loss: 1.0435 - acc: 0.5366\n",
      "Epoch 286/300\n",
      "3429/3429 [==============================] - 0s 49us/step - loss: 1.0412 - acc: 0.5346\n",
      "Epoch 287/300\n",
      "3429/3429 [==============================] - 0s 46us/step - loss: 1.0429 - acc: 0.5314\n",
      "Epoch 288/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.0453 - acc: 0.5328\n",
      "Epoch 289/300\n",
      "3429/3429 [==============================] - 0s 55us/step - loss: 1.0462 - acc: 0.5305\n",
      "Epoch 290/300\n",
      "3429/3429 [==============================] - 0s 52us/step - loss: 1.0408 - acc: 0.5331\n",
      "Epoch 291/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.0440 - acc: 0.5378\n",
      "Epoch 292/300\n",
      "3429/3429 [==============================] - 0s 50us/step - loss: 1.0398 - acc: 0.5316\n",
      "Epoch 293/300\n",
      "3429/3429 [==============================] - 0s 47us/step - loss: 1.0418 - acc: 0.5334\n",
      "Epoch 294/300\n",
      "3429/3429 [==============================] - 0s 45us/step - loss: 1.0433 - acc: 0.5340\n",
      "Epoch 295/300\n",
      "3429/3429 [==============================] - 0s 45us/step - loss: 1.0359 - acc: 0.5314\n",
      "Epoch 296/300\n",
      "3429/3429 [==============================] - 0s 48us/step - loss: 1.0421 - acc: 0.5325\n",
      "Epoch 297/300\n",
      "3429/3429 [==============================] - 0s 43us/step - loss: 1.0412 - acc: 0.5381\n",
      "Epoch 298/300\n",
      "3429/3429 [==============================] - 0s 70us/step - loss: 1.0442 - acc: 0.5273\n",
      "Epoch 299/300\n",
      "3429/3429 [==============================] - 0s 65us/step - loss: 1.0378 - acc: 0.5255\n",
      "Epoch 300/300\n",
      "3429/3429 [==============================] - 0s 55us/step - loss: 1.0378 - acc: 0.5348\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "eveluate loss:1.2187235569613089, evaluate acc:0.47106875384884517\n",
      "(1119, 9)\n",
      "Epoch 1/250\n",
      "1119/1119 [==============================] - 2s 1ms/step - loss: 4.9291 - acc: 0.1555  \n",
      "Epoch 2/250\n",
      "1119/1119 [==============================] - 0s 39us/step - loss: 1.5188 - acc: 0.4879\n",
      "Epoch 3/250\n",
      "1119/1119 [==============================] - 0s 51us/step - loss: 1.2933 - acc: 0.5040\n",
      "Epoch 4/250\n",
      "1119/1119 [==============================] - 0s 44us/step - loss: 1.2121 - acc: 0.5085\n",
      "Epoch 5/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 1.1721 - acc: 0.5156\n",
      "Epoch 6/250\n",
      "1119/1119 [==============================] - 0s 46us/step - loss: 1.1537 - acc: 0.4978\n",
      "Epoch 7/250\n",
      "1119/1119 [==============================] - 0s 54us/step - loss: 1.1495 - acc: 0.5013\n",
      "Epoch 8/250\n",
      "1119/1119 [==============================] - 0s 57us/step - loss: 1.1378 - acc: 0.5228\n",
      "Epoch 9/250\n",
      "1119/1119 [==============================] - 0s 58us/step - loss: 1.1419 - acc: 0.5201\n",
      "Epoch 10/250\n",
      "1119/1119 [==============================] - 0s 64us/step - loss: 1.1244 - acc: 0.5201\n",
      "Epoch 11/250\n",
      "1119/1119 [==============================] - 0s 52us/step - loss: 1.1225 - acc: 0.5174\n",
      "Epoch 12/250\n",
      "1119/1119 [==============================] - 0s 59us/step - loss: 1.1227 - acc: 0.5246\n",
      "Epoch 13/250\n",
      "1119/1119 [==============================] - 0s 60us/step - loss: 1.1188 - acc: 0.5201\n",
      "Epoch 14/250\n",
      "1119/1119 [==============================] - 0s 50us/step - loss: 1.1282 - acc: 0.5139\n",
      "Epoch 15/250\n",
      "1119/1119 [==============================] - 0s 61us/step - loss: 1.1119 - acc: 0.5335\n",
      "Epoch 16/250\n",
      "1119/1119 [==============================] - 0s 58us/step - loss: 1.1069 - acc: 0.5219\n",
      "Epoch 17/250\n",
      "1119/1119 [==============================] - 0s 62us/step - loss: 1.1052 - acc: 0.5273\n",
      "Epoch 18/250\n",
      "1119/1119 [==============================] - 0s 51us/step - loss: 1.1127 - acc: 0.5308\n",
      "Epoch 19/250\n",
      "1119/1119 [==============================] - 0s 45us/step - loss: 1.1028 - acc: 0.5246\n",
      "Epoch 20/250\n",
      "1119/1119 [==============================] - 0s 43us/step - loss: 1.1112 - acc: 0.5299\n",
      "Epoch 21/250\n",
      "1119/1119 [==============================] - 0s 63us/step - loss: 1.0969 - acc: 0.5273\n",
      "Epoch 22/250\n",
      "1119/1119 [==============================] - 0s 62us/step - loss: 1.0951 - acc: 0.5282\n",
      "Epoch 23/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 1.0928 - acc: 0.5371\n",
      "Epoch 24/250\n",
      "1119/1119 [==============================] - 0s 41us/step - loss: 1.1036 - acc: 0.5290\n",
      "Epoch 25/250\n",
      "1119/1119 [==============================] - 0s 51us/step - loss: 1.0954 - acc: 0.5398\n",
      "Epoch 26/250\n",
      "1119/1119 [==============================] - 0s 54us/step - loss: 1.0839 - acc: 0.5353\n",
      "Epoch 27/250\n",
      "1119/1119 [==============================] - 0s 59us/step - loss: 1.0882 - acc: 0.5273\n",
      "Epoch 28/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 1.0963 - acc: 0.5264\n",
      "Epoch 29/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 1.0807 - acc: 0.5398\n",
      "Epoch 30/250\n",
      "1119/1119 [==============================] - 0s 46us/step - loss: 1.0773 - acc: 0.5433\n",
      "Epoch 31/250\n",
      "1119/1119 [==============================] - 0s 61us/step - loss: 1.0865 - acc: 0.5317\n",
      "Epoch 32/250\n",
      "1119/1119 [==============================] - 0s 58us/step - loss: 1.0751 - acc: 0.5308\n",
      "Epoch 33/250\n",
      "1119/1119 [==============================] - 0s 50us/step - loss: 1.0764 - acc: 0.5353\n",
      "Epoch 34/250\n",
      "1119/1119 [==============================] - 0s 59us/step - loss: 1.0791 - acc: 0.5416\n",
      "Epoch 35/250\n",
      "1119/1119 [==============================] - 0s 62us/step - loss: 1.0726 - acc: 0.5433\n",
      "Epoch 36/250\n",
      "1119/1119 [==============================] - 0s 49us/step - loss: 1.0701 - acc: 0.5380\n",
      "Epoch 37/250\n",
      "1119/1119 [==============================] - 0s 54us/step - loss: 1.0656 - acc: 0.5398\n",
      "Epoch 38/250\n",
      "1119/1119 [==============================] - 0s 51us/step - loss: 1.0706 - acc: 0.5523\n",
      "Epoch 39/250\n",
      "1119/1119 [==============================] - 0s 58us/step - loss: 1.0691 - acc: 0.5389\n",
      "Epoch 40/250\n",
      "1119/1119 [==============================] - 0s 53us/step - loss: 1.0670 - acc: 0.5523\n",
      "Epoch 41/250\n",
      "1119/1119 [==============================] - 0s 53us/step - loss: 1.0612 - acc: 0.5469\n",
      "Epoch 42/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 1.0619 - acc: 0.5433\n",
      "Epoch 43/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 1.0534 - acc: 0.5585\n",
      "Epoch 44/250\n",
      "1119/1119 [==============================] - 0s 57us/step - loss: 1.0607 - acc: 0.5407\n",
      "Epoch 45/250\n",
      "1119/1119 [==============================] - 0s 31us/step - loss: 1.0606 - acc: 0.5398\n",
      "Epoch 46/250\n",
      "1119/1119 [==============================] - 0s 41us/step - loss: 1.0533 - acc: 0.5550\n",
      "Epoch 47/250\n",
      "1119/1119 [==============================] - 0s 44us/step - loss: 1.0522 - acc: 0.5567\n",
      "Epoch 48/250\n",
      "1119/1119 [==============================] - 0s 46us/step - loss: 1.0486 - acc: 0.5514\n",
      "Epoch 49/250\n",
      "1119/1119 [==============================] - 0s 45us/step - loss: 1.0464 - acc: 0.5612\n",
      "Epoch 50/250\n",
      "1119/1119 [==============================] - 0s 44us/step - loss: 1.0491 - acc: 0.5460\n",
      "Epoch 51/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 1.0446 - acc: 0.5541\n",
      "Epoch 52/250\n",
      "1119/1119 [==============================] - 0s 56us/step - loss: 1.0431 - acc: 0.5630\n",
      "Epoch 53/250\n",
      "1119/1119 [==============================] - 0s 68us/step - loss: 1.0472 - acc: 0.5576\n",
      "Epoch 54/250\n",
      "1119/1119 [==============================] - 0s 59us/step - loss: 1.0401 - acc: 0.5514\n",
      "Epoch 55/250\n",
      "1119/1119 [==============================] - 0s 56us/step - loss: 1.0459 - acc: 0.5648\n",
      "Epoch 56/250\n",
      "1119/1119 [==============================] - 0s 55us/step - loss: 1.0340 - acc: 0.5559\n",
      "Epoch 57/250\n",
      "1119/1119 [==============================] - 0s 47us/step - loss: 1.0317 - acc: 0.5594\n",
      "Epoch 58/250\n",
      "1119/1119 [==============================] - 0s 47us/step - loss: 1.0287 - acc: 0.5657\n",
      "Epoch 59/250\n",
      "1119/1119 [==============================] - 0s 47us/step - loss: 1.0300 - acc: 0.5746\n",
      "Epoch 60/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 1.0309 - acc: 0.5559\n",
      "Epoch 61/250\n",
      "1119/1119 [==============================] - 0s 51us/step - loss: 1.0271 - acc: 0.5621\n",
      "Epoch 62/250\n",
      "1119/1119 [==============================] - 0s 47us/step - loss: 1.0311 - acc: 0.5693\n",
      "Epoch 63/250\n",
      "1119/1119 [==============================] - 0s 49us/step - loss: 1.0221 - acc: 0.5648\n",
      "Epoch 64/250\n",
      "1119/1119 [==============================] - 0s 45us/step - loss: 1.0252 - acc: 0.5648\n",
      "Epoch 65/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 1.0115 - acc: 0.5702\n",
      "Epoch 66/250\n",
      "1119/1119 [==============================] - 0s 42us/step - loss: 1.0201 - acc: 0.5639\n",
      "Epoch 67/250\n",
      "1119/1119 [==============================] - 0s 51us/step - loss: 1.0155 - acc: 0.5693\n",
      "Epoch 68/250\n",
      "1119/1119 [==============================] - 0s 46us/step - loss: 1.0176 - acc: 0.5702\n",
      "Epoch 69/250\n",
      "1119/1119 [==============================] - 0s 46us/step - loss: 1.0191 - acc: 0.5702\n",
      "Epoch 70/250\n",
      "1119/1119 [==============================] - 0s 49us/step - loss: 1.0128 - acc: 0.5675\n",
      "Epoch 71/250\n",
      "1119/1119 [==============================] - 0s 45us/step - loss: 1.0171 - acc: 0.5862\n",
      "Epoch 72/250\n",
      "1119/1119 [==============================] - 0s 56us/step - loss: 1.0125 - acc: 0.5764\n",
      "Epoch 73/250\n",
      "1119/1119 [==============================] - 0s 59us/step - loss: 1.0138 - acc: 0.5764\n",
      "Epoch 74/250\n",
      "1119/1119 [==============================] - 0s 60us/step - loss: 1.0045 - acc: 0.5782\n",
      "Epoch 75/250\n",
      "1119/1119 [==============================] - 0s 58us/step - loss: 1.0105 - acc: 0.5684\n",
      "Epoch 76/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 1.0128 - acc: 0.5710\n",
      "Epoch 77/250\n",
      "1119/1119 [==============================] - 0s 49us/step - loss: 1.0060 - acc: 0.5567\n",
      "Epoch 78/250\n",
      "1119/1119 [==============================] - 0s 49us/step - loss: 1.0102 - acc: 0.5710\n",
      "Epoch 79/250\n",
      "1119/1119 [==============================] - 0s 50us/step - loss: 1.0018 - acc: 0.5773\n",
      "Epoch 80/250\n",
      "1119/1119 [==============================] - 0s 43us/step - loss: 1.0098 - acc: 0.5737\n",
      "Epoch 81/250\n",
      "1119/1119 [==============================] - 0s 50us/step - loss: 1.0044 - acc: 0.5836\n",
      "Epoch 82/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1119/1119 [==============================] - 0s 57us/step - loss: 1.0077 - acc: 0.5836\n",
      "Epoch 83/250\n",
      "1119/1119 [==============================] - 0s 56us/step - loss: 1.0135 - acc: 0.5737\n",
      "Epoch 84/250\n",
      "1119/1119 [==============================] - 0s 55us/step - loss: 0.9969 - acc: 0.5871\n",
      "Epoch 85/250\n",
      "1119/1119 [==============================] - 0s 50us/step - loss: 0.9959 - acc: 0.5818\n",
      "Epoch 86/250\n",
      "1119/1119 [==============================] - 0s 44us/step - loss: 0.9995 - acc: 0.5621\n",
      "Epoch 87/250\n",
      "1119/1119 [==============================] - 0s 46us/step - loss: 0.9962 - acc: 0.5746\n",
      "Epoch 88/250\n",
      "1119/1119 [==============================] - 0s 43us/step - loss: 0.9888 - acc: 0.5728\n",
      "Epoch 89/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 0.9956 - acc: 0.5755\n",
      "Epoch 90/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 0.9839 - acc: 0.5773\n",
      "Epoch 91/250\n",
      "1119/1119 [==============================] - ETA: 0s - loss: 0.9965 - acc: 0.573 - 0s 59us/step - loss: 0.9954 - acc: 0.5693\n",
      "Epoch 92/250\n",
      "1119/1119 [==============================] - 0s 56us/step - loss: 0.9911 - acc: 0.5836\n",
      "Epoch 93/250\n",
      "1119/1119 [==============================] - 0s 46us/step - loss: 0.9842 - acc: 0.5907\n",
      "Epoch 94/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 0.9957 - acc: 0.5782\n",
      "Epoch 95/250\n",
      "1119/1119 [==============================] - 0s 45us/step - loss: 0.9944 - acc: 0.5836\n",
      "Epoch 96/250\n",
      "1119/1119 [==============================] - 0s 47us/step - loss: 0.9857 - acc: 0.5853\n",
      "Epoch 97/250\n",
      "1119/1119 [==============================] - 0s 55us/step - loss: 0.9736 - acc: 0.6005\n",
      "Epoch 98/250\n",
      "1119/1119 [==============================] - 0s 45us/step - loss: 0.9914 - acc: 0.5827\n",
      "Epoch 99/250\n",
      "1119/1119 [==============================] - 0s 51us/step - loss: 0.9759 - acc: 0.5818\n",
      "Epoch 100/250\n",
      "1119/1119 [==============================] - 0s 50us/step - loss: 0.9828 - acc: 0.5907\n",
      "Epoch 101/250\n",
      "1119/1119 [==============================] - 0s 54us/step - loss: 0.9745 - acc: 0.5791\n",
      "Epoch 102/250\n",
      "1119/1119 [==============================] - 0s 55us/step - loss: 0.9852 - acc: 0.5845\n",
      "Epoch 103/250\n",
      "1119/1119 [==============================] - 0s 57us/step - loss: 0.9760 - acc: 0.5800\n",
      "Epoch 104/250\n",
      "1119/1119 [==============================] - 0s 66us/step - loss: 0.9734 - acc: 0.5898\n",
      "Epoch 105/250\n",
      "1119/1119 [==============================] - 0s 49us/step - loss: 0.9686 - acc: 0.5907\n",
      "Epoch 106/250\n",
      "1119/1119 [==============================] - 0s 54us/step - loss: 0.9723 - acc: 0.5871\n",
      "Epoch 107/250\n",
      "1119/1119 [==============================] - 0s 54us/step - loss: 0.9771 - acc: 0.6005\n",
      "Epoch 108/250\n",
      "1119/1119 [==============================] - 0s 57us/step - loss: 0.9685 - acc: 0.5889\n",
      "Epoch 109/250\n",
      "1119/1119 [==============================] - 0s 69us/step - loss: 0.9760 - acc: 0.5871\n",
      "Epoch 110/250\n",
      "1119/1119 [==============================] - 0s 55us/step - loss: 0.9788 - acc: 0.5836\n",
      "Epoch 111/250\n",
      "1119/1119 [==============================] - 0s 49us/step - loss: 0.9688 - acc: 0.5961\n",
      "Epoch 112/250\n",
      "1119/1119 [==============================] - 0s 46us/step - loss: 0.9779 - acc: 0.5773\n",
      "Epoch 113/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 0.9666 - acc: 0.5987\n",
      "Epoch 114/250\n",
      "1119/1119 [==============================] - 0s 46us/step - loss: 0.9723 - acc: 0.5880\n",
      "Epoch 115/250\n",
      "1119/1119 [==============================] - 0s 46us/step - loss: 0.9696 - acc: 0.5889\n",
      "Epoch 116/250\n",
      "1119/1119 [==============================] - 0s 46us/step - loss: 0.9621 - acc: 0.5853\n",
      "Epoch 117/250\n",
      "1119/1119 [==============================] - 0s 43us/step - loss: 0.9669 - acc: 0.5836\n",
      "Epoch 118/250\n",
      "1119/1119 [==============================] - 0s 61us/step - loss: 0.9639 - acc: 0.5970\n",
      "Epoch 119/250\n",
      "1119/1119 [==============================] - 0s 51us/step - loss: 0.9791 - acc: 0.5684\n",
      "Epoch 120/250\n",
      "1119/1119 [==============================] - 0s 45us/step - loss: 0.9691 - acc: 0.5836\n",
      "Epoch 121/250\n",
      "1119/1119 [==============================] - 0s 55us/step - loss: 0.9672 - acc: 0.5845\n",
      "Epoch 122/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 0.9561 - acc: 0.6005\n",
      "Epoch 123/250\n",
      "1119/1119 [==============================] - 0s 45us/step - loss: 0.9540 - acc: 0.6005\n",
      "Epoch 124/250\n",
      "1119/1119 [==============================] - 0s 49us/step - loss: 0.9594 - acc: 0.5880\n",
      "Epoch 125/250\n",
      "1119/1119 [==============================] - 0s 58us/step - loss: 0.9602 - acc: 0.5889\n",
      "Epoch 126/250\n",
      "1119/1119 [==============================] - 0s 49us/step - loss: 0.9574 - acc: 0.5916\n",
      "Epoch 127/250\n",
      "1119/1119 [==============================] - 0s 63us/step - loss: 0.9675 - acc: 0.5889\n",
      "Epoch 128/250\n",
      "1119/1119 [==============================] - 0s 60us/step - loss: 0.9599 - acc: 0.5907\n",
      "Epoch 129/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 0.9665 - acc: 0.5862\n",
      "Epoch 130/250\n",
      "1119/1119 [==============================] - 0s 49us/step - loss: 0.9550 - acc: 0.5880\n",
      "Epoch 131/250\n",
      "1119/1119 [==============================] - 0s 49us/step - loss: 0.9546 - acc: 0.5925\n",
      "Epoch 132/250\n",
      "1119/1119 [==============================] - 0s 46us/step - loss: 0.9544 - acc: 0.5862\n",
      "Epoch 133/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 0.9642 - acc: 0.5916\n",
      "Epoch 134/250\n",
      "1119/1119 [==============================] - 0s 49us/step - loss: 0.9520 - acc: 0.5952\n",
      "Epoch 135/250\n",
      "1119/1119 [==============================] - 0s 50us/step - loss: 0.9503 - acc: 0.5925\n",
      "Epoch 136/250\n",
      "1119/1119 [==============================] - 0s 47us/step - loss: 0.9640 - acc: 0.5880\n",
      "Epoch 137/250\n",
      "1119/1119 [==============================] - 0s 46us/step - loss: 0.9519 - acc: 0.6041\n",
      "Epoch 138/250\n",
      "1119/1119 [==============================] - 0s 49us/step - loss: 0.9533 - acc: 0.5979\n",
      "Epoch 139/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 0.9523 - acc: 0.5871\n",
      "Epoch 140/250\n",
      "1119/1119 [==============================] - 0s 54us/step - loss: 0.9608 - acc: 0.5889\n",
      "Epoch 141/250\n",
      "1119/1119 [==============================] - 0s 47us/step - loss: 0.9588 - acc: 0.5889\n",
      "Epoch 142/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 0.9518 - acc: 0.5862\n",
      "Epoch 143/250\n",
      "1119/1119 [==============================] - 0s 46us/step - loss: 0.9417 - acc: 0.5987\n",
      "Epoch 144/250\n",
      "1119/1119 [==============================] - 0s 47us/step - loss: 0.9542 - acc: 0.6032\n",
      "Epoch 145/250\n",
      "1119/1119 [==============================] - 0s 57us/step - loss: 0.9459 - acc: 0.5853\n",
      "Epoch 146/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 0.9581 - acc: 0.5898\n",
      "Epoch 147/250\n",
      "1119/1119 [==============================] - 0s 50us/step - loss: 0.9378 - acc: 0.6023\n",
      "Epoch 148/250\n",
      "1119/1119 [==============================] - 0s 52us/step - loss: 0.9570 - acc: 0.5800\n",
      "Epoch 149/250\n",
      "1119/1119 [==============================] - 0s 47us/step - loss: 0.9366 - acc: 0.6014\n",
      "Epoch 150/250\n",
      "1119/1119 [==============================] - 0s 46us/step - loss: 0.9547 - acc: 0.5853\n",
      "Epoch 151/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 0.9468 - acc: 0.5925\n",
      "Epoch 152/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 0.9583 - acc: 0.5889\n",
      "Epoch 153/250\n",
      "1119/1119 [==============================] - 0s 45us/step - loss: 0.9521 - acc: 0.5898\n",
      "Epoch 154/250\n",
      "1119/1119 [==============================] - 0s 50us/step - loss: 0.9434 - acc: 0.5943\n",
      "Epoch 155/250\n",
      "1119/1119 [==============================] - 0s 55us/step - loss: 0.9485 - acc: 0.5952\n",
      "Epoch 156/250\n",
      "1119/1119 [==============================] - 0s 50us/step - loss: 0.9387 - acc: 0.5979\n",
      "Epoch 157/250\n",
      "1119/1119 [==============================] - 0s 47us/step - loss: 0.9453 - acc: 0.5862\n",
      "Epoch 158/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 0.9384 - acc: 0.5862\n",
      "Epoch 159/250\n",
      "1119/1119 [==============================] - 0s 47us/step - loss: 0.9462 - acc: 0.5809\n",
      "Epoch 160/250\n",
      "1119/1119 [==============================] - 0s 52us/step - loss: 0.9369 - acc: 0.5987\n",
      "Epoch 161/250\n",
      "1119/1119 [==============================] - 0s 47us/step - loss: 0.9338 - acc: 0.6068\n",
      "Epoch 162/250\n",
      "1119/1119 [==============================] - 0s 53us/step - loss: 0.9424 - acc: 0.5961\n",
      "Epoch 163/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1119/1119 [==============================] - 0s 59us/step - loss: 0.9517 - acc: 0.5845\n",
      "Epoch 164/250\n",
      "1119/1119 [==============================] - 0s 57us/step - loss: 0.9356 - acc: 0.6005\n",
      "Epoch 165/250\n",
      "1119/1119 [==============================] - 0s 52us/step - loss: 0.9411 - acc: 0.5853\n",
      "Epoch 166/250\n",
      "1119/1119 [==============================] - 0s 54us/step - loss: 0.9343 - acc: 0.6086\n",
      "Epoch 167/250\n",
      "1119/1119 [==============================] - 0s 51us/step - loss: 0.9359 - acc: 0.5943\n",
      "Epoch 168/250\n",
      "1119/1119 [==============================] - 0s 44us/step - loss: 0.9384 - acc: 0.6122\n",
      "Epoch 169/250\n",
      "1119/1119 [==============================] - 0s 47us/step - loss: 0.9335 - acc: 0.5943\n",
      "Epoch 170/250\n",
      "1119/1119 [==============================] - 0s 50us/step - loss: 0.9357 - acc: 0.5836\n",
      "Epoch 171/250\n",
      "1119/1119 [==============================] - 0s 45us/step - loss: 0.9197 - acc: 0.6139\n",
      "Epoch 172/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 0.9504 - acc: 0.6050\n",
      "Epoch 173/250\n",
      "1119/1119 [==============================] - 0s 45us/step - loss: 0.9348 - acc: 0.6041\n",
      "Epoch 174/250\n",
      "1119/1119 [==============================] - 0s 53us/step - loss: 0.9311 - acc: 0.5907\n",
      "Epoch 175/250\n",
      "1119/1119 [==============================] - 0s 47us/step - loss: 0.9405 - acc: 0.5952\n",
      "Epoch 176/250\n",
      "1119/1119 [==============================] - 0s 49us/step - loss: 0.9356 - acc: 0.6023\n",
      "Epoch 177/250\n",
      "1119/1119 [==============================] - 0s 49us/step - loss: 0.9250 - acc: 0.5943\n",
      "Epoch 178/250\n",
      "1119/1119 [==============================] - 0s 50us/step - loss: 0.9337 - acc: 0.5961\n",
      "Epoch 179/250\n",
      "1119/1119 [==============================] - 0s 45us/step - loss: 0.9294 - acc: 0.5916\n",
      "Epoch 180/250\n",
      "1119/1119 [==============================] - 0s 51us/step - loss: 0.9301 - acc: 0.6041\n",
      "Epoch 181/250\n",
      "1119/1119 [==============================] - 0s 54us/step - loss: 0.9227 - acc: 0.6068\n",
      "Epoch 182/250\n",
      "1119/1119 [==============================] - 0s 45us/step - loss: 0.9334 - acc: 0.6122\n",
      "Epoch 183/250\n",
      "1119/1119 [==============================] - 0s 50us/step - loss: 0.9271 - acc: 0.6023\n",
      "Epoch 184/250\n",
      "1119/1119 [==============================] - 0s 51us/step - loss: 0.9281 - acc: 0.6041\n",
      "Epoch 185/250\n",
      "1119/1119 [==============================] - 0s 52us/step - loss: 0.9263 - acc: 0.6032\n",
      "Epoch 186/250\n",
      "1119/1119 [==============================] - 0s 56us/step - loss: 0.9258 - acc: 0.5970\n",
      "Epoch 187/250\n",
      "1119/1119 [==============================] - 0s 44us/step - loss: 0.9275 - acc: 0.6059\n",
      "Epoch 188/250\n",
      "1119/1119 [==============================] - 0s 53us/step - loss: 0.9333 - acc: 0.5996\n",
      "Epoch 189/250\n",
      "1119/1119 [==============================] - 0s 53us/step - loss: 0.9166 - acc: 0.6086\n",
      "Epoch 190/250\n",
      "1119/1119 [==============================] - 0s 51us/step - loss: 0.9167 - acc: 0.5979\n",
      "Epoch 191/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 0.9308 - acc: 0.5970\n",
      "Epoch 192/250\n",
      "1119/1119 [==============================] - 0s 50us/step - loss: 0.9216 - acc: 0.6032\n",
      "Epoch 193/250\n",
      "1119/1119 [==============================] - 0s 50us/step - loss: 0.9216 - acc: 0.6095\n",
      "Epoch 194/250\n",
      "1119/1119 [==============================] - 0s 46us/step - loss: 0.9230 - acc: 0.6050\n",
      "Epoch 195/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 0.9191 - acc: 0.6130\n",
      "Epoch 196/250\n",
      "1119/1119 [==============================] - 0s 47us/step - loss: 0.9308 - acc: 0.5934\n",
      "Epoch 197/250\n",
      "1119/1119 [==============================] - 0s 47us/step - loss: 0.9244 - acc: 0.6032\n",
      "Epoch 198/250\n",
      "1119/1119 [==============================] - 0s 47us/step - loss: 0.9168 - acc: 0.6077\n",
      "Epoch 199/250\n",
      "1119/1119 [==============================] - 0s 47us/step - loss: 0.9256 - acc: 0.5943\n",
      "Epoch 200/250\n",
      "1119/1119 [==============================] - 0s 51us/step - loss: 0.9264 - acc: 0.5970\n",
      "Epoch 201/250\n",
      "1119/1119 [==============================] - 0s 54us/step - loss: 0.9136 - acc: 0.6113\n",
      "Epoch 202/250\n",
      "1119/1119 [==============================] - 0s 46us/step - loss: 0.9231 - acc: 0.6014\n",
      "Epoch 203/250\n",
      "1119/1119 [==============================] - 0s 50us/step - loss: 0.9219 - acc: 0.5987\n",
      "Epoch 204/250\n",
      "1119/1119 [==============================] - 0s 52us/step - loss: 0.9077 - acc: 0.6122\n",
      "Epoch 205/250\n",
      "1119/1119 [==============================] - 0s 55us/step - loss: 0.9186 - acc: 0.6014\n",
      "Epoch 206/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 0.9293 - acc: 0.5970\n",
      "Epoch 207/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 0.9182 - acc: 0.6032\n",
      "Epoch 208/250\n",
      "1119/1119 [==============================] - 0s 46us/step - loss: 0.9131 - acc: 0.6086\n",
      "Epoch 209/250\n",
      "1119/1119 [==============================] - 0s 60us/step - loss: 0.9162 - acc: 0.6247\n",
      "Epoch 210/250\n",
      "1119/1119 [==============================] - 0s 49us/step - loss: 0.9192 - acc: 0.5987\n",
      "Epoch 211/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 0.9184 - acc: 0.6059\n",
      "Epoch 212/250\n",
      "1119/1119 [==============================] - 0s 47us/step - loss: 0.9086 - acc: 0.5979\n",
      "Epoch 213/250\n",
      "1119/1119 [==============================] - 0s 46us/step - loss: 0.9102 - acc: 0.6005\n",
      "Epoch 214/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 0.9149 - acc: 0.6050\n",
      "Epoch 215/250\n",
      "1119/1119 [==============================] - 0s 50us/step - loss: 0.9055 - acc: 0.6220\n",
      "Epoch 216/250\n",
      "1119/1119 [==============================] - 0s 59us/step - loss: 0.9219 - acc: 0.6032\n",
      "Epoch 217/250\n",
      "1119/1119 [==============================] - 0s 55us/step - loss: 0.9123 - acc: 0.5961\n",
      "Epoch 218/250\n",
      "1119/1119 [==============================] - 0s 42us/step - loss: 0.9149 - acc: 0.6104\n",
      "Epoch 219/250\n",
      "1119/1119 [==============================] - 0s 46us/step - loss: 0.9184 - acc: 0.6059\n",
      "Epoch 220/250\n",
      "1119/1119 [==============================] - 0s 51us/step - loss: 0.9055 - acc: 0.6113\n",
      "Epoch 221/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 0.9170 - acc: 0.6104\n",
      "Epoch 222/250\n",
      "1119/1119 [==============================] - 0s 64us/step - loss: 0.9126 - acc: 0.6032\n",
      "Epoch 223/250\n",
      "1119/1119 [==============================] - 0s 49us/step - loss: 0.9234 - acc: 0.6095\n",
      "Epoch 224/250\n",
      "1119/1119 [==============================] - 0s 44us/step - loss: 0.9102 - acc: 0.6113\n",
      "Epoch 225/250\n",
      "1119/1119 [==============================] - 0s 50us/step - loss: 0.9128 - acc: 0.6157\n",
      "Epoch 226/250\n",
      "1119/1119 [==============================] - 0s 47us/step - loss: 0.9132 - acc: 0.6059\n",
      "Epoch 227/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 0.9097 - acc: 0.6139\n",
      "Epoch 228/250\n",
      "1119/1119 [==============================] - 0s 47us/step - loss: 0.9085 - acc: 0.6247\n",
      "Epoch 229/250\n",
      "1119/1119 [==============================] - 0s 46us/step - loss: 0.9195 - acc: 0.6148\n",
      "Epoch 230/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 0.9129 - acc: 0.5925\n",
      "Epoch 231/250\n",
      "1119/1119 [==============================] - 0s 56us/step - loss: 0.9097 - acc: 0.6113\n",
      "Epoch 232/250\n",
      "1119/1119 [==============================] - 0s 54us/step - loss: 0.9130 - acc: 0.5916\n",
      "Epoch 233/250\n",
      "1119/1119 [==============================] - 0s 42us/step - loss: 0.9060 - acc: 0.6113\n",
      "Epoch 234/250\n",
      "1119/1119 [==============================] - 0s 45us/step - loss: 0.9047 - acc: 0.6175\n",
      "Epoch 235/250\n",
      "1119/1119 [==============================] - 0s 44us/step - loss: 0.9027 - acc: 0.6130\n",
      "Epoch 236/250\n",
      "1119/1119 [==============================] - 0s 49us/step - loss: 0.9156 - acc: 0.6122\n",
      "Epoch 237/250\n",
      "1119/1119 [==============================] - 0s 56us/step - loss: 0.9161 - acc: 0.6059\n",
      "Epoch 238/250\n",
      "1119/1119 [==============================] - 0s 46us/step - loss: 0.9045 - acc: 0.6077\n",
      "Epoch 239/250\n",
      "1119/1119 [==============================] - 0s 41us/step - loss: 0.9072 - acc: 0.6086\n",
      "Epoch 240/250\n",
      "1119/1119 [==============================] - 0s 47us/step - loss: 0.8989 - acc: 0.6077\n",
      "Epoch 241/250\n",
      "1119/1119 [==============================] - 0s 49us/step - loss: 0.9052 - acc: 0.6157\n",
      "Epoch 242/250\n",
      "1119/1119 [==============================] - 0s 49us/step - loss: 0.9062 - acc: 0.6238\n",
      "Epoch 243/250\n",
      "1119/1119 [==============================] - 0s 53us/step - loss: 0.9055 - acc: 0.6059\n",
      "Epoch 244/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1119/1119 [==============================] - 0s 47us/step - loss: 0.9081 - acc: 0.5996\n",
      "Epoch 245/250\n",
      "1119/1119 [==============================] - 0s 46us/step - loss: 0.9014 - acc: 0.6193\n",
      "Epoch 246/250\n",
      "1119/1119 [==============================] - 0s 45us/step - loss: 0.9018 - acc: 0.6273\n",
      "Epoch 247/250\n",
      "1119/1119 [==============================] - 0s 48us/step - loss: 0.8961 - acc: 0.6148\n",
      "Epoch 248/250\n",
      "1119/1119 [==============================] - 0s 49us/step - loss: 0.9037 - acc: 0.6184\n",
      "Epoch 249/250\n",
      "1119/1119 [==============================] - 0s 47us/step - loss: 0.8963 - acc: 0.6157\n",
      "Epoch 250/250\n",
      "1119/1119 [==============================] - 0s 50us/step - loss: 0.8992 - acc: 0.6175\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "eveluate loss:0.9662708083788554, evaluate acc:0.6020833333333333\n"
     ]
    }
   ],
   "source": [
    "###########################################################\n",
    "import matplotlib.pyplot as plt\n",
    "##rmsprop<-adam 성능 증가#lr=0.001로 하니 너무 느리게 수렴->0.01로 하면 너무 ㅐ\n",
    "class whwinemodel(object):#,node,opt,loss,learn,act\n",
    "    def __init__(self):\n",
    "        self.model=Sequential()\n",
    "    def construct(self):\n",
    "        self.model.add(Dense(32,activation='relu',input_dim=11)) \n",
    "        self.model.add(Dense(25,activation='relu'))\n",
    "        self.model.add(Dense(20,activation='relu'))\n",
    "        self.model.add(Dense(10,activation='softmax'))\n",
    "        sgd=keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-8, decay=0.0)  \n",
    "        self.model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])#adam,rmsprop각각\n",
    "    def fit(self):\n",
    "        self.history=self.model.fit(x_train,y_train,epochs=300,batch_size=64,verbose=1)#validationset만드든것 성능 비교\n",
    "        \n",
    "    def printkey(self):\n",
    "        print(self.history.history.keys())\n",
    "        \n",
    "    def figure(self):\n",
    "        fig=plt.figure()\n",
    "        ax1=fig.add_subplot(1,2,1)\n",
    "        plt.plot(self.history.history['acc'])\n",
    "        plt.title('test accuracy')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        ax2=fig.add_subplot(1,2,2)\n",
    "        plt.plot(self.history.history['loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.show()\n",
    "        \n",
    "    def evaluate(self):\n",
    "        score = self.model.evaluate(x_train, y_train, verbose=0)\n",
    "        print(\"eveluate loss:{}, evaluate acc:{}\".format(score[0],score[1]))\n",
    "        \n",
    "    def test(self):\n",
    "        score = self.model.evaluate(x_test, y_test, verbose=0)\n",
    "        print(\"test\")\n",
    "        print(\"eveluate loss:{}, evaluate acc:{}\".format(score[0],score[1]))\n",
    "        \n",
    "firstmodel=whwinemodel()\n",
    "firstmodel.construct()\n",
    "firstmodel.fit()\n",
    "firstmodel.figure()\n",
    "firstmodel.test()\n",
    "\n",
    "\n",
    "\n",
    "class redwinemodel(object):#,node,opt,loss,learn,act\n",
    "    def __init__(self):\n",
    "        self.model=Sequential()\n",
    "    def construct(self):\n",
    "        self.model.add(Dense(32,activation='relu',input_dim=11)) \n",
    "        self.model.add(Dense(20,activation='relu'))\n",
    "        self.model.add(Dense(9,activation='softmax'))\n",
    "        sgd=keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-8, decay=0.0)  \n",
    "        self.model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])#adam,rmsprop각각\n",
    "    def fit(self):\n",
    "        print(ry_train.shape)\n",
    "        self.history=self.model.fit(rx_train,ry_train,epochs=250,batch_size=64,verbose=1)#validationset만드든것 성능 비교\n",
    "        #128로 했을 때 성능 더 굳 256보다\n",
    "       # self.history=self.model.train_on_batch(x_train,y_train,sample_weight=128,)\n",
    "    def printkey(self):\n",
    "        print(self.history.history.keys())\n",
    "        \n",
    "    def figure(self):\n",
    "        fig=plt.figure()\n",
    "        ax1=fig.add_subplot(1,2,1)\n",
    "        plt.plot(self.history.history['acc'])\n",
    "        plt.title('test accuracy')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        ax2=fig.add_subplot(1,2,2)\n",
    "        plt.plot(self.history.history['loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.show()\n",
    "        \n",
    "    def evaluate(self):\n",
    "        score = self.model.evaluate(rx_train, ry_train, verbose=0)\n",
    "        print(\"eveluate loss:{}, evaluate acc:{}\".format(score[0],score[1]))\n",
    "        \n",
    "    def test(self):\n",
    "        score = self.model.evaluate(rx_test, ry_test, verbose=0)\n",
    "        print(\"test\")\n",
    "        print(\"eveluate loss:{}, evaluate acc:{}\".format(score[0],score[1]))\n",
    "\n",
    "firstmodel=whwinemodel()\n",
    "firstmodel.construct()\n",
    "firstmodel.fit()\n",
    "firstmodel.figure()\n",
    "firstmodel.test()\n",
    "                \n",
    "rx_train, ry_train, rx_test, ry_test = generate_data(red_wine, 0.7)        \n",
    "redfirstmodel=redwinemodel()\n",
    "redfirstmodel.construct()\n",
    "redfirstmodel.fit()\n",
    "redfirstmodel.figure()\n",
    "redfirstmodel.test()\n",
    "\n",
    "###########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 각 모델의 성능을 향상시킬 수 있는 방법 적용\n",
    "* 하이퍼파라미터를 변경하여 테스트 셋에서의 정확도를 향상시킬 것\n",
    "    * 예) 레이어 수, 노드 수, Learning rate 등\n",
    "* 하이퍼파라미터를 변화시킨 각각의 모델에 대해, 트레이닝 Epoch 당 Loss의 변화를 기록하고 이를 시각화\n",
    "* 그 외 성능을 향상시킬 수 있는 모든 방법을 사용하여 가장 성능이 좋은 모델을 선택\n",
    "    * 예) Dropout, Normalization 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3429 samples, validate on 1469 samples\n",
      "Epoch 1/300\n",
      "3429/3429 [==============================] - 20s 6ms/step - loss: 2.2866 - acc: 0.2155 - val_loss: 2.2087 - val_acc: 0.2308\n",
      "Epoch 2/300\n",
      "3429/3429 [==============================] - 1s 261us/step - loss: 1.8047 - acc: 0.3791 - val_loss: 1.7656 - val_acc: 0.3676\n",
      "Epoch 3/300\n",
      "3429/3429 [==============================] - 1s 245us/step - loss: 1.5325 - acc: 0.4351 - val_loss: 1.7960 - val_acc: 0.2730\n",
      "Epoch 4/300\n",
      "3429/3429 [==============================] - 1s 212us/step - loss: 1.3751 - acc: 0.4678 - val_loss: 1.6985 - val_acc: 0.3097\n",
      "Epoch 5/300\n",
      "3429/3429 [==============================] - 1s 226us/step - loss: 1.2709 - acc: 0.4894 - val_loss: 1.6951 - val_acc: 0.2845\n",
      "Epoch 6/300\n",
      "3429/3429 [==============================] - 1s 222us/step - loss: 1.2076 - acc: 0.5141 - val_loss: 1.8150 - val_acc: 0.2498\n",
      "Epoch 7/300\n",
      "3429/3429 [==============================] - 1s 218us/step - loss: 1.1783 - acc: 0.5080 - val_loss: 1.5712 - val_acc: 0.3206\n",
      "Epoch 8/300\n",
      "3429/3429 [==============================] - 1s 221us/step - loss: 1.1524 - acc: 0.5235 - val_loss: 1.7109 - val_acc: 0.2893\n",
      "Epoch 9/300\n",
      "3429/3429 [==============================] - 1s 221us/step - loss: 1.1323 - acc: 0.5244 - val_loss: 1.8098 - val_acc: 0.2757\n",
      "Epoch 10/300\n",
      "3429/3429 [==============================] - 1s 226us/step - loss: 1.1209 - acc: 0.5176 - val_loss: 1.3569 - val_acc: 0.4302\n",
      "Epoch 11/300\n",
      "3429/3429 [==============================] - 1s 228us/step - loss: 1.1102 - acc: 0.5293 - val_loss: 1.5159 - val_acc: 0.3696\n",
      "Epoch 12/300\n",
      "3429/3429 [==============================] - 1s 227us/step - loss: 1.0923 - acc: 0.5372 - val_loss: 1.3878 - val_acc: 0.4234\n",
      "Epoch 13/300\n",
      "3429/3429 [==============================] - 1s 241us/step - loss: 1.0780 - acc: 0.5296 - val_loss: 1.7422 - val_acc: 0.3070\n",
      "Epoch 14/300\n",
      "3429/3429 [==============================] - 1s 280us/step - loss: 1.0755 - acc: 0.5442 - val_loss: 1.3545 - val_acc: 0.3935\n",
      "Epoch 15/300\n",
      "3429/3429 [==============================] - 1s 252us/step - loss: 1.0661 - acc: 0.5395 - val_loss: 1.4692 - val_acc: 0.3921\n",
      "Epoch 16/300\n",
      "3429/3429 [==============================] - 1s 300us/step - loss: 1.0697 - acc: 0.5357 - val_loss: 1.1893 - val_acc: 0.4799\n",
      "Epoch 17/300\n",
      "3429/3429 [==============================] - 1s 266us/step - loss: 1.0586 - acc: 0.5448 - val_loss: 1.1436 - val_acc: 0.5085\n",
      "Epoch 18/300\n",
      "3429/3429 [==============================] - 1s 309us/step - loss: 1.0470 - acc: 0.5410 - val_loss: 1.2275 - val_acc: 0.4493\n",
      "Epoch 19/300\n",
      "3429/3429 [==============================] - 1s 316us/step - loss: 1.0467 - acc: 0.5427 - val_loss: 1.2337 - val_acc: 0.4343\n",
      "Epoch 20/300\n",
      "3429/3429 [==============================] - 1s 315us/step - loss: 1.0446 - acc: 0.5462 - val_loss: 1.1476 - val_acc: 0.4854\n",
      "Epoch 21/300\n",
      "3429/3429 [==============================] - 1s 307us/step - loss: 1.0407 - acc: 0.5497 - val_loss: 1.2066 - val_acc: 0.4826\n",
      "Epoch 22/300\n",
      "3429/3429 [==============================] - 1s 295us/step - loss: 1.0285 - acc: 0.5570 - val_loss: 1.1871 - val_acc: 0.4779\n",
      "Epoch 23/300\n",
      "3429/3429 [==============================] - 1s 304us/step - loss: 1.0194 - acc: 0.5550 - val_loss: 1.1837 - val_acc: 0.4813\n",
      "Epoch 24/300\n",
      "3429/3429 [==============================] - 1s 282us/step - loss: 1.0212 - acc: 0.5585 - val_loss: 1.1855 - val_acc: 0.5058\n",
      "Epoch 25/300\n",
      "3429/3429 [==============================] - 1s 283us/step - loss: 1.0165 - acc: 0.5561 - val_loss: 1.3002 - val_acc: 0.4575\n",
      "Epoch 26/300\n",
      "3429/3429 [==============================] - 1s 276us/step - loss: 1.0045 - acc: 0.5593 - val_loss: 1.2645 - val_acc: 0.4690\n",
      "Epoch 27/300\n",
      "3429/3429 [==============================] - 1s 277us/step - loss: 0.9975 - acc: 0.5623 - val_loss: 1.1626 - val_acc: 0.4840\n",
      "Epoch 28/300\n",
      "3429/3429 [==============================] - 1s 281us/step - loss: 0.9996 - acc: 0.5652 - val_loss: 1.1938 - val_acc: 0.5024\n",
      "Epoch 29/300\n",
      "3429/3429 [==============================] - 1s 277us/step - loss: 0.9936 - acc: 0.5754 - val_loss: 1.1754 - val_acc: 0.5174\n",
      "Epoch 30/300\n",
      "3429/3429 [==============================] - 1s 312us/step - loss: 0.9991 - acc: 0.5684 - val_loss: 1.2817 - val_acc: 0.4561\n",
      "Epoch 31/300\n",
      "3429/3429 [==============================] - 1s 307us/step - loss: 0.9930 - acc: 0.5684 - val_loss: 1.1914 - val_acc: 0.4901\n",
      "Epoch 32/300\n",
      "3429/3429 [==============================] - 1s 286us/step - loss: 0.9874 - acc: 0.5704 - val_loss: 1.4335 - val_acc: 0.4241\n",
      "Epoch 33/300\n",
      "3429/3429 [==============================] - 1s 287us/step - loss: 0.9847 - acc: 0.5690 - val_loss: 1.1568 - val_acc: 0.4860\n",
      "Epoch 34/300\n",
      "3429/3429 [==============================] - 1s 308us/step - loss: 0.9878 - acc: 0.5675 - val_loss: 1.4245 - val_acc: 0.4030\n",
      "Epoch 35/300\n",
      "3429/3429 [==============================] - 1s 279us/step - loss: 0.9747 - acc: 0.5663 - val_loss: 1.1577 - val_acc: 0.4997\n",
      "Epoch 36/300\n",
      "3429/3429 [==============================] - 1s 316us/step - loss: 0.9748 - acc: 0.5748 - val_loss: 1.4023 - val_acc: 0.4132\n",
      "Epoch 37/300\n",
      "3429/3429 [==============================] - 1s 321us/step - loss: 0.9682 - acc: 0.5774 - val_loss: 1.2054 - val_acc: 0.4813\n",
      "Epoch 38/300\n",
      "3429/3429 [==============================] - 1s 331us/step - loss: 0.9609 - acc: 0.5801 - val_loss: 1.2496 - val_acc: 0.4738\n",
      "Epoch 39/300\n",
      "3429/3429 [==============================] - 1s 316us/step - loss: 0.9668 - acc: 0.5876 - val_loss: 1.2821 - val_acc: 0.4384\n",
      "Epoch 40/300\n",
      "3429/3429 [==============================] - 1s 297us/step - loss: 0.9671 - acc: 0.5722 - val_loss: 1.3088 - val_acc: 0.4506\n",
      "Epoch 41/300\n",
      "3429/3429 [==============================] - 1s 279us/step - loss: 0.9553 - acc: 0.5853 - val_loss: 1.2220 - val_acc: 0.4670\n",
      "Epoch 42/300\n",
      "3429/3429 [==============================] - 1s 280us/step - loss: 0.9551 - acc: 0.5871 - val_loss: 1.2386 - val_acc: 0.4833\n",
      "Epoch 43/300\n",
      "3429/3429 [==============================] - 1s 291us/step - loss: 0.9398 - acc: 0.5946 - val_loss: 1.2715 - val_acc: 0.4670\n",
      "Epoch 44/300\n",
      "3429/3429 [==============================] - 1s 340us/step - loss: 0.9443 - acc: 0.5923 - val_loss: 1.3317 - val_acc: 0.4200\n",
      "Epoch 45/300\n",
      "3429/3429 [==============================] - 1s 314us/step - loss: 0.9435 - acc: 0.5911 - val_loss: 1.1919 - val_acc: 0.5078\n",
      "Epoch 46/300\n",
      "3429/3429 [==============================] - 1s 301us/step - loss: 0.9325 - acc: 0.5941 - val_loss: 1.2804 - val_acc: 0.4609\n",
      "Epoch 47/300\n",
      "3429/3429 [==============================] - 1s 343us/step - loss: 0.9404 - acc: 0.5847 - val_loss: 1.3821 - val_acc: 0.4200\n",
      "Epoch 48/300\n",
      "3429/3429 [==============================] - 1s 305us/step - loss: 0.9232 - acc: 0.6005 - val_loss: 1.2065 - val_acc: 0.4983\n",
      "Epoch 49/300\n",
      "3429/3429 [==============================] - 1s 319us/step - loss: 0.9281 - acc: 0.5978 - val_loss: 1.2117 - val_acc: 0.5044\n",
      "Epoch 50/300\n",
      "3429/3429 [==============================] - 1s 315us/step - loss: 0.9235 - acc: 0.5999 - val_loss: 1.2237 - val_acc: 0.4854\n",
      "Epoch 51/300\n",
      "3429/3429 [==============================] - 1s 326us/step - loss: 0.9214 - acc: 0.6028 - val_loss: 1.1362 - val_acc: 0.5153\n",
      "Epoch 52/300\n",
      "3429/3429 [==============================] - 1s 318us/step - loss: 0.9098 - acc: 0.6095 - val_loss: 1.1950 - val_acc: 0.4860\n",
      "Epoch 53/300\n",
      "3429/3429 [==============================] - 1s 320us/step - loss: 0.9133 - acc: 0.6025 - val_loss: 1.3990 - val_acc: 0.4044\n",
      "Epoch 54/300\n",
      "3429/3429 [==============================] - 1s 324us/step - loss: 0.9120 - acc: 0.6063 - val_loss: 1.2070 - val_acc: 0.4826\n",
      "Epoch 55/300\n",
      "3429/3429 [==============================] - 1s 315us/step - loss: 0.8953 - acc: 0.6159 - val_loss: 1.2086 - val_acc: 0.4683\n",
      "Epoch 56/300\n",
      "3429/3429 [==============================] - 1s 348us/step - loss: 0.8989 - acc: 0.6104 - val_loss: 1.2881 - val_acc: 0.4697\n",
      "Epoch 57/300\n",
      "3429/3429 [==============================] - 1s 332us/step - loss: 0.8908 - acc: 0.6188 - val_loss: 1.2018 - val_acc: 0.4813\n",
      "Epoch 58/300\n",
      "3429/3429 [==============================] - 1s 339us/step - loss: 0.8942 - acc: 0.6019 - val_loss: 1.2336 - val_acc: 0.4602\n",
      "Epoch 59/300\n",
      "3429/3429 [==============================] - 2s 439us/step - loss: 0.8936 - acc: 0.6130 - val_loss: 1.1597 - val_acc: 0.5071\n",
      "Epoch 60/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3429/3429 [==============================] - 1s 346us/step - loss: 0.8895 - acc: 0.6089 - val_loss: 1.3126 - val_acc: 0.4745\n",
      "Epoch 61/300\n",
      "3429/3429 [==============================] - 1s 334us/step - loss: 0.8867 - acc: 0.6215 - val_loss: 1.2228 - val_acc: 0.4901\n",
      "Epoch 62/300\n",
      "3429/3429 [==============================] - 1s 354us/step - loss: 0.8811 - acc: 0.6183 - val_loss: 1.3940 - val_acc: 0.4826\n",
      "Epoch 63/300\n",
      "3429/3429 [==============================] - 1s 330us/step - loss: 0.8838 - acc: 0.6223 - val_loss: 1.4529 - val_acc: 0.4506\n",
      "Epoch 64/300\n",
      "3429/3429 [==============================] - 1s 326us/step - loss: 0.8748 - acc: 0.6174 - val_loss: 1.1713 - val_acc: 0.5051\n",
      "Epoch 65/300\n",
      "3429/3429 [==============================] - 1s 331us/step - loss: 0.8691 - acc: 0.6226 - val_loss: 1.1987 - val_acc: 0.5071\n",
      "Epoch 66/300\n",
      "3429/3429 [==============================] - 1s 335us/step - loss: 0.8637 - acc: 0.6317 - val_loss: 1.3327 - val_acc: 0.4500\n",
      "Epoch 67/300\n",
      "3429/3429 [==============================] - 1s 333us/step - loss: 0.8676 - acc: 0.6194 - val_loss: 1.4771 - val_acc: 0.4132\n",
      "Epoch 68/300\n",
      "3429/3429 [==============================] - 1s 316us/step - loss: 0.8546 - acc: 0.6264 - val_loss: 1.4735 - val_acc: 0.4486\n",
      "Epoch 69/300\n",
      "3429/3429 [==============================] - 1s 318us/step - loss: 0.8684 - acc: 0.6276 - val_loss: 1.5382 - val_acc: 0.4255\n",
      "Epoch 70/300\n",
      "3429/3429 [==============================] - 1s 327us/step - loss: 0.8570 - acc: 0.6174 - val_loss: 1.2855 - val_acc: 0.4717\n",
      "Epoch 71/300\n",
      "3429/3429 [==============================] - 1s 329us/step - loss: 0.8471 - acc: 0.6390 - val_loss: 1.2222 - val_acc: 0.4867\n",
      "Epoch 72/300\n",
      "3429/3429 [==============================] - 1s 313us/step - loss: 0.8641 - acc: 0.6285 - val_loss: 1.3515 - val_acc: 0.4683\n",
      "Epoch 73/300\n",
      "3429/3429 [==============================] - 1s 334us/step - loss: 0.8522 - acc: 0.6276 - val_loss: 1.5720 - val_acc: 0.4289\n",
      "Epoch 74/300\n",
      "3429/3429 [==============================] - 1s 312us/step - loss: 0.8393 - acc: 0.6398 - val_loss: 1.3354 - val_acc: 0.4915\n",
      "Epoch 75/300\n",
      "3429/3429 [==============================] - 1s 316us/step - loss: 0.8385 - acc: 0.6337 - val_loss: 1.2311 - val_acc: 0.4922\n",
      "Epoch 76/300\n",
      "3429/3429 [==============================] - 1s 300us/step - loss: 0.8270 - acc: 0.6401 - val_loss: 1.2979 - val_acc: 0.4963\n",
      "Epoch 77/300\n",
      "3429/3429 [==============================] - 1s 306us/step - loss: 0.8424 - acc: 0.6372 - val_loss: 1.2312 - val_acc: 0.4969\n",
      "Epoch 78/300\n",
      "3429/3429 [==============================] - 1s 312us/step - loss: 0.8336 - acc: 0.6311 - val_loss: 1.1823 - val_acc: 0.5153\n",
      "Epoch 79/300\n",
      "3429/3429 [==============================] - 1s 314us/step - loss: 0.8244 - acc: 0.6422 - val_loss: 1.4142 - val_acc: 0.4711\n",
      "Epoch 80/300\n",
      "3429/3429 [==============================] - 1s 312us/step - loss: 0.8249 - acc: 0.6425 - val_loss: 1.1972 - val_acc: 0.4935\n",
      "Epoch 81/300\n",
      "3429/3429 [==============================] - 1s 310us/step - loss: 0.8163 - acc: 0.6465 - val_loss: 1.1998 - val_acc: 0.5037\n",
      "Epoch 82/300\n",
      "3429/3429 [==============================] - 1s 316us/step - loss: 0.8024 - acc: 0.6547 - val_loss: 1.2912 - val_acc: 0.4588\n",
      "Epoch 83/300\n",
      "3429/3429 [==============================] - 1s 313us/step - loss: 0.8204 - acc: 0.6445 - val_loss: 1.3064 - val_acc: 0.4663\n",
      "Epoch 84/300\n",
      "3429/3429 [==============================] - 1s 308us/step - loss: 0.8297 - acc: 0.6390 - val_loss: 1.2526 - val_acc: 0.4915\n",
      "Epoch 85/300\n",
      "3429/3429 [==============================] - 1s 306us/step - loss: 0.8158 - acc: 0.6495 - val_loss: 1.2134 - val_acc: 0.4888\n",
      "Epoch 86/300\n",
      "3429/3429 [==============================] - 1s 303us/step - loss: 0.8062 - acc: 0.6553 - val_loss: 1.4446 - val_acc: 0.4486\n",
      "Epoch 87/300\n",
      "3429/3429 [==============================] - 1s 301us/step - loss: 0.7978 - acc: 0.6544 - val_loss: 1.1369 - val_acc: 0.5514\n",
      "Epoch 88/300\n",
      "3429/3429 [==============================] - 1s 303us/step - loss: 0.7820 - acc: 0.6565 - val_loss: 1.1886 - val_acc: 0.5289\n",
      "Epoch 89/300\n",
      "3429/3429 [==============================] - 1s 309us/step - loss: 0.8004 - acc: 0.6626 - val_loss: 1.2515 - val_acc: 0.4922\n",
      "Epoch 90/300\n",
      "3429/3429 [==============================] - 1s 308us/step - loss: 0.7861 - acc: 0.6661 - val_loss: 1.2776 - val_acc: 0.4997\n",
      "Epoch 91/300\n",
      "3429/3429 [==============================] - 1s 297us/step - loss: 0.7837 - acc: 0.6570 - val_loss: 1.1745 - val_acc: 0.5235\n",
      "Epoch 92/300\n",
      "3429/3429 [==============================] - 1s 302us/step - loss: 0.7747 - acc: 0.6710 - val_loss: 1.3580 - val_acc: 0.4847\n",
      "Epoch 93/300\n",
      "3429/3429 [==============================] - 1s 304us/step - loss: 0.7680 - acc: 0.6678 - val_loss: 1.3728 - val_acc: 0.4820\n",
      "Epoch 94/300\n",
      "3429/3429 [==============================] - 1s 302us/step - loss: 0.7723 - acc: 0.6664 - val_loss: 1.2714 - val_acc: 0.4949\n",
      "Epoch 95/300\n",
      "3429/3429 [==============================] - 1s 307us/step - loss: 0.7684 - acc: 0.6643 - val_loss: 1.2620 - val_acc: 0.5174\n",
      "Epoch 96/300\n",
      "3429/3429 [==============================] - 1s 304us/step - loss: 0.7560 - acc: 0.6734 - val_loss: 1.1439 - val_acc: 0.5310\n",
      "Epoch 97/300\n",
      "3429/3429 [==============================] - 1s 302us/step - loss: 0.7554 - acc: 0.6705 - val_loss: 1.6245 - val_acc: 0.4649\n",
      "Epoch 98/300\n",
      "3429/3429 [==============================] - 1s 299us/step - loss: 0.7759 - acc: 0.6702 - val_loss: 1.4962 - val_acc: 0.4241\n",
      "Epoch 99/300\n",
      "3429/3429 [==============================] - 1s 306us/step - loss: 0.7646 - acc: 0.6638 - val_loss: 1.2748 - val_acc: 0.4969\n",
      "Epoch 100/300\n",
      "3429/3429 [==============================] - 1s 300us/step - loss: 0.7457 - acc: 0.6775 - val_loss: 1.2830 - val_acc: 0.5180\n",
      "Epoch 101/300\n",
      "3429/3429 [==============================] - 1s 302us/step - loss: 0.7430 - acc: 0.6795 - val_loss: 1.1816 - val_acc: 0.5391\n",
      "Epoch 102/300\n",
      "3429/3429 [==============================] - 1s 301us/step - loss: 0.7400 - acc: 0.6801 - val_loss: 1.2586 - val_acc: 0.5058\n",
      "Epoch 103/300\n",
      "3429/3429 [==============================] - 1s 324us/step - loss: 0.7449 - acc: 0.6707 - val_loss: 1.4605 - val_acc: 0.4820\n",
      "Epoch 104/300\n",
      "3429/3429 [==============================] - 1s 312us/step - loss: 0.7441 - acc: 0.6783 - val_loss: 1.3121 - val_acc: 0.5031\n",
      "Epoch 105/300\n",
      "3429/3429 [==============================] - 1s 307us/step - loss: 0.7341 - acc: 0.6865 - val_loss: 1.3380 - val_acc: 0.4956\n",
      "Epoch 106/300\n",
      "3429/3429 [==============================] - 1s 327us/step - loss: 0.7333 - acc: 0.6815 - val_loss: 1.2524 - val_acc: 0.4983\n",
      "Epoch 107/300\n",
      "3429/3429 [==============================] - 1s 316us/step - loss: 0.7287 - acc: 0.6842 - val_loss: 1.2669 - val_acc: 0.5194\n",
      "Epoch 108/300\n",
      "3429/3429 [==============================] - 1s 298us/step - loss: 0.7244 - acc: 0.6897 - val_loss: 1.2670 - val_acc: 0.5242\n",
      "Epoch 109/300\n",
      "3429/3429 [==============================] - 1s 300us/step - loss: 0.7131 - acc: 0.6923 - val_loss: 1.3748 - val_acc: 0.5078\n",
      "Epoch 110/300\n",
      "3429/3429 [==============================] - 1s 309us/step - loss: 0.7108 - acc: 0.6906 - val_loss: 1.1662 - val_acc: 0.5310\n",
      "Epoch 111/300\n",
      "3429/3429 [==============================] - 1s 372us/step - loss: 0.7014 - acc: 0.6903 - val_loss: 1.4096 - val_acc: 0.4779\n",
      "Epoch 112/300\n",
      "3429/3429 [==============================] - 1s 358us/step - loss: 0.7190 - acc: 0.6885 - val_loss: 1.2314 - val_acc: 0.5160\n",
      "Epoch 113/300\n",
      "3429/3429 [==============================] - 1s 336us/step - loss: 0.6947 - acc: 0.7063 - val_loss: 1.2343 - val_acc: 0.5276\n",
      "Epoch 114/300\n",
      "3429/3429 [==============================] - 1s 315us/step - loss: 0.7047 - acc: 0.7028 - val_loss: 1.4364 - val_acc: 0.4949\n",
      "Epoch 115/300\n",
      "3429/3429 [==============================] - 1s 293us/step - loss: 0.6990 - acc: 0.6923 - val_loss: 1.4357 - val_acc: 0.5024\n",
      "Epoch 116/300\n",
      "3429/3429 [==============================] - 1s 294us/step - loss: 0.6999 - acc: 0.7066 - val_loss: 1.5709 - val_acc: 0.4479\n",
      "Epoch 117/300\n",
      "3429/3429 [==============================] - 1s 298us/step - loss: 0.7033 - acc: 0.7011 - val_loss: 1.3270 - val_acc: 0.5058\n",
      "Epoch 118/300\n",
      "3429/3429 [==============================] - 1s 296us/step - loss: 0.6942 - acc: 0.6985 - val_loss: 1.2097 - val_acc: 0.5296\n",
      "Epoch 119/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3429/3429 [==============================] - 1s 291us/step - loss: 0.6751 - acc: 0.7107 - val_loss: 1.3801 - val_acc: 0.4894\n",
      "Epoch 120/300\n",
      "3429/3429 [==============================] - 1s 290us/step - loss: 0.6933 - acc: 0.7025 - val_loss: 1.2546 - val_acc: 0.5330\n",
      "Epoch 121/300\n",
      "3429/3429 [==============================] - 1s 289us/step - loss: 0.6747 - acc: 0.7151 - val_loss: 1.3621 - val_acc: 0.4990\n",
      "Epoch 122/300\n",
      "3429/3429 [==============================] - 1s 297us/step - loss: 0.6769 - acc: 0.7090 - val_loss: 1.2254 - val_acc: 0.5330\n",
      "Epoch 123/300\n",
      "3429/3429 [==============================] - 1s 294us/step - loss: 0.6706 - acc: 0.7113 - val_loss: 1.2737 - val_acc: 0.5180\n",
      "Epoch 124/300\n",
      "3429/3429 [==============================] - 1s 365us/step - loss: 0.6686 - acc: 0.7133 - val_loss: 1.2443 - val_acc: 0.5153\n",
      "Epoch 125/300\n",
      "3429/3429 [==============================] - 1s 344us/step - loss: 0.6593 - acc: 0.7180 - val_loss: 1.3262 - val_acc: 0.4942\n",
      "Epoch 126/300\n",
      "3429/3429 [==============================] - 1s 405us/step - loss: 0.6804 - acc: 0.7090 - val_loss: 1.2626 - val_acc: 0.5310\n",
      "Epoch 127/300\n",
      "3429/3429 [==============================] - 1s 357us/step - loss: 0.6607 - acc: 0.7157 - val_loss: 1.4776 - val_acc: 0.4942\n",
      "Epoch 128/300\n",
      "3429/3429 [==============================] - 1s 311us/step - loss: 0.6683 - acc: 0.7154 - val_loss: 1.3372 - val_acc: 0.5037\n",
      "Epoch 129/300\n",
      "3429/3429 [==============================] - 1s 320us/step - loss: 0.6494 - acc: 0.7197 - val_loss: 1.5071 - val_acc: 0.4731\n",
      "Epoch 130/300\n",
      "3429/3429 [==============================] - 1s 322us/step - loss: 0.6600 - acc: 0.7113 - val_loss: 1.3761 - val_acc: 0.5078\n",
      "Epoch 131/300\n",
      "3429/3429 [==============================] - 1s 303us/step - loss: 0.6436 - acc: 0.7340 - val_loss: 1.3737 - val_acc: 0.4901\n",
      "Epoch 132/300\n",
      "3429/3429 [==============================] - 1s 287us/step - loss: 0.6487 - acc: 0.7125 - val_loss: 1.3294 - val_acc: 0.4983\n",
      "Epoch 133/300\n",
      "3429/3429 [==============================] - 1s 288us/step - loss: 0.6394 - acc: 0.7241 - val_loss: 1.4640 - val_acc: 0.4636\n",
      "Epoch 134/300\n",
      "3429/3429 [==============================] - 1s 291us/step - loss: 0.6433 - acc: 0.7180 - val_loss: 1.3227 - val_acc: 0.5303\n",
      "Epoch 135/300\n",
      "3429/3429 [==============================] - 1s 290us/step - loss: 0.6300 - acc: 0.7396 - val_loss: 1.3008 - val_acc: 0.5214\n",
      "Epoch 136/300\n",
      "3429/3429 [==============================] - 1s 278us/step - loss: 0.6359 - acc: 0.7300 - val_loss: 1.6995 - val_acc: 0.4915\n",
      "Epoch 137/300\n",
      "3429/3429 [==============================] - 1s 278us/step - loss: 0.6271 - acc: 0.7311 - val_loss: 1.3424 - val_acc: 0.5133\n",
      "Epoch 138/300\n",
      "3429/3429 [==============================] - 1s 277us/step - loss: 0.6253 - acc: 0.7381 - val_loss: 1.3166 - val_acc: 0.5303\n",
      "Epoch 139/300\n",
      "3429/3429 [==============================] - 1s 282us/step - loss: 0.6350 - acc: 0.7215 - val_loss: 1.3414 - val_acc: 0.4874\n",
      "Epoch 140/300\n",
      "3429/3429 [==============================] - 1s 282us/step - loss: 0.6309 - acc: 0.7361 - val_loss: 1.3708 - val_acc: 0.5065\n",
      "Epoch 141/300\n",
      "3429/3429 [==============================] - 1s 311us/step - loss: 0.6260 - acc: 0.7288 - val_loss: 1.4146 - val_acc: 0.4963\n",
      "Epoch 142/300\n",
      "3429/3429 [==============================] - 1s 299us/step - loss: 0.6119 - acc: 0.7352 - val_loss: 1.3444 - val_acc: 0.5283- loss: 0.6123 - acc: 0.7\n",
      "Epoch 143/300\n",
      "3429/3429 [==============================] - 1s 294us/step - loss: 0.5987 - acc: 0.7489 - val_loss: 1.4309 - val_acc: 0.4942\n",
      "Epoch 144/300\n",
      "3429/3429 [==============================] - 1s 332us/step - loss: 0.5921 - acc: 0.7533 - val_loss: 1.5970 - val_acc: 0.4724\n",
      "Epoch 145/300\n",
      "3429/3429 [==============================] - 1s 320us/step - loss: 0.6095 - acc: 0.7413 - val_loss: 1.4211 - val_acc: 0.5153\n",
      "Epoch 146/300\n",
      "3429/3429 [==============================] - 1s 326us/step - loss: 0.6116 - acc: 0.7361 - val_loss: 1.3985 - val_acc: 0.4935\n",
      "Epoch 147/300\n",
      "3429/3429 [==============================] - 1s 320us/step - loss: 0.6077 - acc: 0.7474 - val_loss: 1.4519 - val_acc: 0.4717\n",
      "Epoch 148/300\n",
      "3429/3429 [==============================] - 1s 291us/step - loss: 0.6036 - acc: 0.7437 - val_loss: 1.2833 - val_acc: 0.5391\n",
      "Epoch 149/300\n",
      "3429/3429 [==============================] - 1s 293us/step - loss: 0.5962 - acc: 0.7480 - val_loss: 1.4002 - val_acc: 0.5242\n",
      "Epoch 150/300\n",
      "3429/3429 [==============================] - 1s 299us/step - loss: 0.5748 - acc: 0.7629 - val_loss: 1.5824 - val_acc: 0.4874\n",
      "Epoch 151/300\n",
      "3429/3429 [==============================] - 1s 297us/step - loss: 0.5887 - acc: 0.7565 - val_loss: 1.5246 - val_acc: 0.5187\n",
      "Epoch 152/300\n",
      "3429/3429 [==============================] - 1s 330us/step - loss: 0.5951 - acc: 0.7474 - val_loss: 1.5128 - val_acc: 0.4963\n",
      "Epoch 153/300\n",
      "3429/3429 [==============================] - 1s 302us/step - loss: 0.5733 - acc: 0.7542 - val_loss: 1.3703 - val_acc: 0.4806\n",
      "Epoch 154/300\n",
      "3429/3429 [==============================] - 1s 306us/step - loss: 0.5916 - acc: 0.7437 - val_loss: 1.4710 - val_acc: 0.4874\n",
      "Epoch 155/300\n",
      "3429/3429 [==============================] - 1s 291us/step - loss: 0.5927 - acc: 0.7539 - val_loss: 1.4487 - val_acc: 0.4963\n",
      "Epoch 156/300\n",
      "3429/3429 [==============================] - 1s 281us/step - loss: 0.5844 - acc: 0.7507 - val_loss: 1.3792 - val_acc: 0.5283\n",
      "Epoch 157/300\n",
      "3429/3429 [==============================] - 1s 273us/step - loss: 0.5755 - acc: 0.7536 - val_loss: 1.3049 - val_acc: 0.5405\n",
      "Epoch 158/300\n",
      "3429/3429 [==============================] - 1s 274us/step - loss: 0.5552 - acc: 0.7617 - val_loss: 1.3049 - val_acc: 0.5187\n",
      "Epoch 159/300\n",
      "3429/3429 [==============================] - 1s 287us/step - loss: 0.5883 - acc: 0.7562 - val_loss: 1.4679 - val_acc: 0.5208\n",
      "Epoch 160/300\n",
      "3429/3429 [==============================] - 1s 290us/step - loss: 0.5745 - acc: 0.7539 - val_loss: 1.4274 - val_acc: 0.5242\n",
      "Epoch 161/300\n",
      "3429/3429 [==============================] - 1s 293us/step - loss: 0.5425 - acc: 0.7722 - val_loss: 1.3240 - val_acc: 0.5235\n",
      "Epoch 162/300\n",
      "3429/3429 [==============================] - 1s 295us/step - loss: 0.5618 - acc: 0.7603 - val_loss: 1.3730 - val_acc: 0.5276\n",
      "Epoch 163/300\n",
      "3429/3429 [==============================] - 1s 294us/step - loss: 0.5673 - acc: 0.7571 - val_loss: 1.3667 - val_acc: 0.5548\n",
      "Epoch 164/300\n",
      "3429/3429 [==============================] - 1s 296us/step - loss: 0.5572 - acc: 0.7638 - val_loss: 1.4769 - val_acc: 0.4922\n",
      "Epoch 165/300\n",
      "3429/3429 [==============================] - 1s 293us/step - loss: 0.5647 - acc: 0.7568 - val_loss: 1.6800 - val_acc: 0.4806\n",
      "Epoch 166/300\n",
      "3429/3429 [==============================] - 1s 288us/step - loss: 0.5698 - acc: 0.7547 - val_loss: 1.4759 - val_acc: 0.5201\n",
      "Epoch 167/300\n",
      "3429/3429 [==============================] - 1s 295us/step - loss: 0.5491 - acc: 0.7699 - val_loss: 1.3482 - val_acc: 0.5432\n",
      "Epoch 168/300\n",
      "3429/3429 [==============================] - 1s 278us/step - loss: 0.5602 - acc: 0.7661 - val_loss: 1.3846 - val_acc: 0.5262\n",
      "Epoch 169/300\n",
      "3429/3429 [==============================] - 1s 299us/step - loss: 0.5319 - acc: 0.7760 - val_loss: 1.5582 - val_acc: 0.4711\n",
      "Epoch 170/300\n",
      "3429/3429 [==============================] - 1s 295us/step - loss: 0.5453 - acc: 0.7661 - val_loss: 1.3605 - val_acc: 0.5432\n",
      "Epoch 171/300\n",
      "3429/3429 [==============================] - 1s 291us/step - loss: 0.5471 - acc: 0.7728 - val_loss: 1.5881 - val_acc: 0.5180\n",
      "Epoch 172/300\n",
      "3429/3429 [==============================] - 1s 287us/step - loss: 0.5444 - acc: 0.7731 - val_loss: 1.5312 - val_acc: 0.5126\n",
      "Epoch 173/300\n",
      "3429/3429 [==============================] - 1s 281us/step - loss: 0.5300 - acc: 0.7763 - val_loss: 1.4654 - val_acc: 0.5017\n",
      "Epoch 174/300\n",
      "3429/3429 [==============================] - 1s 278us/step - loss: 0.5360 - acc: 0.7757 - val_loss: 1.4566 - val_acc: 0.5140\n",
      "Epoch 175/300\n",
      "3429/3429 [==============================] - 1s 283us/step - loss: 0.5299 - acc: 0.7813 - val_loss: 1.3437 - val_acc: 0.5398\n",
      "Epoch 176/300\n",
      "3429/3429 [==============================] - 1s 283us/step - loss: 0.5728 - acc: 0.7539 - val_loss: 1.3815 - val_acc: 0.5344\n",
      "Epoch 177/300\n",
      "3429/3429 [==============================] - 1s 290us/step - loss: 0.5447 - acc: 0.7731 - val_loss: 1.4424 - val_acc: 0.5310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178/300\n",
      "3429/3429 [==============================] - 1s 291us/step - loss: 0.5144 - acc: 0.7897 - val_loss: 1.6092 - val_acc: 0.5024\n",
      "Epoch 179/300\n",
      "3429/3429 [==============================] - 1s 292us/step - loss: 0.5261 - acc: 0.7827 - val_loss: 1.5828 - val_acc: 0.5044\n",
      "Epoch 180/300\n",
      "3429/3429 [==============================] - 1s 292us/step - loss: 0.5348 - acc: 0.7644 - val_loss: 1.5287 - val_acc: 0.5194\n",
      "Epoch 181/300\n",
      "3429/3429 [==============================] - 1s 289us/step - loss: 0.5247 - acc: 0.7833 - val_loss: 1.5895 - val_acc: 0.4833\n",
      "Epoch 182/300\n",
      "3429/3429 [==============================] - 1s 292us/step - loss: 0.5377 - acc: 0.7746 - val_loss: 1.4897 - val_acc: 0.5167\n",
      "Epoch 183/300\n",
      "3429/3429 [==============================] - 1s 297us/step - loss: 0.5433 - acc: 0.7673 - val_loss: 1.5866 - val_acc: 0.5351\n",
      "Epoch 184/300\n",
      "3429/3429 [==============================] - 1s 296us/step - loss: 0.5274 - acc: 0.7760 - val_loss: 1.3903 - val_acc: 0.5330\n",
      "Epoch 185/300\n",
      "3429/3429 [==============================] - 1s 295us/step - loss: 0.5226 - acc: 0.7810 - val_loss: 1.5551 - val_acc: 0.5058\n",
      "Epoch 186/300\n",
      "3429/3429 [==============================] - 1s 289us/step - loss: 0.5103 - acc: 0.7842 - val_loss: 1.4897 - val_acc: 0.5357\n",
      "Epoch 187/300\n",
      "3429/3429 [==============================] - 1s 280us/step - loss: 0.5249 - acc: 0.7784 - val_loss: 1.7182 - val_acc: 0.4595\n",
      "Epoch 188/300\n",
      "3429/3429 [==============================] - 1s 278us/step - loss: 0.5022 - acc: 0.7924 - val_loss: 1.6277 - val_acc: 0.4894\n",
      "Epoch 189/300\n",
      "3429/3429 [==============================] - 1s 296us/step - loss: 0.5022 - acc: 0.7865 - val_loss: 1.5136 - val_acc: 0.4956\n",
      "Epoch 190/300\n",
      "3429/3429 [==============================] - 1s 295us/step - loss: 0.4946 - acc: 0.7938 - val_loss: 1.5125 - val_acc: 0.5357\n",
      "Epoch 191/300\n",
      "3429/3429 [==============================] - 1s 298us/step - loss: 0.4899 - acc: 0.7918 - val_loss: 1.6101 - val_acc: 0.5187\n",
      "Epoch 192/300\n",
      "3429/3429 [==============================] - 1s 289us/step - loss: 0.4984 - acc: 0.7903 - val_loss: 1.5623 - val_acc: 0.5126\n",
      "Epoch 193/300\n",
      "3429/3429 [==============================] - 1s 292us/step - loss: 0.4835 - acc: 0.7932 - val_loss: 1.6433 - val_acc: 0.4745\n",
      "Epoch 194/300\n",
      "3429/3429 [==============================] - 1s 298us/step - loss: 0.4802 - acc: 0.7947 - val_loss: 1.8224 - val_acc: 0.4786\n",
      "Epoch 195/300\n",
      "3429/3429 [==============================] - 1s 298us/step - loss: 0.4938 - acc: 0.7938 - val_loss: 1.5214 - val_acc: 0.5255\n",
      "Epoch 196/300\n",
      "3429/3429 [==============================] - 1s 296us/step - loss: 0.4826 - acc: 0.7962 - val_loss: 1.6058 - val_acc: 0.5344\n",
      "Epoch 197/300\n",
      "3429/3429 [==============================] - 1s 297us/step - loss: 0.4957 - acc: 0.7868 - val_loss: 1.5575 - val_acc: 0.5269\n",
      "Epoch 198/300\n",
      "3429/3429 [==============================] - 1s 286us/step - loss: 0.4652 - acc: 0.8151 - val_loss: 1.5253 - val_acc: 0.5071\n",
      "Epoch 199/300\n",
      "3429/3429 [==============================] - 1s 283us/step - loss: 0.4911 - acc: 0.7973 - val_loss: 1.4145 - val_acc: 0.5310\n",
      "Epoch 200/300\n",
      "3429/3429 [==============================] - 1s 280us/step - loss: 0.4816 - acc: 0.8049 - val_loss: 1.7849 - val_acc: 0.5010\n",
      "Epoch 201/300\n",
      "3429/3429 [==============================] - 1s 286us/step - loss: 0.4895 - acc: 0.7962 - val_loss: 1.4330 - val_acc: 0.5459\n",
      "Epoch 202/300\n",
      "3429/3429 [==============================] - 1s 290us/step - loss: 0.4865 - acc: 0.8017 - val_loss: 1.5363 - val_acc: 0.5473\n",
      "Epoch 203/300\n",
      "3429/3429 [==============================] - 1s 294us/step - loss: 0.4714 - acc: 0.7982 - val_loss: 1.4561 - val_acc: 0.5691\n",
      "Epoch 204/300\n",
      "3429/3429 [==============================] - 1s 292us/step - loss: 0.4554 - acc: 0.8171 - val_loss: 1.5213 - val_acc: 0.5405\n",
      "Epoch 205/300\n",
      "3429/3429 [==============================] - 1s 291us/step - loss: 0.4768 - acc: 0.8008 - val_loss: 1.7811 - val_acc: 0.5180\n",
      "Epoch 206/300\n",
      "3429/3429 [==============================] - 1s 289us/step - loss: 0.4621 - acc: 0.8049 - val_loss: 1.4107 - val_acc: 0.5283\n",
      "Epoch 207/300\n",
      "3429/3429 [==============================] - 1s 292us/step - loss: 0.4672 - acc: 0.8046 - val_loss: 1.5447 - val_acc: 0.5412\n",
      "Epoch 208/300\n",
      "3429/3429 [==============================] - 1s 291us/step - loss: 0.4875 - acc: 0.8014 - val_loss: 1.3964 - val_acc: 0.5562\n",
      "Epoch 209/300\n",
      "3429/3429 [==============================] - 1s 291us/step - loss: 0.4715 - acc: 0.8072 - val_loss: 1.6370 - val_acc: 0.5310\n",
      "Epoch 210/300\n",
      "3429/3429 [==============================] - 1s 291us/step - loss: 0.4677 - acc: 0.8017 - val_loss: 1.6549 - val_acc: 0.4969\n",
      "Epoch 211/300\n",
      "3429/3429 [==============================] - 1s 296us/step - loss: 0.4641 - acc: 0.8034 - val_loss: 1.6476 - val_acc: 0.4915\n",
      "Epoch 212/300\n",
      "3429/3429 [==============================] - 1s 301us/step - loss: 0.4589 - acc: 0.8148 - val_loss: 1.4703 - val_acc: 0.5459\n",
      "Epoch 213/300\n",
      "3429/3429 [==============================] - 1s 295us/step - loss: 0.4634 - acc: 0.8084 - val_loss: 1.5797 - val_acc: 0.5214\n",
      "Epoch 214/300\n",
      "3429/3429 [==============================] - 1s 296us/step - loss: 0.4571 - acc: 0.8128 - val_loss: 1.4820 - val_acc: 0.5344\n",
      "Epoch 215/300\n",
      "3429/3429 [==============================] - 1s 292us/step - loss: 0.4671 - acc: 0.8099 - val_loss: 1.4726 - val_acc: 0.5283\n",
      "Epoch 216/300\n",
      "3429/3429 [==============================] - 1s 283us/step - loss: 0.4501 - acc: 0.8136 - val_loss: 1.7262 - val_acc: 0.4745\n",
      "Epoch 217/300\n",
      "3429/3429 [==============================] - 1s 284us/step - loss: 0.4694 - acc: 0.7991 - val_loss: 1.6046 - val_acc: 0.5058\n",
      "Epoch 218/300\n",
      "3429/3429 [==============================] - 1s 282us/step - loss: 0.4505 - acc: 0.8093 - val_loss: 1.6534 - val_acc: 0.5071\n",
      "Epoch 219/300\n",
      "3429/3429 [==============================] - 1s 289us/step - loss: 0.4434 - acc: 0.8166 - val_loss: 1.5568 - val_acc: 0.5303\n",
      "Epoch 220/300\n",
      "3429/3429 [==============================] - 1s 298us/step - loss: 0.4437 - acc: 0.8081 - val_loss: 1.6297 - val_acc: 0.5385\n",
      "Epoch 221/300\n",
      "3429/3429 [==============================] - 1s 293us/step - loss: 0.4285 - acc: 0.8230 - val_loss: 1.4703 - val_acc: 0.5242\n",
      "Epoch 222/300\n",
      "3429/3429 [==============================] - 1s 298us/step - loss: 0.4511 - acc: 0.8131 - val_loss: 1.5291 - val_acc: 0.5562\n",
      "Epoch 223/300\n",
      "3429/3429 [==============================] - 1s 303us/step - loss: 0.4260 - acc: 0.8241 - val_loss: 1.4676 - val_acc: 0.5398\n",
      "Epoch 224/300\n",
      "3429/3429 [==============================] - 1s 312us/step - loss: 0.4253 - acc: 0.8256 - val_loss: 1.6499 - val_acc: 0.5044\n",
      "Epoch 225/300\n",
      "3429/3429 [==============================] - 1s 297us/step - loss: 0.4224 - acc: 0.8233 - val_loss: 1.7295 - val_acc: 0.4949\n",
      "Epoch 226/300\n",
      "3429/3429 [==============================] - 1s 295us/step - loss: 0.4318 - acc: 0.8233 - val_loss: 1.6281 - val_acc: 0.5078\n",
      "Epoch 227/300\n",
      "3429/3429 [==============================] - 1s 318us/step - loss: 0.4426 - acc: 0.8195 - val_loss: 1.4690 - val_acc: 0.5664\n",
      "Epoch 228/300\n",
      "3429/3429 [==============================] - 1s 311us/step - loss: 0.4352 - acc: 0.8163 - val_loss: 1.5607 - val_acc: 0.5480\n",
      "Epoch 229/300\n",
      "3429/3429 [==============================] - 1s 317us/step - loss: 0.4248 - acc: 0.8300 - val_loss: 1.5326 - val_acc: 0.5528\n",
      "Epoch 230/300\n",
      "3429/3429 [==============================] - 1s 324us/step - loss: 0.4198 - acc: 0.8317 - val_loss: 1.5844 - val_acc: 0.5303\n",
      "Epoch 231/300\n",
      "3429/3429 [==============================] - 1s 327us/step - loss: 0.4389 - acc: 0.8134 - val_loss: 1.5842 - val_acc: 0.5235\n",
      "Epoch 232/300\n",
      "3429/3429 [==============================] - 1s 330us/step - loss: 0.4309 - acc: 0.8177 - val_loss: 1.5103 - val_acc: 0.5616\n",
      "Epoch 233/300\n",
      "3429/3429 [==============================] - 1s 318us/step - loss: 0.4433 - acc: 0.8160 - val_loss: 1.5215 - val_acc: 0.5623\n",
      "Epoch 234/300\n",
      "3429/3429 [==============================] - 1s 345us/step - loss: 0.4222 - acc: 0.8271 - val_loss: 1.5894 - val_acc: 0.5106\n",
      "Epoch 235/300\n",
      "3429/3429 [==============================] - 1s 327us/step - loss: 0.4285 - acc: 0.8236 - val_loss: 1.7716 - val_acc: 0.5078\n",
      "Epoch 236/300\n",
      "3429/3429 [==============================] - 1s 290us/step - loss: 0.4120 - acc: 0.8282 - val_loss: 1.8856 - val_acc: 0.5044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 237/300\n",
      "3429/3429 [==============================] - 1s 301us/step - loss: 0.4254 - acc: 0.8241 - val_loss: 1.6139 - val_acc: 0.5242\n",
      "Epoch 238/300\n",
      "3429/3429 [==============================] - 1s 289us/step - loss: 0.4241 - acc: 0.8256 - val_loss: 1.7829 - val_acc: 0.5126\n",
      "Epoch 239/300\n",
      "3429/3429 [==============================] - 1s 299us/step - loss: 0.4238 - acc: 0.8244 - val_loss: 1.8036 - val_acc: 0.5146\n",
      "Epoch 240/300\n",
      "3429/3429 [==============================] - 1s 334us/step - loss: 0.4014 - acc: 0.8364 - val_loss: 1.9515 - val_acc: 0.4602\n",
      "Epoch 241/300\n",
      "3429/3429 [==============================] - 1s 311us/step - loss: 0.3982 - acc: 0.8399 - val_loss: 1.6779 - val_acc: 0.5364\n",
      "Epoch 242/300\n",
      "3429/3429 [==============================] - 1s 300us/step - loss: 0.4436 - acc: 0.8128 - val_loss: 1.9617 - val_acc: 0.4806\n",
      "Epoch 243/300\n",
      "3429/3429 [==============================] - 1s 305us/step - loss: 0.4150 - acc: 0.8338 - val_loss: 1.6940 - val_acc: 0.5235\n",
      "Epoch 244/300\n",
      "3429/3429 [==============================] - 1s 300us/step - loss: 0.4042 - acc: 0.8326 - val_loss: 1.7641 - val_acc: 0.5187\n",
      "Epoch 245/300\n",
      "3429/3429 [==============================] - 1s 342us/step - loss: 0.3948 - acc: 0.8370 - val_loss: 1.7517 - val_acc: 0.5044\n",
      "Epoch 246/300\n",
      "3429/3429 [==============================] - 1s 346us/step - loss: 0.4062 - acc: 0.8265 - val_loss: 1.7234 - val_acc: 0.5092\n",
      "Epoch 247/300\n",
      "3429/3429 [==============================] - 1s 342us/step - loss: 0.4069 - acc: 0.8373 - val_loss: 1.8505 - val_acc: 0.4874\n",
      "Epoch 248/300\n",
      "3429/3429 [==============================] - 1s 331us/step - loss: 0.4076 - acc: 0.8294 - val_loss: 1.5834 - val_acc: 0.5507\n",
      "Epoch 249/300\n",
      "3429/3429 [==============================] - 1s 378us/step - loss: 0.3721 - acc: 0.8478 - val_loss: 1.5161 - val_acc: 0.5786\n",
      "Epoch 250/300\n",
      "3429/3429 [==============================] - 1s 324us/step - loss: 0.3761 - acc: 0.8513 - val_loss: 1.5142 - val_acc: 0.5596\n",
      "Epoch 251/300\n",
      "3429/3429 [==============================] - 1s 360us/step - loss: 0.4100 - acc: 0.8338 - val_loss: 1.6009 - val_acc: 0.5528\n",
      "Epoch 252/300\n",
      "3429/3429 [==============================] - 1s 303us/step - loss: 0.4033 - acc: 0.8396 - val_loss: 1.5124 - val_acc: 0.5609\n",
      "Epoch 253/300\n",
      "3429/3429 [==============================] - 1s 306us/step - loss: 0.4110 - acc: 0.8309 - val_loss: 1.7638 - val_acc: 0.4969\n",
      "Epoch 254/300\n",
      "3429/3429 [==============================] - 1s 339us/step - loss: 0.4122 - acc: 0.8288 - val_loss: 1.6249 - val_acc: 0.5636\n",
      "Epoch 255/300\n",
      "3429/3429 [==============================] - 1s 348us/step - loss: 0.4140 - acc: 0.8358 - val_loss: 1.5434 - val_acc: 0.5684\n",
      "Epoch 256/300\n",
      "3429/3429 [==============================] - 1s 317us/step - loss: 0.3856 - acc: 0.8449 - val_loss: 1.8153 - val_acc: 0.5303\n",
      "Epoch 257/300\n",
      "3429/3429 [==============================] - 1s 332us/step - loss: 0.3724 - acc: 0.8498 - val_loss: 1.6373 - val_acc: 0.5405\n",
      "Epoch 258/300\n",
      "3429/3429 [==============================] - 1s 335us/step - loss: 0.3969 - acc: 0.8390 - val_loss: 1.6412 - val_acc: 0.5351\n",
      "Epoch 259/300\n",
      "3429/3429 [==============================] - 1s 348us/step - loss: 0.3741 - acc: 0.8501 - val_loss: 1.7683 - val_acc: 0.5003\n",
      "Epoch 260/300\n",
      "3429/3429 [==============================] - 1s 351us/step - loss: 0.4178 - acc: 0.8335 - val_loss: 1.6973 - val_acc: 0.5378\n",
      "Epoch 261/300\n",
      "3429/3429 [==============================] - 1s 344us/step - loss: 0.4106 - acc: 0.8303 - val_loss: 1.8477 - val_acc: 0.5310\n",
      "Epoch 262/300\n",
      "3429/3429 [==============================] - 1s 305us/step - loss: 0.3890 - acc: 0.8437 - val_loss: 1.7353 - val_acc: 0.5432\n",
      "Epoch 263/300\n",
      "3429/3429 [==============================] - 1s 299us/step - loss: 0.3929 - acc: 0.8396 - val_loss: 2.0243 - val_acc: 0.5003\n",
      "Epoch 264/300\n",
      "3429/3429 [==============================] - 1s 302us/step - loss: 0.3842 - acc: 0.8437 - val_loss: 1.7876 - val_acc: 0.4908\n",
      "Epoch 265/300\n",
      "3429/3429 [==============================] - 1s 303us/step - loss: 0.3933 - acc: 0.8341 - val_loss: 1.6700 - val_acc: 0.5351\n",
      "Epoch 266/300\n",
      "3429/3429 [==============================] - 1s 305us/step - loss: 0.3707 - acc: 0.8390 - val_loss: 1.6362 - val_acc: 0.5439\n",
      "Epoch 267/300\n",
      "3429/3429 [==============================] - 1s 302us/step - loss: 0.3879 - acc: 0.8361 - val_loss: 1.6351 - val_acc: 0.5698\n",
      "Epoch 268/300\n",
      "3429/3429 [==============================] - 1s 279us/step - loss: 0.3971 - acc: 0.8399 - val_loss: 1.6951 - val_acc: 0.5487\n",
      "Epoch 269/300\n",
      "3429/3429 [==============================] - 1s 282us/step - loss: 0.3606 - acc: 0.8559 - val_loss: 1.7388 - val_acc: 0.5609\n",
      "Epoch 270/300\n",
      "3429/3429 [==============================] - 1s 278us/step - loss: 0.3779 - acc: 0.8472 - val_loss: 1.8408 - val_acc: 0.5194\n",
      "Epoch 271/300\n",
      "3429/3429 [==============================] - 1s 278us/step - loss: 0.3639 - acc: 0.8618 - val_loss: 1.6313 - val_acc: 0.5405\n",
      "Epoch 272/300\n",
      "3429/3429 [==============================] - 1s 273us/step - loss: 0.3834 - acc: 0.8402 - val_loss: 1.8971 - val_acc: 0.5065\n",
      "Epoch 273/300\n",
      "3429/3429 [==============================] - 1s 282us/step - loss: 0.3830 - acc: 0.8428 - val_loss: 1.6638 - val_acc: 0.5405\n",
      "Epoch 274/300\n",
      "3429/3429 [==============================] - 1s 305us/step - loss: 0.3614 - acc: 0.8481 - val_loss: 1.6293 - val_acc: 0.5596- loss: 0.3519 - acc: 0\n",
      "Epoch 275/300\n",
      "3429/3429 [==============================] - 1s 297us/step - loss: 0.3705 - acc: 0.8513 - val_loss: 1.5978 - val_acc: 0.5548\n",
      "Epoch 276/300\n",
      "3429/3429 [==============================] - 1s 291us/step - loss: 0.3976 - acc: 0.8364 - val_loss: 1.6164 - val_acc: 0.5548\n",
      "Epoch 277/300\n",
      "3429/3429 [==============================] - 1s 299us/step - loss: 0.3749 - acc: 0.8428 - val_loss: 1.5862 - val_acc: 0.5528\n",
      "Epoch 278/300\n",
      "3429/3429 [==============================] - 1s 297us/step - loss: 0.3667 - acc: 0.8516 - val_loss: 1.6837 - val_acc: 0.5398\n",
      "Epoch 279/300\n",
      "3429/3429 [==============================] - 1s 287us/step - loss: 0.3600 - acc: 0.8536 - val_loss: 1.7745 - val_acc: 0.4990\n",
      "Epoch 280/300\n",
      "3429/3429 [==============================] - 1s 305us/step - loss: 0.3622 - acc: 0.8524 - val_loss: 1.6120 - val_acc: 0.5609\n",
      "Epoch 281/300\n",
      "3429/3429 [==============================] - 1s 295us/step - loss: 0.3726 - acc: 0.8425 - val_loss: 1.6844 - val_acc: 0.5555\n",
      "Epoch 282/300\n",
      "3429/3429 [==============================] - 1s 296us/step - loss: 0.3495 - acc: 0.8612 - val_loss: 1.6840 - val_acc: 0.5514\n",
      "Epoch 283/300\n",
      "3429/3429 [==============================] - 1s 297us/step - loss: 0.3664 - acc: 0.8551 - val_loss: 1.6568 - val_acc: 0.5487\n",
      "Epoch 284/300\n",
      "3429/3429 [==============================] - 1s 327us/step - loss: 0.3260 - acc: 0.8682 - val_loss: 1.9379 - val_acc: 0.4997\n",
      "Epoch 285/300\n",
      "3429/3429 [==============================] - 1s 363us/step - loss: 0.3597 - acc: 0.8554 - val_loss: 1.7509 - val_acc: 0.5378\n",
      "Epoch 286/300\n",
      "3429/3429 [==============================] - 1s 363us/step - loss: 0.3463 - acc: 0.8626 - val_loss: 1.6710 - val_acc: 0.5466\n",
      "Epoch 287/300\n",
      "3429/3429 [==============================] - 1s 369us/step - loss: 0.3737 - acc: 0.8460 - val_loss: 1.7099 - val_acc: 0.5487\n",
      "Epoch 288/300\n",
      "3429/3429 [==============================] - 1s 353us/step - loss: 0.3321 - acc: 0.8659 - val_loss: 1.7224 - val_acc: 0.5725\n",
      "Epoch 289/300\n",
      "3429/3429 [==============================] - 1s 299us/step - loss: 0.3608 - acc: 0.8513 - val_loss: 1.6949 - val_acc: 0.5562\n",
      "Epoch 290/300\n",
      "3429/3429 [==============================] - 1s 287us/step - loss: 0.3546 - acc: 0.8618 - val_loss: 1.6512 - val_acc: 0.5466\n",
      "Epoch 291/300\n",
      "3429/3429 [==============================] - 1s 287us/step - loss: 0.3303 - acc: 0.8629 - val_loss: 1.7122 - val_acc: 0.5466\n",
      "Epoch 292/300\n",
      "3429/3429 [==============================] - 1s 311us/step - loss: 0.3448 - acc: 0.8688 - val_loss: 1.6313 - val_acc: 0.5596\n",
      "Epoch 293/300\n",
      "3429/3429 [==============================] - 1s 313us/step - loss: 0.3459 - acc: 0.8597 - val_loss: 2.0879 - val_acc: 0.5126\n",
      "Epoch 294/300\n",
      "3429/3429 [==============================] - 1s 366us/step - loss: 0.3529 - acc: 0.8568 - val_loss: 1.6395 - val_acc: 0.5677\n",
      "Epoch 295/300\n",
      "3429/3429 [==============================] - 1s 364us/step - loss: 0.3555 - acc: 0.8556 - val_loss: 1.7063 - val_acc: 0.5283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296/300\n",
      "3429/3429 [==============================] - 1s 337us/step - loss: 0.3549 - acc: 0.8498 - val_loss: 1.7297 - val_acc: 0.5718\n",
      "Epoch 297/300\n",
      "3429/3429 [==============================] - 1s 314us/step - loss: 0.3483 - acc: 0.8568 - val_loss: 1.6937 - val_acc: 0.5405\n",
      "Epoch 298/300\n",
      "3429/3429 [==============================] - 1s 301us/step - loss: 0.3383 - acc: 0.8644 - val_loss: 1.8200 - val_acc: 0.5432\n",
      "Epoch 299/300\n",
      "3429/3429 [==============================] - 1s 348us/step - loss: 0.3508 - acc: 0.8632 - val_loss: 1.9785 - val_acc: 0.5037\n",
      "Epoch 300/300\n",
      "3429/3429 [==============================] - 1s 363us/step - loss: 0.3508 - acc: 0.8615 - val_loss: 1.5979 - val_acc: 0.5568\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red test\n",
      "eveluate loss:1.597883395389118, evaluate acc:0.5568413889432456\n"
     ]
    }
   ],
   "source": [
    "###########################################################minmaxscaler성능 뱃\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers.normalization import BatchNormalization,regularizers\n",
    "\n",
    "class whwinemodel(object):#,node,opt,loss,learn,act\n",
    "    def __init__(self):\n",
    "        self.model=Sequential()\n",
    "    def construct(self):\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Dense(512,input_dim=11))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(keras.layers.core.Dropout(0.2))\n",
    "        self.model.add(Dense(256))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(128))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(64))\n",
    "        self.model.add(BatchNormalization())#지워보기\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(32))#64개인거로 했을떄 성능은 비슷하거나 더 안좋았음 \n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(10,activation='softmax'))#rms,adagrad,adadelta\n",
    "        sgd=keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "        self.model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])#adam,rmsprop각각\n",
    "    def fit(self):\n",
    "        self.history=self.model.fit(x_train,y_train,epochs=200,batch_size=256,verbose=1,validation_data=(x_test,y_test))#validationset만드든것 성능 비교\n",
    "        #128로 했을 때 성능 더 굳 256보다\n",
    "       # self.history=self.model.train_on_batch(x_train,y_train,sample_weight=128,)\n",
    "    def printkey(self):\n",
    "        print(self.history.history.keys())\n",
    "        \n",
    "    def figure(self):\n",
    "        fig=plt.figure()\n",
    "        ax1=fig.add_subplot(1,2,1)\n",
    "        plt.plot(self.history.history['acc'])\n",
    "        plt.title('test accuracy')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        ax2=fig.add_subplot(1,2,2)\n",
    "        plt.plot(self.history.history['loss'])\n",
    "        #plt.plot(history.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.show()\n",
    "        \n",
    "    def evaluate(self):\n",
    "        score = self.model.evaluate(x_train, y_train, verbose=0)\n",
    "        print(\"training\")\n",
    "        print(\"eveluate loss:{}, evaluate acc:{}\".format(score[0],score[1]))\n",
    "        \n",
    "    def test(self):\n",
    "        score = self.model.evaluate(x_test, y_test, verbose=0)\n",
    "        print(\"test\")\n",
    "        print(\"eveluate loss:{}, evaluate acc:{}\".format(score[0],score[1]))\n",
    "        \n",
    "#firstmodel=whwinemodel()\n",
    "#firstmodel.construct()\n",
    "#firstmodel.fit()\n",
    "#firstmodel.figure()\n",
    "#firstmodel.test()\n",
    "\n",
    "\n",
    "\n",
    "class redwinemodel(object):#,node,opt,loss,learn,act\n",
    "    def __init__(self):\n",
    "        self.model=Sequential()\n",
    "    def construct(self):\n",
    "        self.model.add(Dense(256))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(keras.layers.core.Dropout(0.2))#이거만 냅두기\n",
    "        self.model.add(Dense(128))#64개인거로 했을떄 성능은 비슷하거나 더 안좋았음 \n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(64))#64개인거로 했을떄 성능은 비슷하거나 더 안좋았음 \n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(32))#64개인거로 했을떄 성능은 비슷하거나 더 안좋았음 \n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "      #  self.model.add(Dense(20))\n",
    "      #  self.model.add(BatchNormalization())\n",
    "     #   self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(10,activation='softmax'))#rms,adagrad,adadelta\n",
    "        sgd=keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)#성능 굳굳...ㄸ\n",
    "    #    sgd=keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-8, decay=0.0)  \n",
    "        self.model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])#adam,rmsprop각각\n",
    "    def fit(self):\n",
    "        self.history=self.model.fit(x_train,y_train,epochs=300,batch_size=256,verbose=1,validation_data=(x_test,y_test))#validationset만드든것 성능 비교\n",
    "        #128로 했을 때 성능 더 굳 256보다\n",
    "       # self.history=self.model.train_on_batch(x_train,y_train,sample_weight=128,)\n",
    "    def printkey(self):\n",
    "        print(self.history.history.keys())\n",
    "        \n",
    "    def figure(self):\n",
    "        fig=plt.figure()\n",
    "        ax1=fig.add_subplot(1,2,1)\n",
    "        plt.plot(self.history.history['acc'])\n",
    "        plt.title('test accuracy')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        ax2=fig.add_subplot(1,2,2)\n",
    "        plt.plot(self.history.history['loss'])\n",
    "        #plt.plot(history.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.show()\n",
    "        \n",
    "    def evaluate(self):\n",
    "        score = self.model.evaluate(x_train, y_train, verbose=0)\n",
    "        print(\"training\")\n",
    "        print(\"eveluate loss:{}, evaluate acc:{}\".format(score[0],score[1]))\n",
    "        \n",
    "    def test(self):\n",
    "        score = self.model.evaluate(x_test, y_test, verbose=0)\n",
    "        print(\"red test\")\n",
    "        print(\"eveluate loss:{}, evaluate acc:{}\".format(score[0],score[1]))\n",
    "        \n",
    "redmodel=redwinemodel()\n",
    "redmodel.construct()\n",
    "redmodel.fit()\n",
    "redmodel.figure()\n",
    "redmodel.test()\n",
    "\n",
    "\n",
    "\n",
    "###########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 화이트 와인과 레드 와인을 하나의 모델만 사용하여 분류\n",
    "* 화이트 와인과 레드 와인 데이터를 합쳐 wine 데이터 셋 생성\n",
    "* 입력이 화이트 와인인지 레드 와인인지에 관계없이 와인 품질을 분류하는 모델 생성\n",
    "* 모델의 성능을 향상시킬 수 있는 방법을 찾아 적용할 것\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4548 samples, validate on 1257 samples\n",
      "Epoch 1/300\n",
      "4548/4548 [==============================] - 19s 4ms/step - loss: 1.6194 - acc: 0.4464 - val_loss: 1.2206 - val_acc: 0.5473\n",
      "Epoch 2/300\n",
      "4548/4548 [==============================] - 1s 222us/step - loss: 1.1299 - acc: 0.5587 - val_loss: 1.0787 - val_acc: 0.5704\n",
      "Epoch 3/300\n",
      "4548/4548 [==============================] - 1s 229us/step - loss: 1.0511 - acc: 0.5620 - val_loss: 1.0471 - val_acc: 0.5768\n",
      "Epoch 4/300\n",
      "4548/4548 [==============================] - 1s 236us/step - loss: 1.0235 - acc: 0.5697 - val_loss: 1.0232 - val_acc: 0.5990\n",
      "Epoch 5/300\n",
      "4548/4548 [==============================] - 1s 243us/step - loss: 0.9939 - acc: 0.5750 - val_loss: 1.0355 - val_acc: 0.5807\n",
      "Epoch 6/300\n",
      "4548/4548 [==============================] - 1s 245us/step - loss: 0.9862 - acc: 0.5787 - val_loss: 1.0283 - val_acc: 0.5903\n",
      "Epoch 7/300\n",
      "4548/4548 [==============================] - 1s 244us/step - loss: 0.9684 - acc: 0.5840 - val_loss: 1.0336 - val_acc: 0.5688\n",
      "Epoch 8/300\n",
      "4548/4548 [==============================] - 1s 241us/step - loss: 0.9584 - acc: 0.5908 - val_loss: 1.0003 - val_acc: 0.5935\n",
      "Epoch 9/300\n",
      "4548/4548 [==============================] - 1s 238us/step - loss: 0.9553 - acc: 0.5796 - val_loss: 1.0132 - val_acc: 0.5784\n",
      "Epoch 10/300\n",
      "4548/4548 [==============================] - 1s 244us/step - loss: 0.9411 - acc: 0.6014 - val_loss: 1.0014 - val_acc: 0.5903\n",
      "Epoch 11/300\n",
      "4548/4548 [==============================] - 1s 245us/step - loss: 0.9293 - acc: 0.6040 - val_loss: 1.0053 - val_acc: 0.5784\n",
      "Epoch 12/300\n",
      "4548/4548 [==============================] - 1s 251us/step - loss: 0.9223 - acc: 0.6055 - val_loss: 0.9887 - val_acc: 0.5815\n",
      "Epoch 13/300\n",
      "4548/4548 [==============================] - 1s 256us/step - loss: 0.9137 - acc: 0.6088 - val_loss: 1.0300 - val_acc: 0.5720\n",
      "Epoch 14/300\n",
      "4548/4548 [==============================] - 1s 257us/step - loss: 0.9031 - acc: 0.6058 - val_loss: 0.9890 - val_acc: 0.5879\n",
      "Epoch 15/300\n",
      "4548/4548 [==============================] - 1s 262us/step - loss: 0.8808 - acc: 0.6124 - val_loss: 0.9984 - val_acc: 0.5911\n",
      "Epoch 16/300\n",
      "4548/4548 [==============================] - 1s 261us/step - loss: 0.8730 - acc: 0.6299 - val_loss: 0.9796 - val_acc: 0.5847\n",
      "Epoch 17/300\n",
      "4548/4548 [==============================] - 1s 278us/step - loss: 0.8749 - acc: 0.6255 - val_loss: 0.9933 - val_acc: 0.5911\n",
      "Epoch 18/300\n",
      "4548/4548 [==============================] - 1s 277us/step - loss: 0.8650 - acc: 0.6297 - val_loss: 0.9871 - val_acc: 0.5951\n",
      "Epoch 19/300\n",
      "4548/4548 [==============================] - 1s 275us/step - loss: 0.8562 - acc: 0.6275 - val_loss: 0.9825 - val_acc: 0.5855\n",
      "Epoch 20/300\n",
      "4548/4548 [==============================] - 1s 274us/step - loss: 0.8436 - acc: 0.6473 - val_loss: 0.9916 - val_acc: 0.5951\n",
      "Epoch 21/300\n",
      "4548/4548 [==============================] - 1s 274us/step - loss: 0.8392 - acc: 0.6363 - val_loss: 1.0126 - val_acc: 0.5736\n",
      "Epoch 22/300\n",
      "4548/4548 [==============================] - 1s 277us/step - loss: 0.8261 - acc: 0.6533 - val_loss: 1.0333 - val_acc: 0.5688\n",
      "Epoch 23/300\n",
      "4548/4548 [==============================] - 1s 278us/step - loss: 0.8189 - acc: 0.6489 - val_loss: 0.9958 - val_acc: 0.5879\n",
      "Epoch 24/300\n",
      "4548/4548 [==============================] - 1s 284us/step - loss: 0.8162 - acc: 0.6539 - val_loss: 1.0362 - val_acc: 0.5656\n",
      "Epoch 25/300\n",
      "4548/4548 [==============================] - 1s 283us/step - loss: 0.8019 - acc: 0.6495 - val_loss: 1.0023 - val_acc: 0.5927\n",
      "Epoch 26/300\n",
      "4548/4548 [==============================] - 1s 285us/step - loss: 0.7877 - acc: 0.6603 - val_loss: 0.9976 - val_acc: 0.5982\n",
      "Epoch 27/300\n",
      "4548/4548 [==============================] - 1s 288us/step - loss: 0.7903 - acc: 0.6689 - val_loss: 0.9895 - val_acc: 0.5800\n",
      "Epoch 28/300\n",
      "4548/4548 [==============================] - 1s 286us/step - loss: 0.7722 - acc: 0.6807 - val_loss: 1.0155 - val_acc: 0.5847\n",
      "Epoch 29/300\n",
      "4548/4548 [==============================] - 1s 289us/step - loss: 0.7707 - acc: 0.6733 - val_loss: 1.0089 - val_acc: 0.5807\n",
      "Epoch 30/300\n",
      "4548/4548 [==============================] - 1s 289us/step - loss: 0.7738 - acc: 0.6728 - val_loss: 0.9960 - val_acc: 0.5943\n",
      "Epoch 31/300\n",
      "4548/4548 [==============================] - 1s 290us/step - loss: 0.7656 - acc: 0.6733 - val_loss: 1.0657 - val_acc: 0.5752\n",
      "Epoch 32/300\n",
      "4548/4548 [==============================] - 1s 294us/step - loss: 0.7597 - acc: 0.6832 - val_loss: 1.0141 - val_acc: 0.5800\n",
      "Epoch 33/300\n",
      "4548/4548 [==============================] - 1s 298us/step - loss: 0.7429 - acc: 0.6884 - val_loss: 1.0406 - val_acc: 0.5967\n",
      "Epoch 34/300\n",
      "4548/4548 [==============================] - 1s 299us/step - loss: 0.7522 - acc: 0.6796 - val_loss: 1.0293 - val_acc: 0.5863\n",
      "Epoch 35/300\n",
      "4548/4548 [==============================] - 1s 299us/step - loss: 0.7488 - acc: 0.6715 - val_loss: 1.0181 - val_acc: 0.5998\n",
      "Epoch 36/300\n",
      "4548/4548 [==============================] - 1s 301us/step - loss: 0.7261 - acc: 0.6922 - val_loss: 1.0828 - val_acc: 0.5760\n",
      "Epoch 37/300\n",
      "4548/4548 [==============================] - 1s 295us/step - loss: 0.7120 - acc: 0.7091 - val_loss: 1.0483 - val_acc: 0.5847\n",
      "Epoch 38/300\n",
      "4548/4548 [==============================] - 1s 303us/step - loss: 0.7116 - acc: 0.6994 - val_loss: 1.0333 - val_acc: 0.5879\n",
      "Epoch 39/300\n",
      "4548/4548 [==============================] - 1s 306us/step - loss: 0.7089 - acc: 0.7082 - val_loss: 1.0646 - val_acc: 0.5807\n",
      "Epoch 40/300\n",
      "4548/4548 [==============================] - 1s 313us/step - loss: 0.6990 - acc: 0.7155 - val_loss: 1.0447 - val_acc: 0.6062\n",
      "Epoch 41/300\n",
      "4548/4548 [==============================] - 1s 313us/step - loss: 0.6945 - acc: 0.7100 - val_loss: 1.0336 - val_acc: 0.6237\n",
      "Epoch 42/300\n",
      "4548/4548 [==============================] - 1s 318us/step - loss: 0.6831 - acc: 0.7159 - val_loss: 1.0176 - val_acc: 0.5975\n",
      "Epoch 43/300\n",
      "4548/4548 [==============================] - 1s 322us/step - loss: 0.6762 - acc: 0.7168 - val_loss: 1.0626 - val_acc: 0.5982\n",
      "Epoch 44/300\n",
      "4548/4548 [==============================] - 1s 321us/step - loss: 0.6691 - acc: 0.7166 - val_loss: 1.0624 - val_acc: 0.6054\n",
      "Epoch 45/300\n",
      "4548/4548 [==============================] - 1s 322us/step - loss: 0.6790 - acc: 0.7155 - val_loss: 1.0985 - val_acc: 0.5943\n",
      "Epoch 46/300\n",
      "4548/4548 [==============================] - 1s 323us/step - loss: 0.6639 - acc: 0.7223 - val_loss: 1.0621 - val_acc: 0.5998\n",
      "Epoch 47/300\n",
      "4548/4548 [==============================] - 1s 322us/step - loss: 0.6543 - acc: 0.7293 - val_loss: 1.0746 - val_acc: 0.5895\n",
      "Epoch 48/300\n",
      "4548/4548 [==============================] - 1s 325us/step - loss: 0.6633 - acc: 0.7236 - val_loss: 1.0788 - val_acc: 0.6014\n",
      "Epoch 49/300\n",
      "4548/4548 [==============================] - 2s 338us/step - loss: 0.6450 - acc: 0.7267 - val_loss: 1.0556 - val_acc: 0.6150\n",
      "Epoch 50/300\n",
      "4548/4548 [==============================] - 2s 340us/step - loss: 0.6420 - acc: 0.7328 - val_loss: 1.1127 - val_acc: 0.5927\n",
      "Epoch 51/300\n",
      "4548/4548 [==============================] - 2s 343us/step - loss: 0.6346 - acc: 0.7276 - val_loss: 1.0981 - val_acc: 0.6110\n",
      "Epoch 52/300\n",
      "4548/4548 [==============================] - 2s 347us/step - loss: 0.6311 - acc: 0.7339 - val_loss: 1.0870 - val_acc: 0.6062\n",
      "Epoch 53/300\n",
      "4548/4548 [==============================] - 2s 361us/step - loss: 0.6323 - acc: 0.7353 - val_loss: 1.1095 - val_acc: 0.6054\n",
      "Epoch 54/300\n",
      "4548/4548 [==============================] - 2s 356us/step - loss: 0.6365 - acc: 0.7423 - val_loss: 1.0792 - val_acc: 0.6142\n",
      "Epoch 55/300\n",
      "4548/4548 [==============================] - 2s 361us/step - loss: 0.6280 - acc: 0.7328 - val_loss: 1.1013 - val_acc: 0.6014\n",
      "Epoch 56/300\n",
      "4548/4548 [==============================] - 2s 356us/step - loss: 0.6170 - acc: 0.7423 - val_loss: 1.1369 - val_acc: 0.6006\n",
      "Epoch 57/300\n",
      "4548/4548 [==============================] - 2s 341us/step - loss: 0.6023 - acc: 0.7493 - val_loss: 1.1099 - val_acc: 0.6142\n",
      "Epoch 58/300\n",
      "4548/4548 [==============================] - 2s 339us/step - loss: 0.6153 - acc: 0.7447 - val_loss: 1.1130 - val_acc: 0.5998\n",
      "Epoch 59/300\n",
      "4548/4548 [==============================] - 2s 344us/step - loss: 0.5950 - acc: 0.7465 - val_loss: 1.1423 - val_acc: 0.5982\n",
      "Epoch 60/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4548/4548 [==============================] - 2s 331us/step - loss: 0.5992 - acc: 0.7518 - val_loss: 1.1665 - val_acc: 0.6062\n",
      "Epoch 61/300\n",
      "4548/4548 [==============================] - 1s 326us/step - loss: 0.5846 - acc: 0.7619 - val_loss: 1.1821 - val_acc: 0.5943\n",
      "Epoch 62/300\n",
      "4548/4548 [==============================] - 2s 334us/step - loss: 0.5965 - acc: 0.7476 - val_loss: 1.1358 - val_acc: 0.5935\n",
      "Epoch 63/300\n",
      "4548/4548 [==============================] - 2s 333us/step - loss: 0.5746 - acc: 0.7663 - val_loss: 1.1536 - val_acc: 0.5967\n",
      "Epoch 64/300\n",
      "4548/4548 [==============================] - 1s 324us/step - loss: 0.5650 - acc: 0.7667 - val_loss: 1.1597 - val_acc: 0.6062\n",
      "Epoch 65/300\n",
      "4548/4548 [==============================] - 1s 326us/step - loss: 0.5705 - acc: 0.7737 - val_loss: 1.1418 - val_acc: 0.5895\n",
      "Epoch 66/300\n",
      "4548/4548 [==============================] - 1s 325us/step - loss: 0.5740 - acc: 0.7647 - val_loss: 1.1092 - val_acc: 0.6189\n",
      "Epoch 67/300\n",
      "4548/4548 [==============================] - 2s 355us/step - loss: 0.5748 - acc: 0.7656 - val_loss: 1.1550 - val_acc: 0.6205\n",
      "Epoch 68/300\n",
      "4548/4548 [==============================] - 2s 346us/step - loss: 0.5647 - acc: 0.7718 - val_loss: 1.1988 - val_acc: 0.6014\n",
      "Epoch 69/300\n",
      "4548/4548 [==============================] - 2s 343us/step - loss: 0.5713 - acc: 0.7636 - val_loss: 1.1436 - val_acc: 0.6118\n",
      "Epoch 70/300\n",
      "4548/4548 [==============================] - 2s 332us/step - loss: 0.5518 - acc: 0.7733 - val_loss: 1.1537 - val_acc: 0.6205\n",
      "Epoch 71/300\n",
      "4548/4548 [==============================] - 1s 319us/step - loss: 0.5583 - acc: 0.7722 - val_loss: 1.1608 - val_acc: 0.6142\n",
      "Epoch 72/300\n",
      "4548/4548 [==============================] - 1s 323us/step - loss: 0.5466 - acc: 0.7810 - val_loss: 1.1650 - val_acc: 0.6317\n",
      "Epoch 73/300\n",
      "4548/4548 [==============================] - 1s 316us/step - loss: 0.5285 - acc: 0.7841 - val_loss: 1.1946 - val_acc: 0.6030\n",
      "Epoch 74/300\n",
      "4548/4548 [==============================] - 1s 309us/step - loss: 0.5678 - acc: 0.7643 - val_loss: 1.1785 - val_acc: 0.5943\n",
      "Epoch 75/300\n",
      "4548/4548 [==============================] - 1s 299us/step - loss: 0.5474 - acc: 0.7784 - val_loss: 1.1505 - val_acc: 0.6205\n",
      "Epoch 76/300\n",
      "4548/4548 [==============================] - 1s 306us/step - loss: 0.5274 - acc: 0.7843 - val_loss: 1.1372 - val_acc: 0.6269\n",
      "Epoch 77/300\n",
      "4548/4548 [==============================] - 1s 311us/step - loss: 0.5500 - acc: 0.7702 - val_loss: 1.1533 - val_acc: 0.6126\n",
      "Epoch 78/300\n",
      "4548/4548 [==============================] - 1s 328us/step - loss: 0.5527 - acc: 0.7724 - val_loss: 1.1924 - val_acc: 0.6205\n",
      "Epoch 79/300\n",
      "4548/4548 [==============================] - 1s 328us/step - loss: 0.5440 - acc: 0.7726 - val_loss: 1.1473 - val_acc: 0.6181\n",
      "Epoch 80/300\n",
      "4548/4548 [==============================] - 2s 341us/step - loss: 0.5223 - acc: 0.7850 - val_loss: 1.2126 - val_acc: 0.6173\n",
      "Epoch 81/300\n",
      "4548/4548 [==============================] - 1s 309us/step - loss: 0.5285 - acc: 0.7759 - val_loss: 1.2371 - val_acc: 0.6237\n",
      "Epoch 82/300\n",
      "4548/4548 [==============================] - 1s 309us/step - loss: 0.5141 - acc: 0.7869 - val_loss: 1.1483 - val_acc: 0.6364\n",
      "Epoch 83/300\n",
      "4548/4548 [==============================] - 1s 302us/step - loss: 0.5292 - acc: 0.7861 - val_loss: 1.2432 - val_acc: 0.6189\n",
      "Epoch 84/300\n",
      "4548/4548 [==============================] - 1s 318us/step - loss: 0.5047 - acc: 0.7962 - val_loss: 1.2427 - val_acc: 0.6022\n",
      "Epoch 85/300\n",
      "4548/4548 [==============================] - 1s 329us/step - loss: 0.5053 - acc: 0.7962 - val_loss: 1.2262 - val_acc: 0.6301\n",
      "Epoch 86/300\n",
      "4548/4548 [==============================] - 2s 331us/step - loss: 0.5044 - acc: 0.7940 - val_loss: 1.2187 - val_acc: 0.6181\n",
      "Epoch 87/300\n",
      "4548/4548 [==============================] - 1s 327us/step - loss: 0.4999 - acc: 0.7883 - val_loss: 1.2134 - val_acc: 0.6229\n",
      "Epoch 88/300\n",
      "4548/4548 [==============================] - 2s 338us/step - loss: 0.5057 - acc: 0.7940 - val_loss: 1.2635 - val_acc: 0.6110\n",
      "Epoch 89/300\n",
      "4548/4548 [==============================] - 2s 352us/step - loss: 0.5209 - acc: 0.7843 - val_loss: 1.3018 - val_acc: 0.6030\n",
      "Epoch 90/300\n",
      "4548/4548 [==============================] - 2s 339us/step - loss: 0.4933 - acc: 0.7944 - val_loss: 1.2386 - val_acc: 0.6110\n",
      "Epoch 91/300\n",
      "4548/4548 [==============================] - 2s 364us/step - loss: 0.4920 - acc: 0.7971 - val_loss: 1.2701 - val_acc: 0.6253\n",
      "Epoch 92/300\n",
      "4548/4548 [==============================] - 2s 400us/step - loss: 0.4975 - acc: 0.7966 - val_loss: 1.2302 - val_acc: 0.6150\n",
      "Epoch 93/300\n",
      "4548/4548 [==============================] - 2s 346us/step - loss: 0.4969 - acc: 0.8004 - val_loss: 1.2607 - val_acc: 0.6142\n",
      "Epoch 94/300\n",
      "4548/4548 [==============================] - 2s 351us/step - loss: 0.4800 - acc: 0.8030 - val_loss: 1.2338 - val_acc: 0.6277\n",
      "Epoch 95/300\n",
      "4548/4548 [==============================] - 2s 348us/step - loss: 0.4963 - acc: 0.7977 - val_loss: 1.2862 - val_acc: 0.6158\n",
      "Epoch 96/300\n",
      "4548/4548 [==============================] - 2s 337us/step - loss: 0.4962 - acc: 0.7962 - val_loss: 1.2820 - val_acc: 0.6173\n",
      "Epoch 97/300\n",
      "4548/4548 [==============================] - 2s 330us/step - loss: 0.4839 - acc: 0.8039 - val_loss: 1.3210 - val_acc: 0.6046\n",
      "Epoch 98/300\n",
      "4548/4548 [==============================] - 1s 320us/step - loss: 0.4829 - acc: 0.8001 - val_loss: 1.2566 - val_acc: 0.6006\n",
      "Epoch 99/300\n",
      "4548/4548 [==============================] - 1s 329us/step - loss: 0.4702 - acc: 0.8083 - val_loss: 1.2916 - val_acc: 0.5998\n",
      "Epoch 100/300\n",
      "4548/4548 [==============================] - 2s 331us/step - loss: 0.4901 - acc: 0.7931 - val_loss: 1.2667 - val_acc: 0.6142\n",
      "Epoch 101/300\n",
      "4548/4548 [==============================] - 2s 336us/step - loss: 0.4856 - acc: 0.7964 - val_loss: 1.2657 - val_acc: 0.6261\n",
      "Epoch 102/300\n",
      "4548/4548 [==============================] - 2s 333us/step - loss: 0.4851 - acc: 0.7984 - val_loss: 1.2868 - val_acc: 0.6293\n",
      "Epoch 103/300\n",
      "4548/4548 [==============================] - 1s 323us/step - loss: 0.4860 - acc: 0.8008 - val_loss: 1.2715 - val_acc: 0.6229\n",
      "Epoch 104/300\n",
      "4548/4548 [==============================] - 1s 329us/step - loss: 0.4903 - acc: 0.8006 - val_loss: 1.3319 - val_acc: 0.6189\n",
      "Epoch 105/300\n",
      "4548/4548 [==============================] - 2s 336us/step - loss: 0.4635 - acc: 0.8078 - val_loss: 1.2606 - val_acc: 0.6165\n",
      "Epoch 106/300\n",
      "4548/4548 [==============================] - 2s 333us/step - loss: 0.4748 - acc: 0.8041 - val_loss: 1.2871 - val_acc: 0.6293\n",
      "Epoch 107/300\n",
      "4548/4548 [==============================] - 2s 330us/step - loss: 0.4665 - acc: 0.8091 - val_loss: 1.2919 - val_acc: 0.6420\n",
      "Epoch 108/300\n",
      "4548/4548 [==============================] - 1s 318us/step - loss: 0.4645 - acc: 0.8113 - val_loss: 1.3277 - val_acc: 0.6213\n",
      "Epoch 109/300\n",
      "4548/4548 [==============================] - 1s 320us/step - loss: 0.4696 - acc: 0.8030 - val_loss: 1.3230 - val_acc: 0.6038\n",
      "Epoch 110/300\n",
      "4548/4548 [==============================] - 1s 325us/step - loss: 0.4595 - acc: 0.8153 - val_loss: 1.3021 - val_acc: 0.6269\n",
      "Epoch 111/300\n",
      "4548/4548 [==============================] - 1s 324us/step - loss: 0.4449 - acc: 0.8243 - val_loss: 1.3339 - val_acc: 0.6221\n",
      "Epoch 112/300\n",
      "4548/4548 [==============================] - 1s 327us/step - loss: 0.4548 - acc: 0.8140 - val_loss: 1.2740 - val_acc: 0.6325\n",
      "Epoch 113/300\n",
      "4548/4548 [==============================] - 2s 336us/step - loss: 0.4573 - acc: 0.8175 - val_loss: 1.2992 - val_acc: 0.6205\n",
      "Epoch 114/300\n",
      "4548/4548 [==============================] - 2s 338us/step - loss: 0.4653 - acc: 0.8153 - val_loss: 1.3125 - val_acc: 0.6102\n",
      "Epoch 115/300\n",
      "4548/4548 [==============================] - 2s 330us/step - loss: 0.4595 - acc: 0.8155 - val_loss: 1.3272 - val_acc: 0.6110\n",
      "Epoch 116/300\n",
      "4548/4548 [==============================] - 2s 333us/step - loss: 0.4606 - acc: 0.8160 - val_loss: 1.3234 - val_acc: 0.6229\n",
      "Epoch 117/300\n",
      "4548/4548 [==============================] - 2s 338us/step - loss: 0.4539 - acc: 0.8151 - val_loss: 1.3321 - val_acc: 0.6221\n",
      "Epoch 118/300\n",
      "4548/4548 [==============================] - 2s 342us/step - loss: 0.4444 - acc: 0.8223 - val_loss: 1.3574 - val_acc: 0.6317\n",
      "Epoch 119/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4548/4548 [==============================] - 1s 317us/step - loss: 0.4457 - acc: 0.8217 - val_loss: 1.3112 - val_acc: 0.6181\n",
      "Epoch 120/300\n",
      "4548/4548 [==============================] - 1s 308us/step - loss: 0.4517 - acc: 0.8193 - val_loss: 1.3224 - val_acc: 0.6102\n",
      "Epoch 121/300\n",
      "4548/4548 [==============================] - 1s 310us/step - loss: 0.4439 - acc: 0.8199 - val_loss: 1.3758 - val_acc: 0.6245\n",
      "Epoch 122/300\n",
      "4548/4548 [==============================] - 1s 318us/step - loss: 0.4618 - acc: 0.8140 - val_loss: 1.3579 - val_acc: 0.6269\n",
      "Epoch 123/300\n",
      "4548/4548 [==============================] - 1s 316us/step - loss: 0.4442 - acc: 0.8175 - val_loss: 1.3012 - val_acc: 0.6333\n",
      "Epoch 124/300\n",
      "4548/4548 [==============================] - 1s 320us/step - loss: 0.4497 - acc: 0.8155 - val_loss: 1.3586 - val_acc: 0.6070\n",
      "Epoch 125/300\n",
      "4548/4548 [==============================] - 1s 322us/step - loss: 0.4370 - acc: 0.8173 - val_loss: 1.3053 - val_acc: 0.6333\n",
      "Epoch 126/300\n",
      "4548/4548 [==============================] - 2s 332us/step - loss: 0.4631 - acc: 0.8078 - val_loss: 1.3666 - val_acc: 0.6205\n",
      "Epoch 127/300\n",
      "4548/4548 [==============================] - 1s 317us/step - loss: 0.4300 - acc: 0.8208 - val_loss: 1.3247 - val_acc: 0.6181\n",
      "Epoch 128/300\n",
      "4548/4548 [==============================] - 1s 315us/step - loss: 0.4056 - acc: 0.8311 - val_loss: 1.3153 - val_acc: 0.6301\n",
      "Epoch 129/300\n",
      "4548/4548 [==============================] - 1s 310us/step - loss: 0.4308 - acc: 0.8250 - val_loss: 1.3231 - val_acc: 0.6317\n",
      "Epoch 130/300\n",
      "4548/4548 [==============================] - 1s 313us/step - loss: 0.4493 - acc: 0.8160 - val_loss: 1.3151 - val_acc: 0.6221\n",
      "Epoch 131/300\n",
      "4548/4548 [==============================] - 1s 317us/step - loss: 0.4376 - acc: 0.8250 - val_loss: 1.2938 - val_acc: 0.6269\n",
      "Epoch 132/300\n",
      "4548/4548 [==============================] - 1s 317us/step - loss: 0.4252 - acc: 0.8329 - val_loss: 1.3127 - val_acc: 0.6317\n",
      "Epoch 133/300\n",
      "4548/4548 [==============================] - 1s 324us/step - loss: 0.4311 - acc: 0.8248 - val_loss: 1.3438 - val_acc: 0.6364\n",
      "Epoch 134/300\n",
      "4548/4548 [==============================] - 2s 364us/step - loss: 0.4248 - acc: 0.8263 - val_loss: 1.3502 - val_acc: 0.6356\n",
      "Epoch 135/300\n",
      "4548/4548 [==============================] - 2s 339us/step - loss: 0.4240 - acc: 0.8285 - val_loss: 1.3538 - val_acc: 0.6404\n",
      "Epoch 136/300\n",
      "4548/4548 [==============================] - 1s 321us/step - loss: 0.4205 - acc: 0.8340 - val_loss: 1.3435 - val_acc: 0.6285\n",
      "Epoch 137/300\n",
      "4548/4548 [==============================] - 1s 310us/step - loss: 0.4260 - acc: 0.8190 - val_loss: 1.3631 - val_acc: 0.6309\n",
      "Epoch 138/300\n",
      "4548/4548 [==============================] - 1s 315us/step - loss: 0.4257 - acc: 0.8232 - val_loss: 1.3813 - val_acc: 0.6356\n",
      "Epoch 139/300\n",
      "4548/4548 [==============================] - 1s 319us/step - loss: 0.4294 - acc: 0.8305 - val_loss: 1.3684 - val_acc: 0.6213\n",
      "Epoch 140/300\n",
      "4548/4548 [==============================] - 1s 323us/step - loss: 0.4261 - acc: 0.8276 - val_loss: 1.4070 - val_acc: 0.6277\n",
      "Epoch 141/300\n",
      "4548/4548 [==============================] - 1s 319us/step - loss: 0.4226 - acc: 0.8333 - val_loss: 1.3913 - val_acc: 0.6372\n",
      "Epoch 142/300\n",
      "4548/4548 [==============================] - 1s 315us/step - loss: 0.4130 - acc: 0.8355 - val_loss: 1.3803 - val_acc: 0.6277\n",
      "Epoch 143/300\n",
      "4548/4548 [==============================] - 1s 315us/step - loss: 0.4367 - acc: 0.8278 - val_loss: 1.4837 - val_acc: 0.6062\n",
      "Epoch 144/300\n",
      "4548/4548 [==============================] - 1s 320us/step - loss: 0.4311 - acc: 0.8241 - val_loss: 1.3407 - val_acc: 0.6269\n",
      "Epoch 145/300\n",
      "4548/4548 [==============================] - 1s 310us/step - loss: 0.4101 - acc: 0.8331 - val_loss: 1.3690 - val_acc: 0.6221\n",
      "Epoch 146/300\n",
      "4548/4548 [==============================] - 1s 315us/step - loss: 0.4029 - acc: 0.8395 - val_loss: 1.4158 - val_acc: 0.6197\n",
      "Epoch 147/300\n",
      "4548/4548 [==============================] - 1s 320us/step - loss: 0.4181 - acc: 0.8336 - val_loss: 1.3985 - val_acc: 0.6205\n",
      "Epoch 148/300\n",
      "4548/4548 [==============================] - 1s 324us/step - loss: 0.4160 - acc: 0.8388 - val_loss: 1.4175 - val_acc: 0.6317\n",
      "Epoch 149/300\n",
      "4548/4548 [==============================] - 1s 325us/step - loss: 0.4121 - acc: 0.8338 - val_loss: 1.3982 - val_acc: 0.6269\n",
      "Epoch 150/300\n",
      "4548/4548 [==============================] - 1s 326us/step - loss: 0.4032 - acc: 0.8423 - val_loss: 1.3705 - val_acc: 0.6388\n",
      "Epoch 151/300\n",
      "4548/4548 [==============================] - 1s 326us/step - loss: 0.3989 - acc: 0.8412 - val_loss: 1.3812 - val_acc: 0.6579\n",
      "Epoch 152/300\n",
      "4548/4548 [==============================] - 1s 321us/step - loss: 0.4163 - acc: 0.8369 - val_loss: 1.3679 - val_acc: 0.6444\n",
      "Epoch 153/300\n",
      "4548/4548 [==============================] - 1s 314us/step - loss: 0.4053 - acc: 0.8404 - val_loss: 1.3637 - val_acc: 0.6253\n",
      "Epoch 154/300\n",
      "4548/4548 [==============================] - 1s 310us/step - loss: 0.3973 - acc: 0.8439 - val_loss: 1.4054 - val_acc: 0.6253\n",
      "Epoch 155/300\n",
      "4548/4548 [==============================] - 1s 315us/step - loss: 0.3861 - acc: 0.8472 - val_loss: 1.4079 - val_acc: 0.6364\n",
      "Epoch 156/300\n",
      "4548/4548 [==============================] - 1s 320us/step - loss: 0.3903 - acc: 0.8421 - val_loss: 1.4192 - val_acc: 0.6086\n",
      "Epoch 157/300\n",
      "4548/4548 [==============================] - 1s 323us/step - loss: 0.3888 - acc: 0.8467 - val_loss: 1.4468 - val_acc: 0.6205\n",
      "Epoch 158/300\n",
      "4548/4548 [==============================] - 1s 325us/step - loss: 0.3933 - acc: 0.8399 - val_loss: 1.4588 - val_acc: 0.6388\n",
      "Epoch 159/300\n",
      "4548/4548 [==============================] - 1s 312us/step - loss: 0.3955 - acc: 0.8408 - val_loss: 1.3672 - val_acc: 0.6348\n",
      "Epoch 160/300\n",
      "4548/4548 [==============================] - 1s 319us/step - loss: 0.3711 - acc: 0.8533 - val_loss: 1.4120 - val_acc: 0.6269\n",
      "Epoch 161/300\n",
      "4548/4548 [==============================] - 1s 316us/step - loss: 0.3974 - acc: 0.8423 - val_loss: 1.4260 - val_acc: 0.6293\n",
      "Epoch 162/300\n",
      "4548/4548 [==============================] - 1s 317us/step - loss: 0.3858 - acc: 0.8443 - val_loss: 1.4546 - val_acc: 0.6261\n",
      "Epoch 163/300\n",
      "4548/4548 [==============================] - 1s 315us/step - loss: 0.3907 - acc: 0.8417 - val_loss: 1.4652 - val_acc: 0.6428\n",
      "Epoch 164/300\n",
      "4548/4548 [==============================] - 1s 321us/step - loss: 0.4140 - acc: 0.8327 - val_loss: 1.4197 - val_acc: 0.6356\n",
      "Epoch 165/300\n",
      "4548/4548 [==============================] - 1s 321us/step - loss: 0.3922 - acc: 0.8434 - val_loss: 1.4163 - val_acc: 0.6269\n",
      "Epoch 166/300\n",
      "4548/4548 [==============================] - 1s 323us/step - loss: 0.3937 - acc: 0.8437 - val_loss: 1.4580 - val_acc: 0.6245\n",
      "Epoch 167/300\n",
      "4548/4548 [==============================] - 1s 313us/step - loss: 0.3683 - acc: 0.8599 - val_loss: 1.4589 - val_acc: 0.6388\n",
      "Epoch 168/300\n",
      "4548/4548 [==============================] - 1s 320us/step - loss: 0.3904 - acc: 0.8412 - val_loss: 1.4544 - val_acc: 0.6205\n",
      "Epoch 169/300\n",
      "4548/4548 [==============================] - 1s 321us/step - loss: 0.3871 - acc: 0.8487 - val_loss: 1.4346 - val_acc: 0.6444\n",
      "Epoch 170/300\n",
      "4548/4548 [==============================] - 1s 328us/step - loss: 0.3830 - acc: 0.8516 - val_loss: 1.4318 - val_acc: 0.6420\n",
      "Epoch 171/300\n",
      "4548/4548 [==============================] - 1s 317us/step - loss: 0.3716 - acc: 0.8564 - val_loss: 1.4568 - val_acc: 0.6229\n",
      "Epoch 172/300\n",
      "4548/4548 [==============================] - 1s 315us/step - loss: 0.3904 - acc: 0.8467 - val_loss: 1.4702 - val_acc: 0.6380\n",
      "Epoch 173/300\n",
      "4548/4548 [==============================] - 1s 316us/step - loss: 0.3884 - acc: 0.8373 - val_loss: 1.4178 - val_acc: 0.6428\n",
      "Epoch 174/300\n",
      "4548/4548 [==============================] - 1s 321us/step - loss: 0.3823 - acc: 0.8456 - val_loss: 1.4186 - val_acc: 0.6213\n",
      "Epoch 175/300\n",
      "4548/4548 [==============================] - 1s 329us/step - loss: 0.3693 - acc: 0.8518 - val_loss: 1.4864 - val_acc: 0.6340\n",
      "Epoch 176/300\n",
      "4548/4548 [==============================] - 1s 325us/step - loss: 0.4216 - acc: 0.8283 - val_loss: 1.4849 - val_acc: 0.6317\n",
      "Epoch 177/300\n",
      "4548/4548 [==============================] - 1s 319us/step - loss: 0.3855 - acc: 0.8496 - val_loss: 1.4367 - val_acc: 0.6333\n",
      "Epoch 178/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4548/4548 [==============================] - 1s 309us/step - loss: 0.3728 - acc: 0.8542 - val_loss: 1.4567 - val_acc: 0.6500\n",
      "Epoch 179/300\n",
      "4548/4548 [==============================] - 1s 313us/step - loss: 0.3841 - acc: 0.8463 - val_loss: 1.4592 - val_acc: 0.6309\n",
      "Epoch 180/300\n",
      "4548/4548 [==============================] - 1s 320us/step - loss: 0.3709 - acc: 0.8522 - val_loss: 1.4600 - val_acc: 0.6364\n",
      "Epoch 181/300\n",
      "4548/4548 [==============================] - 2s 337us/step - loss: 0.3630 - acc: 0.8586 - val_loss: 1.4506 - val_acc: 0.6396\n",
      "Epoch 182/300\n",
      "4548/4548 [==============================] - 2s 340us/step - loss: 0.3916 - acc: 0.8461 - val_loss: 1.4357 - val_acc: 0.6317\n",
      "Epoch 183/300\n",
      "4548/4548 [==============================] - 1s 323us/step - loss: 0.4248 - acc: 0.8300 - val_loss: 1.4951 - val_acc: 0.6388\n",
      "Epoch 184/300\n",
      "4548/4548 [==============================] - 1s 326us/step - loss: 0.3965 - acc: 0.8439 - val_loss: 1.4896 - val_acc: 0.6285\n",
      "Epoch 185/300\n",
      "4548/4548 [==============================] - 1s 320us/step - loss: 0.3689 - acc: 0.8621 - val_loss: 1.4589 - val_acc: 0.6356\n",
      "Epoch 186/300\n",
      "4548/4548 [==============================] - 2s 330us/step - loss: 0.3707 - acc: 0.8529 - val_loss: 1.4875 - val_acc: 0.6118\n",
      "Epoch 187/300\n",
      "4548/4548 [==============================] - 1s 329us/step - loss: 0.3996 - acc: 0.8421 - val_loss: 1.4785 - val_acc: 0.6229\n",
      "Epoch 188/300\n",
      "4548/4548 [==============================] - 2s 330us/step - loss: 0.3692 - acc: 0.8448 - val_loss: 1.4976 - val_acc: 0.6356\n",
      "Epoch 189/300\n",
      "4548/4548 [==============================] - 1s 324us/step - loss: 0.3771 - acc: 0.8498 - val_loss: 1.4700 - val_acc: 0.6269\n",
      "Epoch 190/300\n",
      "4548/4548 [==============================] - 2s 332us/step - loss: 0.3662 - acc: 0.8518 - val_loss: 1.4460 - val_acc: 0.6173\n",
      "Epoch 191/300\n",
      "4548/4548 [==============================] - 1s 324us/step - loss: 0.3711 - acc: 0.8529 - val_loss: 1.4954 - val_acc: 0.6460\n",
      "Epoch 192/300\n",
      "4548/4548 [==============================] - 1s 324us/step - loss: 0.3733 - acc: 0.8555 - val_loss: 1.5000 - val_acc: 0.6309\n",
      "Epoch 193/300\n",
      "4548/4548 [==============================] - 1s 326us/step - loss: 0.3909 - acc: 0.8459 - val_loss: 1.5001 - val_acc: 0.6229\n",
      "Epoch 194/300\n",
      "4548/4548 [==============================] - 1s 323us/step - loss: 0.3765 - acc: 0.8454 - val_loss: 1.4681 - val_acc: 0.6277\n",
      "Epoch 195/300\n",
      "4548/4548 [==============================] - 2s 330us/step - loss: 0.3597 - acc: 0.8575 - val_loss: 1.4600 - val_acc: 0.6372\n",
      "Epoch 196/300\n",
      "4548/4548 [==============================] - 1s 319us/step - loss: 0.3737 - acc: 0.8465 - val_loss: 1.5025 - val_acc: 0.6325\n",
      "Epoch 197/300\n",
      "4548/4548 [==============================] - 1s 325us/step - loss: 0.3651 - acc: 0.8496 - val_loss: 1.5079 - val_acc: 0.6309\n",
      "Epoch 198/300\n",
      "4548/4548 [==============================] - 1s 316us/step - loss: 0.3530 - acc: 0.8575 - val_loss: 1.4700 - val_acc: 0.6396\n",
      "Epoch 199/300\n",
      "4548/4548 [==============================] - 1s 323us/step - loss: 0.3568 - acc: 0.8595 - val_loss: 1.5169 - val_acc: 0.6420\n",
      "Epoch 200/300\n",
      "4548/4548 [==============================] - 1s 323us/step - loss: 0.3892 - acc: 0.8421 - val_loss: 1.5584 - val_acc: 0.6261\n",
      "Epoch 201/300\n",
      "4548/4548 [==============================] - 1s 315us/step - loss: 0.3618 - acc: 0.8580 - val_loss: 1.4867 - val_acc: 0.6245\n",
      "Epoch 202/300\n",
      "4548/4548 [==============================] - 2s 346us/step - loss: 0.3693 - acc: 0.8494 - val_loss: 1.5249 - val_acc: 0.6340\n",
      "Epoch 203/300\n",
      "4548/4548 [==============================] - 1s 328us/step - loss: 0.3710 - acc: 0.8485 - val_loss: 1.4842 - val_acc: 0.6229\n",
      "Epoch 204/300\n",
      "4548/4548 [==============================] - 1s 326us/step - loss: 0.3844 - acc: 0.8507 - val_loss: 1.5141 - val_acc: 0.6181\n",
      "Epoch 205/300\n",
      "4548/4548 [==============================] - 1s 319us/step - loss: 0.3825 - acc: 0.8520 - val_loss: 1.5317 - val_acc: 0.6340\n",
      "Epoch 206/300\n",
      "4548/4548 [==============================] - 1s 325us/step - loss: 0.3574 - acc: 0.8674 - val_loss: 1.4976 - val_acc: 0.6309\n",
      "Epoch 207/300\n",
      "4548/4548 [==============================] - 2s 334us/step - loss: 0.3553 - acc: 0.8591 - val_loss: 1.5009 - val_acc: 0.6484\n",
      "Epoch 208/300\n",
      "4548/4548 [==============================] - 2s 337us/step - loss: 0.3681 - acc: 0.8536 - val_loss: 1.4776 - val_acc: 0.6333\n",
      "Epoch 209/300\n",
      "4548/4548 [==============================] - 1s 323us/step - loss: 0.3566 - acc: 0.8595 - val_loss: 1.5205 - val_acc: 0.6364\n",
      "Epoch 210/300\n",
      "4548/4548 [==============================] - 1s 327us/step - loss: 0.3505 - acc: 0.8569 - val_loss: 1.4988 - val_acc: 0.6380\n",
      "Epoch 211/300\n",
      "4548/4548 [==============================] - 1s 323us/step - loss: 0.3581 - acc: 0.8544 - val_loss: 1.4942 - val_acc: 0.6372\n",
      "Epoch 212/300\n",
      "4548/4548 [==============================] - 1s 314us/step - loss: 0.3533 - acc: 0.8582 - val_loss: 1.4988 - val_acc: 0.6388\n",
      "Epoch 213/300\n",
      "4548/4548 [==============================] - 1s 323us/step - loss: 0.3345 - acc: 0.8694 - val_loss: 1.5337 - val_acc: 0.6372\n",
      "Epoch 214/300\n",
      "4548/4548 [==============================] - 1s 321us/step - loss: 0.3491 - acc: 0.8615 - val_loss: 1.5241 - val_acc: 0.6468\n",
      "Epoch 215/300\n",
      "4548/4548 [==============================] - 1s 329us/step - loss: 0.3367 - acc: 0.8646 - val_loss: 1.5221 - val_acc: 0.6277\n",
      "Epoch 216/300\n",
      "4548/4548 [==============================] - 2s 343us/step - loss: 0.3517 - acc: 0.8621 - val_loss: 1.5303 - val_acc: 0.6380\n",
      "Epoch 217/300\n",
      "4548/4548 [==============================] - 2s 343us/step - loss: 0.3593 - acc: 0.8544 - val_loss: 1.5105 - val_acc: 0.6356\n",
      "Epoch 218/300\n",
      "4548/4548 [==============================] - 1s 313us/step - loss: 0.3593 - acc: 0.8626 - val_loss: 1.5257 - val_acc: 0.6420\n",
      "Epoch 219/300\n",
      "4548/4548 [==============================] - 1s 316us/step - loss: 0.3626 - acc: 0.8542 - val_loss: 1.5012 - val_acc: 0.6388\n",
      "Epoch 220/300\n",
      "4548/4548 [==============================] - 1s 322us/step - loss: 0.3471 - acc: 0.8628 - val_loss: 1.5004 - val_acc: 0.6468\n",
      "Epoch 221/300\n",
      "4548/4548 [==============================] - 1s 327us/step - loss: 0.3244 - acc: 0.8747 - val_loss: 1.5006 - val_acc: 0.6420\n",
      "Epoch 222/300\n",
      "4548/4548 [==============================] - 1s 324us/step - loss: 0.3434 - acc: 0.8650 - val_loss: 1.4515 - val_acc: 0.6436\n",
      "Epoch 223/300\n",
      "4548/4548 [==============================] - 2s 332us/step - loss: 0.3377 - acc: 0.8635 - val_loss: 1.4955 - val_acc: 0.6356\n",
      "Epoch 224/300\n",
      "4548/4548 [==============================] - 2s 337us/step - loss: 0.3636 - acc: 0.8615 - val_loss: 1.5159 - val_acc: 0.6348\n",
      "Epoch 225/300\n",
      "4548/4548 [==============================] - 2s 343us/step - loss: 0.3629 - acc: 0.8593 - val_loss: 1.5666 - val_acc: 0.6460\n",
      "Epoch 226/300\n",
      "4548/4548 [==============================] - 2s 332us/step - loss: 0.3515 - acc: 0.8580 - val_loss: 1.5710 - val_acc: 0.6404\n",
      "Epoch 227/300\n",
      "4548/4548 [==============================] - 1s 326us/step - loss: 0.3646 - acc: 0.8560 - val_loss: 1.5550 - val_acc: 0.6468\n",
      "Epoch 228/300\n",
      "4548/4548 [==============================] - 2s 337us/step - loss: 0.3492 - acc: 0.8602 - val_loss: 1.5677 - val_acc: 0.6476\n",
      "Epoch 229/300\n",
      "4548/4548 [==============================] - 1s 326us/step - loss: 0.3530 - acc: 0.8580 - val_loss: 1.5727 - val_acc: 0.6476\n",
      "Epoch 230/300\n",
      "4548/4548 [==============================] - 1s 315us/step - loss: 0.3511 - acc: 0.8615 - val_loss: 1.5797 - val_acc: 0.6253\n",
      "Epoch 231/300\n",
      "4548/4548 [==============================] - 1s 318us/step - loss: 0.3579 - acc: 0.8593 - val_loss: 1.5346 - val_acc: 0.6388\n",
      "Epoch 232/300\n",
      "4548/4548 [==============================] - 1s 323us/step - loss: 0.3316 - acc: 0.8727 - val_loss: 1.5378 - val_acc: 0.6396\n",
      "Epoch 233/300\n",
      "4548/4548 [==============================] - 1s 329us/step - loss: 0.3472 - acc: 0.8641 - val_loss: 1.5475 - val_acc: 0.6420\n",
      "Epoch 234/300\n",
      "4548/4548 [==============================] - 1s 327us/step - loss: 0.3230 - acc: 0.8683 - val_loss: 1.5573 - val_acc: 0.6460\n",
      "Epoch 235/300\n",
      "4548/4548 [==============================] - 2s 330us/step - loss: 0.3286 - acc: 0.8709 - val_loss: 1.5619 - val_acc: 0.6404\n",
      "Epoch 236/300\n",
      "4548/4548 [==============================] - 1s 325us/step - loss: 0.3493 - acc: 0.8610 - val_loss: 1.5754 - val_acc: 0.6484\n",
      "Epoch 237/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4548/4548 [==============================] - 1s 320us/step - loss: 0.3363 - acc: 0.8709 - val_loss: 1.5382 - val_acc: 0.6412\n",
      "Epoch 238/300\n",
      "4548/4548 [==============================] - 1s 321us/step - loss: 0.3335 - acc: 0.8652 - val_loss: 1.5594 - val_acc: 0.6436\n",
      "Epoch 239/300\n",
      "4548/4548 [==============================] - 1s 327us/step - loss: 0.3272 - acc: 0.8661 - val_loss: 1.5213 - val_acc: 0.6428\n",
      "Epoch 240/300\n",
      "4548/4548 [==============================] - 1s 324us/step - loss: 0.3518 - acc: 0.8588 - val_loss: 1.5599 - val_acc: 0.6460\n",
      "Epoch 241/300\n",
      "4548/4548 [==============================] - 1s 317us/step - loss: 0.3460 - acc: 0.8639 - val_loss: 1.5311 - val_acc: 0.6452\n",
      "Epoch 242/300\n",
      "4548/4548 [==============================] - 1s 319us/step - loss: 0.3461 - acc: 0.8617 - val_loss: 1.5416 - val_acc: 0.6364\n",
      "Epoch 243/300\n",
      "4548/4548 [==============================] - 1s 320us/step - loss: 0.3411 - acc: 0.8639 - val_loss: 1.5390 - val_acc: 0.6396\n",
      "Epoch 244/300\n",
      "4548/4548 [==============================] - 1s 308us/step - loss: 0.3363 - acc: 0.8694 - val_loss: 1.6298 - val_acc: 0.6317\n",
      "Epoch 245/300\n",
      "4548/4548 [==============================] - 1s 309us/step - loss: 0.3412 - acc: 0.8665 - val_loss: 1.5601 - val_acc: 0.6380\n",
      "Epoch 246/300\n",
      "4548/4548 [==============================] - 1s 307us/step - loss: 0.3391 - acc: 0.8659 - val_loss: 1.5339 - val_acc: 0.6356\n",
      "Epoch 247/300\n",
      "4548/4548 [==============================] - 1s 304us/step - loss: 0.3446 - acc: 0.8674 - val_loss: 1.5247 - val_acc: 0.6444\n",
      "Epoch 248/300\n",
      "4548/4548 [==============================] - 1s 314us/step - loss: 0.3245 - acc: 0.8731 - val_loss: 1.5589 - val_acc: 0.6492\n",
      "Epoch 249/300\n",
      "4548/4548 [==============================] - 1s 316us/step - loss: 0.3334 - acc: 0.8661 - val_loss: 1.6044 - val_acc: 0.6245\n",
      "Epoch 250/300\n",
      "4548/4548 [==============================] - 1s 316us/step - loss: 0.3311 - acc: 0.8672 - val_loss: 1.5960 - val_acc: 0.6277\n",
      "Epoch 251/300\n",
      "4548/4548 [==============================] - 1s 322us/step - loss: 0.3448 - acc: 0.8610 - val_loss: 1.6197 - val_acc: 0.6317\n",
      "Epoch 252/300\n",
      "4548/4548 [==============================] - 1s 319us/step - loss: 0.3290 - acc: 0.8747 - val_loss: 1.6401 - val_acc: 0.6484\n",
      "Epoch 253/300\n",
      "4548/4548 [==============================] - 1s 315us/step - loss: 0.3285 - acc: 0.8676 - val_loss: 1.5745 - val_acc: 0.6388\n",
      "Epoch 254/300\n",
      "4548/4548 [==============================] - 1s 316us/step - loss: 0.3535 - acc: 0.8632 - val_loss: 1.5648 - val_acc: 0.6420\n",
      "Epoch 255/300\n",
      "4548/4548 [==============================] - 1s 320us/step - loss: 0.3312 - acc: 0.8690 - val_loss: 1.5559 - val_acc: 0.6348\n",
      "Epoch 256/300\n",
      "4548/4548 [==============================] - 1s 322us/step - loss: 0.3489 - acc: 0.8659 - val_loss: 1.5601 - val_acc: 0.6484\n",
      "Epoch 257/300\n",
      "4548/4548 [==============================] - 2s 330us/step - loss: 0.3313 - acc: 0.8701 - val_loss: 1.5642 - val_acc: 0.6396\n",
      "Epoch 258/300\n",
      "4548/4548 [==============================] - 2s 331us/step - loss: 0.3082 - acc: 0.8782 - val_loss: 1.6016 - val_acc: 0.6325\n",
      "Epoch 259/300\n",
      "4548/4548 [==============================] - 1s 325us/step - loss: 0.3430 - acc: 0.8617 - val_loss: 1.6088 - val_acc: 0.6245\n",
      "Epoch 260/300\n",
      "4548/4548 [==============================] - 1s 323us/step - loss: 0.3256 - acc: 0.8738 - val_loss: 1.6692 - val_acc: 0.6325\n",
      "Epoch 261/300\n",
      "4548/4548 [==============================] - 1s 326us/step - loss: 0.3131 - acc: 0.8751 - val_loss: 1.5733 - val_acc: 0.6460\n",
      "Epoch 262/300\n",
      "4548/4548 [==============================] - 1s 318us/step - loss: 0.3212 - acc: 0.8705 - val_loss: 1.5691 - val_acc: 0.6356\n",
      "Epoch 263/300\n",
      "4548/4548 [==============================] - 1s 313us/step - loss: 0.3279 - acc: 0.8723 - val_loss: 1.5618 - val_acc: 0.6436\n",
      "Epoch 264/300\n",
      "4548/4548 [==============================] - 1s 309us/step - loss: 0.3249 - acc: 0.8712 - val_loss: 1.6454 - val_acc: 0.6165\n",
      "Epoch 265/300\n",
      "4548/4548 [==============================] - 1s 318us/step - loss: 0.3304 - acc: 0.8745 - val_loss: 1.6243 - val_acc: 0.6348\n",
      "Epoch 266/300\n",
      "4548/4548 [==============================] - 1s 320us/step - loss: 0.3264 - acc: 0.8725 - val_loss: 1.6303 - val_acc: 0.6396\n",
      "Epoch 267/300\n",
      "4548/4548 [==============================] - 1s 321us/step - loss: 0.3541 - acc: 0.8608 - val_loss: 1.6045 - val_acc: 0.6309\n",
      "Epoch 268/300\n",
      "4548/4548 [==============================] - 1s 322us/step - loss: 0.3430 - acc: 0.8650 - val_loss: 1.5690 - val_acc: 0.6340\n",
      "Epoch 269/300\n",
      "4548/4548 [==============================] - 2s 335us/step - loss: 0.3327 - acc: 0.8663 - val_loss: 1.6234 - val_acc: 0.6197\n",
      "Epoch 270/300\n",
      "4548/4548 [==============================] - 2s 367us/step - loss: 0.3347 - acc: 0.8690 - val_loss: 1.6263 - val_acc: 0.6309\n",
      "Epoch 271/300\n",
      "4548/4548 [==============================] - 2s 341us/step - loss: 0.3300 - acc: 0.8610 - val_loss: 1.6020 - val_acc: 0.6356\n",
      "Epoch 272/300\n",
      "4548/4548 [==============================] - 2s 331us/step - loss: 0.3318 - acc: 0.8714 - val_loss: 1.6521 - val_acc: 0.6181\n",
      "Epoch 273/300\n",
      "4548/4548 [==============================] - 1s 320us/step - loss: 0.3433 - acc: 0.8646 - val_loss: 1.5899 - val_acc: 0.6539\n",
      "Epoch 274/300\n",
      "4548/4548 [==============================] - 1s 321us/step - loss: 0.3287 - acc: 0.8705 - val_loss: 1.6209 - val_acc: 0.6277\n",
      "Epoch 275/300\n",
      "4548/4548 [==============================] - 1s 325us/step - loss: 0.3268 - acc: 0.8698 - val_loss: 1.6290 - val_acc: 0.6301\n",
      "Epoch 276/300\n",
      "4548/4548 [==============================] - 1s 328us/step - loss: 0.3019 - acc: 0.8819 - val_loss: 1.5745 - val_acc: 0.6420\n",
      "Epoch 277/300\n",
      "4548/4548 [==============================] - 1s 315us/step - loss: 0.3076 - acc: 0.8791 - val_loss: 1.6270 - val_acc: 0.6396\n",
      "Epoch 278/300\n",
      "4548/4548 [==============================] - 1s 322us/step - loss: 0.3179 - acc: 0.8755 - val_loss: 1.6444 - val_acc: 0.6364\n",
      "Epoch 279/300\n",
      "4548/4548 [==============================] - 1s 319us/step - loss: 0.3131 - acc: 0.8755 - val_loss: 1.6464 - val_acc: 0.6476\n",
      "Epoch 280/300\n",
      "4548/4548 [==============================] - 1s 322us/step - loss: 0.3351 - acc: 0.8657 - val_loss: 1.6267 - val_acc: 0.6420\n",
      "Epoch 281/300\n",
      "4548/4548 [==============================] - 2s 332us/step - loss: 0.3083 - acc: 0.8782 - val_loss: 1.6410 - val_acc: 0.6333\n",
      "Epoch 282/300\n",
      "4548/4548 [==============================] - 1s 322us/step - loss: 0.3178 - acc: 0.8753 - val_loss: 1.6235 - val_acc: 0.6460\n",
      "Epoch 283/300\n",
      "4548/4548 [==============================] - 1s 325us/step - loss: 0.3111 - acc: 0.8793 - val_loss: 1.6273 - val_acc: 0.6404\n",
      "Epoch 284/300\n",
      "4548/4548 [==============================] - 1s 325us/step - loss: 0.3182 - acc: 0.8808 - val_loss: 1.5687 - val_acc: 0.6364\n",
      "Epoch 285/300\n",
      "4548/4548 [==============================] - 1s 314us/step - loss: 0.3254 - acc: 0.8718 - val_loss: 1.5892 - val_acc: 0.6412\n",
      "Epoch 286/300\n",
      "4548/4548 [==============================] - 1s 314us/step - loss: 0.3120 - acc: 0.8775 - val_loss: 1.6594 - val_acc: 0.6340\n",
      "Epoch 287/300\n",
      "4548/4548 [==============================] - 1s 309us/step - loss: 0.3160 - acc: 0.8753 - val_loss: 1.6503 - val_acc: 0.6364\n",
      "Epoch 288/300\n",
      "4548/4548 [==============================] - 1s 307us/step - loss: 0.2978 - acc: 0.8839 - val_loss: 1.7033 - val_acc: 0.6229\n",
      "Epoch 289/300\n",
      "4548/4548 [==============================] - 1s 311us/step - loss: 0.3153 - acc: 0.8681 - val_loss: 1.6450 - val_acc: 0.6380\n",
      "Epoch 290/300\n",
      "4548/4548 [==============================] - 1s 315us/step - loss: 0.3369 - acc: 0.8698 - val_loss: 1.6571 - val_acc: 0.6428\n",
      "Epoch 291/300\n",
      "4548/4548 [==============================] - 1s 325us/step - loss: 0.3329 - acc: 0.8698 - val_loss: 1.6323 - val_acc: 0.6261\n",
      "Epoch 292/300\n",
      "4548/4548 [==============================] - 1s 323us/step - loss: 0.3083 - acc: 0.8771 - val_loss: 1.6101 - val_acc: 0.6460\n",
      "Epoch 293/300\n",
      "4548/4548 [==============================] - 1s 319us/step - loss: 0.3258 - acc: 0.8703 - val_loss: 1.6346 - val_acc: 0.6317\n",
      "Epoch 294/300\n",
      "4548/4548 [==============================] - 1s 317us/step - loss: 0.3219 - acc: 0.8731 - val_loss: 1.6415 - val_acc: 0.6269\n",
      "Epoch 295/300\n",
      "4548/4548 [==============================] - 1s 317us/step - loss: 0.3181 - acc: 0.8731 - val_loss: 1.6162 - val_acc: 0.6508\n",
      "Epoch 296/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4548/4548 [==============================] - 1s 321us/step - loss: 0.3156 - acc: 0.8716 - val_loss: 1.6474 - val_acc: 0.6333\n",
      "Epoch 297/300\n",
      "4548/4548 [==============================] - 1s 315us/step - loss: 0.3151 - acc: 0.8766 - val_loss: 1.6354 - val_acc: 0.6340\n",
      "Epoch 298/300\n",
      "4548/4548 [==============================] - 1s 313us/step - loss: 0.3160 - acc: 0.8764 - val_loss: 1.6650 - val_acc: 0.6333\n",
      "Epoch 299/300\n",
      "4548/4548 [==============================] - 1s 316us/step - loss: 0.3120 - acc: 0.8764 - val_loss: 1.6618 - val_acc: 0.6380\n",
      "Epoch 300/300\n",
      "4548/4548 [==============================] - 1s 323us/step - loss: 0.3002 - acc: 0.8795 - val_loss: 1.6576 - val_acc: 0.6277\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "eveluate loss:1.657555634575224, evaluate acc:0.62768496439015\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "data=white_wine.append(red_wine)\n",
    "xt_train, yt_train, xt_test, yt_test = generate_data(data, 0.7)\n",
    "class wrwinemodel(object):#,node,opt,loss,learn,act\n",
    "    def __init__(self):\n",
    "        self.model=Sequential()\n",
    "    def construct(self):\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Dense(256,activation='relu',input_dim=11))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(128))\n",
    "        self.model.add(keras.layers.core.Dropout(0.2))\n",
    "        self.model.add(Dense(64,activation='relu'))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Dense(32,activation='relu'))\n",
    "        self.model.add(Dense(10,activation='softmax'))#rms,adagrad,adadelta\n",
    "        sgd=keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)#성능 굳굳...ㄸ\n",
    "        self.model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])#adam,rmsprop각각\n",
    "    def fit(self):\n",
    "        self.history=self.model.fit(xt_train,yt_train,epochs=300,batch_size=128,verbose=1,validation_data=(xt_test,yt_test))#validationset만드든것 성능 비교\n",
    "        #128로 했을 때 성능 더 굳 256보다\n",
    "       # self.history=self.model.train_on_batch(x_train,y_train,sample_weight=128,)\n",
    "    def printkey(self):\n",
    "        print(self.history.history.keys())\n",
    "        \n",
    "    def figure(self):\n",
    "        fig=plt.figure()\n",
    "        ax1=fig.add_subplot(1,2,1)\n",
    "        plt.plot(self.history.history['acc'])\n",
    "        plt.title('test accuracy')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        ax2=fig.add_subplot(1,2,2)\n",
    "        plt.plot(self.history.history['loss'])\n",
    "        #plt.plot(history.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.show()\n",
    "        \n",
    "    def evaluate(self):\n",
    "        score = self.model.evaluate(xt_train, yt_train, verbose=0)\n",
    "        print(\"training\")\n",
    "        print(\"eveluate loss:{}, evaluate acc:{}\".format(score[0],score[1]))\n",
    "        \n",
    "    def test(self):\n",
    "        score = self.model.evaluate(xt_test, yt_test, verbose=0)\n",
    "        print(\"test\")\n",
    "        print(\"eveluate loss:{}, evaluate acc:{}\".format(score[0],score[1]))\n",
    "        \n",
    "wrmodel=wrwinemodel()\n",
    "wrmodel.construct()\n",
    "wrmodel.fit()\n",
    "wrmodel.figure()\n",
    "#firstmodel.evaluate()\n",
    "wrmodel.test()\n",
    "\n",
    "###########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
